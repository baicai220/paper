38
DeepWukong: Statically Detecting Software Vulnerabilities Using Deep Graph Neural Network
XIAO CHENG, HAOYU WANG, JIAYI HUA, and GUOAI XU, Beijing University of Posts and Telecommunications, China YULEI SUI, University of Technology Sydney, Australia
Static bug detection has shown its effectiveness in detecting well-defined memory errors, e.g., memory leaks, buffer overflows, and null dereference. However, modern software systems have a wide variety of vulnerabilities. These vulnerabilities are extremely complicated with sophisticated programming logic, and these bugs are often caused by different bad programming practices, challenging existing bug detection solutions. It is hard and labor-intensive to develop precise and efficient static analysis solutions for different types of vulnerabilities, particularly for those that may not have a clear specification as the traditional well-defined vulnerabilities. This article presents DeepWukong, a new deep-learning-based embedding approach to static detection of software vulnerabilities for C/C++ programs. Our approach makes a new attempt by leveraging advanced recent graph neural networks to embed code fragments in a compact and low-dimensional representation, producing a new code representation that preserves high-level programming logic (in the form of controland data-flows) together with the natural language information of a program. Our evaluation studies the top 10 most common C/C++ vulnerabilities during the past 3 years. We have conducted our experiments using 105,428 real-world programs by comparing our approach with four well-known traditional static vulnerability detectors and three state-of-the-art deep-learning-based approaches. The experimental results demonstrate the effectiveness of our research and have shed light on the promising direction of combining program analysis with deep learning techniques to address the general static code analysis challenges.
CCS Concepts: • Security and privacy → Software security engineering; • Computing methodologies → Machine learning; • Software and its engineering → Automated static analysis;
Additional Key Words and Phrases: Static analysis, graph embedding, vulnerabilities
ACM Reference format:
Xiao Cheng, Haoyu Wang, Jiayi Hua, Guoai Xu, and Yulei Sui. 2021. DeepWukong: Statically Detecting Software Vulnerabilities Using Deep Graph Neural Network. ACM Trans. Softw. Eng. Methodol. 30, 3, Article 38 (April 2021), 33 pages. https://doi.org/10.1145/3436877
This work was supported by the National Key Research and Development Program of China (No. 2018YFB0803605), the National Natural Science Foundation of China (grants No. 61702045 and No. 62072046), and Australian Research Grants (DP210101348). Authors’ addresses: X. Cheng, University of Technology Sydney, 15 Broadway, Sydney, NSW, Australia, 2006; email: 13965013@student.uts.edu.au; H. Wang (co-corresponding author), J. Hua, and G. Xu (co-corresponding author), Beijing University of Posts and Telecommunications, No. 10, Xitucheng Road, Haidian District, Beijing, China, 100876; emails: {haoyuwang, huajiayi, xga}@bupt.edu.cn. Y. Sui, University of Technology Sydney, 15 Broadway, Sydney, NSW, Australia, 2006; email: yulei.sui@uts.edu.au. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2021 Association for Computing Machinery. 1049-331X/2021/04-ART38 $15.00 https://doi.org/10.1145/3436877
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:2 X. Cheng et al.
1 INTRODUCTION
Modern software systems are often plagued with a wide variety of software vulnerabilities. Detecting and fixing these complicated, emerging, and wide-ranging vulnerabilities are extremely hard. The number of vulnerabilities registered in the Common Vulnerabilities and Exposures (CVEs) [1] has increased significantly during the past 3 years. Statistics show that there are 34,473 newly registered CVEs and 34,093 of them are above medium-level security from January 1, 2017, to July 20, 2019, according to NVD [1]. Table 1 lists the number of CVEs for the top 10 most Common C/C++ Weakness Enumeration (CWE) vulnerabilities. It can be seen that these vulnerabilities behave differently with quite different specifications. Existing efforts. Static bug detection, which approximates the runtime behavior of a program without running it, is the major way to pinpoint bugs at the early stage of the software development cycle, thus reducing software maintenance cost. Traditional static analysis techniques (e.g., Clang Static Analyzer [2], Coverity [3], Fortify [4], Flawfinder [5], Infer [6], ITS4 [7], RATS [8], Checkmarx [9], and SVF [10]) have shown their success in detecting well-defined memory corruption bugs. However, adapting the existing solutions for detecting a wide variety of emerging vulnerabilities has two major limitations. First, they rely on static analysis experts to define specific detection strategies for different types of vulnerabilities, which is labor-intensive and time-consuming. Second, the effectiveness of the predeveloped detection systems highly relies on the expertise of the analysis developers and the knowledge of existing vulnerabilities. The emerging high-level vulnerabilities pose big challenges to existing bug detection approaches, making it hard to extend the existing bug detectors. Challenges. We give two real-world vulnerabilities to demonstrate the challenges in identifying different high-level vulnerabilities. Figure 1(a) shows a vulnerable code fragment from IPsecTools 0.8.2.1 It contains a remotely exploitable attack when parsing and storing ISAKMP fragments when missing a conditional guard. The code below the black dotted line is vulnerable and the above one is safe (after it was fixed by the developer). Most of the two code fragments share the same logic (omitted here), but they are different in checking the last ISAKMP fragment. Unfortunately, the conditional check in the vulnerable code is incorrect and inconsistent with many other parts of the code in this project. This vulnerability took an experienced developer 23 days to eventually find and fix it.2 Figure 1(b) shows an example of improper resource shutdown or release.3 The vulnerable code fragment does not close an opened file handle if an error occurs. If this is a long-running process, it can run out of file handle resources; therefore, missing the “try/catch” exceptional handling may cause program crashes or hang. These high-level bugs are caused by inconsistent business logic and bad programming practices; therefore, it is challenging for traditional static detectors (e.g., memory error detectors) that rely on well-defined specifications to capture these vulnerabilities. Insights. In reality, the unexpected behaviors of a vulnerable program often manifest in different aspects of the code features, including code tokens, APIs, and control- and data-flow of a program. The code pattern can be quite different between vulnerable and safe code because it can reflect the interprocedural execution order, the logic of a program, and good/bad programming practices. There are a handful of efforts in pinpointing vulnerabilities at different levels of granularity (e.g., program [11], package [12], component [13], file [14], method [15–17], and slice [18]) by
1The whole fixing patch is available at http://cvsweb.netbsd.org/bsdweb.cgi/src/crypto/dist/ipsec-tools/src/racoon/ isakmp_frag.c.diff ?r1=1.5&r2=1.5.36.1. 2https://nvd.nist.gov/vuln/detail/CVE-2016-10396. 3The detailed description is available at https://cwe.mitre.org/data/definitions/404.html.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:3
Table 1. Top 10 Most Common C/C++ CVEs (in Terms of Their Numbers Appearing in NVD [1]) from January 1, 2017 to July 20, 2019
Vul Category Description CVE-num CWE119 Improper Restriction of Operations within the Bounds of a Memory Buffer 4,524 CWE20 Improper Input Validation 2,769 CWE125 Out-of-Bounds Read 1,245 CWE190 Integer Overflow or Wraparound 877 CWE22 Improper Limitation of a Pathname to a Restricted Directory 857 CWE399 Resource Management Errors 699 CWE787 Out-of-Bounds Write 534 CWE254 Security Features 483 CWE400 Uncontrolled Resource Consumption 393 CWE78 Improper Neutralization of Special Elements Used in an OS Command 258
Fig. 1. Real-world vulnerabilities that are hard to be automatically identified by traditional static vulnerability detection approaches. For each example, we list the safe code fragment and the corresponding identified vulnerable code.
combining machine learning with static bug detection. The general idea is to generate a prediction model that captures the correlation between vulnerable programs and their (extracted) program features through sample training. Later, a new program can be predicted as safe or vulnerable based on the trained prediction model. However, almost all of them focus on detecting low-level memory errors, e.g., buffer overflows and use-after-frees. The real-world vulnerabilities are much more complicated (cf. Figure 1), posing challenges for existing approaches. Moreover, current approaches extract shallow code features (e.g., code tokens and abstract syntax trees) by directly applying the embedding techniques, such as Word2Vec [19] and Doc2Vec [20], while the comprehensive code features (e.g., control- and data-dependence) are not precisely preserved in the embedding space. Allamanis et al. [21] have proposed to use gated graph neural networks to represent the syntactic and semantic structure of a program, aiming at solving two software engineering tasks, i.e., variable renaming and misuses, rather than detecting general software vulnerabilities. In addition, they did not perform interprocedural program dependence analysis (by considering pointer aliases), which is essential for real-world programs since function calls are quite common in modern software projects. Finally,
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:4 X. Cheng et al.
Fig. 2. Overview of DeepWukong. Models encoded with vulnerability patterns are saved after training, which then are loaded to flag whether the target gray code is vulnerable or not.
existing deep-learning-based approaches predict a bug at method or file level in a coarse-grained manner. It is hard to provide a more fine-grained way (e.g., identifying the program slices that may trigger a bug) for developers to precisely pinpoint a vulnerability. Our solution. In this article, we propose DeepWukong, a new deep-learning-based code embedding approach, to detect 10 different types of popular vulnerabilities. Our approach makes a new attempt by leveraging recent graph neural networks to embed code fragments in a compact low-dimensional code representation that preserves a program’s high-level control- and data-flow information, without the need for manually defining rules. Program slices (or XFGs) are first extracted from code fragments. A slice (or an XFG) is labeled as vulnerable if it contains a vulnerable statement (vulnerable program statements are all annotated in our ground truth samples) and safe otherwise. A neural network model is then trained using both these safe and vulnerable program slices. Both the unstructured and structured code information of a program are embedded when building our neural networks. The unstructured information is code tokens, while the structured information is manifested by the connections of nodes on XFG containing both control- and datadependence of a program. Both pieces of information are fed into the graph neural networks to produce compact code representation in the latent feature space. By learning the vulnerable and safe program slices using recent advances in GNN, DeepWukong supports more precise bug prediction to localize vulnerabilities at the finer-grained program slice level rather than at the file or method level. Overall framework. Figure 2 shows the overall framework of DeepWukong consisting of two phases: a training phase and a detecting phase. For the training phase, in (a.1), DeepWukong first computes the control- and data-dependence over the interprocedural control-flow graph (CFG) and value-flow graph (VFG), with pointer aliases information being considered, and constructs the Program Dependence Graph (PDG) based on the control- and data-flow information. In (a.2), DeepWukong generates each slice or XFG (a subgraph of the PDG) for the program by conducting forward and backward traversal along the PDG starting from a program point of interest (i.e., slicing criteria) until a fixed point is reached, thereby maintaining both the data- and control-flow of a program.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:5
In order to precisely preserve the semantic information of the source code for training a neural network, DeepWukong first conducts the variable name normalization by mapping userdefined variables and functions to their canonical symbolic names in (b.1). DeepWukong then uses Doc2Vec [20] to transform the tokens for each statement (i.e., each node on XFG) of the source code into vector representations as shown in (b.2). Next, we obtain both structured (XFG edges) and unstructured information (code tokens in the form of embedding vectors for each node on XFG) as the inputs of our neural networks as depicted in (c.1). Three kinds of graph neural networks are then used to evaluate the performance of our deep-learning-based approach as illustrated in (c.2). Finally, a trained model is produced for bug prediction. For the detecting phase, the control- and data-dependence of a target program is first extracted (a.1) to produce a set of slices (XFGs) (a.2). For each XFG, after symbolization (b.1) and embedding (b.2), both its edges and the code tokens of its nodes are used as the features (c.1) to feed into the previously trained model to predict whether each slice (XFG) of the target program is vulnerable or not. We have evaluated DeepWukong using a comprehensive benchmark that contains a list of 105,428 vulnerable programs, with 104,104 from SARD [22] and 1,324 from two real-world projects (i.e., redis and lua), which are related to the top 10 most common C/C++ vulnerabilities. We have conducted the experiments by comparing our approach with conventional static detectors, including Flawfinder [5], RATS [8], Clang Static Analyzer [2], Infer [6], and three deep-learningbased approaches [15, 16, 18]. We also compare the performance of three different graph neural networks for our code embedding, including GCN [23], GAT [24], and k-GNNs [25]. The key research contributions of this article are:
• We propose DeepWukong, a new deep-learning-based approach that embeds both textual and structured information of code into a comprehensive code representation by leveraging graph neural networks to support precise static bug detection of 10 types of popular C/C++ vulnerabilities. To enable precise code embedding, we propose a new program slicing approach to extract complicated high-level semantic features including data- and control-flows of a program. • We have conducted our experiments by comparing DeepWukong with the traditional static bug detectors and three recent learning-based bug detection approaches. Experimental results show that our deep-learning-based approach outperforms the existing approaches in terms of informedness, markedness, and F1 Score. • We have contributed a comprehensive reference dataset to our community from real-world programs, along with all the experiment results in this article. The data is available at https: //github.com/DeepWukong/DeepWukong.
2 MOTIVATING EXAMPLE
Figure 3 gives an example to show our key idea by going through the three steps in Figure 2. It is a simplified pseudo code fragment from a real-world web server.4 Note that the node number represents the statement number (line number) of the program. The code tries to establish a server socket connection. It accepts a request (Nodes 2–6) to store data on the local file system by reading the message repeatedly via a while loop (Nodes 7–9). (a) Program slicing. DeepWukong starts with a program point of interest for its slicing. The program point is usually from an API call or a program point specified by users. We use the API
4https://github.com/mongrel2/mongrel2.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:6 X. Cheng et al.
Fig. 3. A real-world program extracted from mongrel2 (a web server). It shows the key idea of DeepWukong by going through the three steps in Figure 2.
Fig. 4. An example to demonstrate our slicing by revisiting the code in Figure 3. For the PDG shown in Figure 4(a), each node represents a code statement and each edge represents a control- or data-dependence relation. Node 7 is the program point of interest. The algorithm iteratively performs backward and forward slicing until a fixed point is reached.
call “getNextMessageAPI” at Node 7 as an example (highlighted in red). We first obtain its controland data-flow-related nodes 1, 2, 3, 4, 7, 8, 9, 10, and 11 based on the transitive closure of the union of control- and data-dependence extracted by SVF [10]. Note that Nodes 4, 9, 10, and 11 are marked blue because they are control-flow related, Nodes 2 and 3 are marked green because they are data-flow related, and Nodes 1 and 8 are marked yellow since they are both control- and dataflow related. The detailed program slicing algorithm is illustrated in Figure 4 and Section 3.1.3. The corresponding statements from slicing (i.e., Nodes 1, 2, 3, 4, 7, 8, 9, 10, and 11) form the nodes of the XFG, while the control- and data-dependence between different statements contributes to the edges of XFG. For instance, Node 4 and Node 7 are connected with an Edge 4->7 because Node 7 is control dependent on Node 4. Node 3 is linked with Node 7 because Node 7 is data dependent on Node 3 (the variable “socket” defined at Node 3 is used at Node 7).
(b) Code token symbolization and embedding. After we obtain the unstructured and structured code information, i.e., XFG, we then process each node on the XFG by symbolizing their variable and function names to make a canonical form of the textual representation. We map user-defined variables and functions to their symbolic names (e.g., variables are named as “VAR0” and “VAR1,” functions are named as “FUN0” and “FUN1”). For example, we use “FUN0” to represent function name “WriteDataFromSocketToFile” and “VAR0” to represent variable name “host.”
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:7
Doc2Vec [20] is then used to transform the textual information, i.e., code tokens, of each node into a vector representation. For example, “vector1” is the vector representation of the code tokens at Node 1 (i.e., “int FUN0 (char * VAR0 , int VAR1)”). (c) Deep graph neural networks learning. The structured information of the XFG can be obtained from its edges (i.e., 1->2, 1->3, . . . , 8->9). The unstructured code information is the vector representations in the previous step (i.e., the embedding vectors of nodes on the XFG (i.e., vector1, vector2, . . . , vector11). We then feed the structured and unstructured information to the first layer of our graph neural networks. A model reflecting the code patterns is learned in the training phase, which is then used to predict whether a target XFG is vulnerable or not in the detecting phase.
3 DEEPWUKONG
This section further details each component of our approach, including program slicing (Section 3.1), code token symbolization and embedding (Section 3.2), and bug prediction based on deep graph neural networks (Section 3.3).
3.1 Program Slicing
In order to conduct program slicing, we first generate the PDG by considering both the controldependence and data-dependence, which are computed over the interprocedural CFG and VFG (Section 3.1.1). Then we perform code slicing based on a slicing criterion (i.e., starting traversing from a program point of interest (Section 3.1.2)) to construct each slice or XFG (a subgraph of the PDG) via forward and backward analysis on the PDG (Section 3.1.3).
3.1.1 Control- and Data-Dependence on PDG. A PDG is a directed graph where each node represents an instruction (program statement) and each edge represents a data- or control-dependence relation between two statements. A control-dependence edge Ni → Nj means the execution of Nj is determined by Ni , while a data-dependence edge N ′
i → N′
j represents that the definition at N ′
i
is used at N ′
j.
The control-dependence between two statements is computed over the CFG of a program, where each node represents an instruction (program statement) and each edge connects two nodes, signifying the control-flow or execution order between two instructions. CFG is commonly used in static analysis and compiler optimizations, since it contains the basic execution path information such as branch conditions. Control-dependence is usually defined in terms of postdominance. Given nodes X and Y (X ! = Y ) on a CFG, we say X postdominates Y if all paths from Y to the end of the program traverse through X . Y is control-dependent on X if (1) there exists a directed path P from X to Y with any Z in P (excluding X and Y ) postdominated by Y and (2) X is not postdominated by Y . We use augmented postdominator tree (APT) [26], which is constructed in space and time proportional to the size of a program, to calculate the control-dependence of a program. Data-dependence is obtained by the def-use relations on the interprocedural VFG [10, 27] of a program. In this graph, each node represents an instruction (i.e., program statement), and each edge represents a def-use relation of a variable between two statements. Given nodes X ′ and Y ′ (X ′! = Y ′) on a VFG, Y ′ is data-dependent on X ′ if a variable used at Y ′ is defined at X ′. Consider two nodes X ′ (a definition of a variable v ′) and Y ′ (a def or use of v ′) in VFG; if there is a path from X ′ to Y ′ and there is no redefinition of v ′, Y ′ is data dependent on X ′.
3.1.2 Program Points of Interest. Previous studies [18, 28, 29] show that system API calls are widely used by application programs and the misuse of them is one of the major causes of vulnerabilities. We use system API calls as the main program point of interest for our slicing. For some vulnerabilities (e.g., integer overflows, CWE190) that occur when applying arithmetic
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:8 X. Cheng et al.
operators, such as addition and multiplication operations, we further choose code statements containing arithmetic operators as the program point of interest to complement the system API calls. We refer to SVF [10] to automatically identify system API calls (1,449 in total), and we use antlr [30] to identify arithmetic operators (i.e., +, -, *, /, %), bit-wise operators (&, |, ∧ , ∼ , <<, >>), compound assignment expressions (i.e.,+=, -=, *=, /=, %=, >>=, <<=, &=, ∧ =, |=), and increment/decrement expressions (++, –). For these identified statements of code, we regard them as the points of interest for our code slicing.
3.1.3 Slicing. After building the PDG of a program, we perform slicing [31] based on each program point of interest to produce its corresponding XFG, a subgraph of PDG. The nodes of XFG are obtained by conducting forward and backward slicing starting from a program point of interest pi . For the forward slicing, we conduct forward traversal along the PDG starting from pi and get forward-sliced statements set Sf . For the backward slicing, we traverse the PDG starting from pi to include all the visited statements in Sb . The statements in both Sf and Sb are the final set of statements (i.e., Sf ∪ Sb ), which preserve both control- and data-flow information of the source code and form the nodes of the resulting XFG. Note that all the reachable statements from forward or backward traversal are included in the final set of statements. We then connect those nodes based on the edges of PDG to produce the final XFG, thereby capturing both control- and data-dependence of the program.
Example 3.1. Figure 4 shows an example (originated from the code in Figure 3) to illustrate our slicing approach. We first generate PDG by constructing control-dependence (solid edge) and data-dependence (dotted edge) (as shown in Figure 4(a)). After that, we perform forward and backward traversals along PDG starting from the API call “getNextMessageAPI” at Line 7 to obtain the forward statements set (Lines 8, 9, 10, and 11) and backward statements set (Lines 1, 2, 3, and 4) to generate the nodes of XFG (as shown in Figure 4(b)). Finally, we connect the relevant nodes by following the edges of PDG, which represent the control- or data-dependence between two nodes. For example, Node 1 and Node 2 are connected with an edge 1->2 since Node 2 is control dependent on Node 1 (as shown in Figure 4(c)).
3.2 Code Tokens Symbolization and Embedding
This section will introduce how we perform code token normalization and how to embed code tokens of each XFG’s node into a vector representation.
3.2.1 Code Tokens Symbolization. For each constructed XFG, we first perform the embedding of unstructured code information (i.e., the code tokens of each node on XFG) using Doc2Vec [20]. Before the embedding, we normalize the code tokens into a canonical symbolic form in order to reduce the noise introduced by personalized naming conventions for program variables to better preserve the original code semantics.
Example 3.2. Figure 5 gives an example to show that Doc2Vec may imprecisely produce different vector representations of two cloned code fragments in the latent embedding space due to different variable naming conventions. XFG (B) is a clone code of XFG (A), which shares the same code semantic information but uses distinct user-defined variables and functions. With our symbolization, “writeDataFromSocketToFile,” “host,” “port,” “buffer,” “BUFFER_SIZE,” “socket,” and “openSocketConnection” in XFG (A), and “write,” “h,” “p,” “buf,” “BUF_SIZE,” “s,” and “open_socket” in XFG (B) are mapped to “FUN0,” “VAR0,” “VAR1,” “VAR2,” “VAR3,” “VAR4,” and “FUN1,” respectively. In a two-dimensional embedding space, the two embedding vectors are different (with a distance between vectors A and B) without our symbolization, while normalizing the code tokens will help produce exactly the same vector.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:9
Fig. 5. The distraction caused by personalized naming. XFG (B) and XFG (A) have the same semantic but different user-defined method and variable names.
Following [32], we have replaced the user-defined variables and functions with their symbolic names, that is, mapping each one with a common identifier together with a separate index (i.e., FUN0, FUN1, VAR0, VAR1). Note that variables in two different code fragments (XFGs) can be mapped to the same symbolic name. For example, “buffer” in Code (A) and “buf” in Code (B) are mapped to the same symbolic name “VAR2.” It is worth mentioning that our symbolization, which only changes the names of variables, does not affect the semantic of a program.
3.2.2 Code Token Embedding. After symbolization, we will transform the code tokens of each XFG’s node (i.e., program statement) into a vector using Doc2Vec [20], a widely used technique to represent documents as low-dimensional vectors. Doc2Vec is an unsupervised model that can encode the entire code statement instead of an individual code token into a fixed-length vector. The key algorithm of Doc2Vec is called Distributed Memory version of Paragraph Vector, where a unique statement-topic token is used to represent the semantic meaning of a statement. Given a corpus comprising a sequence of tokens w1, w2, . . . , wN including a number of code tokens and statement-topic tokens, the task is to predict a token wi given its surrounding tokens wi−k , . . . , wi+k within a certain window of size 2k + 1 (or so-called fixed-length context). Tokens with a similar meaning are close to each other in the latent vector space when maximizing the average log probability of tokens:
1 N
N∑−k
i =k
log p (wi |wi−k , . . . , wi+k ) . (1)
The calculation of the probability of wi is typically achieved using a multiclassification classifier, such as softmax:
p (wi |wi−k , . . . , wi+k ) = exp(ywi )
∑
j exp(ywj ) . (2)
Here, ywj stands for the jth element in the unnormalized log-probability distribution vector y for output tokens:
y = Uh (wi−k , . . . , wi+k ; V) + b, (3)
where each column of V is a unique vector representing each token (i.e., a sequence of code tokens and statement-topic tokens) and is initialized randomly. U, b are the softmax parameters and h(·) is a concatenation of token vectors extracted from V. The representation vectors of the code tokens of a program statement are then trained and updated automatically using stochastic gradient descent (SGD) [33]. After training, each code
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:10 X. Cheng et al.
Fig. 6. An example to explain the usage of Doc2Vec for learning a statement vector. A statement-topic token represents the topic of a function declaration statement (“int FUN0 (char * VAR0 , int VAR1)”) and a code token corpus consisting of eight tokens of the statement including “int,” “FUN0,” “(”,“char”, “VAR0”, “,”, “VAR1”, “),” which are then used to predict the token (“*”) by concatenating token vectors and applying a softmax function. A fixed-length statement-topic vector (vs ) is obtained after training.
token vector is used to represent the token’s feature, while the statement-topic token vector holds the overall feature of a code statement.
Example 3.3. Figure 6 illustrates the key steps for embedding a code statement into a vector representation. The inputs are a statement-topic token vector (vs ) that represents the topic of a function declaration statement (“int FUN0 (char * VAR0 , int VAR1)”) and eight token vectors (v1, v2, . . . , v8) that represent code tokens (“int,” “FUN0,” “(”,“char”, “VAR0”, “,”, “VAR1”, “)”). Those vectors are initialized randomly and are concatenated to produce a fixed-length vector, which is then used to predict the token “*” (shown in green) by applying a softmax function. The vectors are updated automatically in the training process. vs can eventually represent the overall feature of the entire function declaration statement (“int FUN0 (char * VAR0 , int VAR1)”), while v1, v2, . . . , v8 capture the semantics of “int,” “FUN0,” . . . , “VAR1,” respectively.
Doc2Vec can provide a more precise embedding of the code statement compared to simply assembling vectors produced by individual token embedding, because important information may be lost during the token padding process, which will probably discard essential semantic-related tokens to produce a fixed-length sequence. Each node in XFG represents a code statement, which is independently considered as a short document (i.e., sequential words). After the training process of Doc2Vec, each statement-topic vector is obtained to represent each node of the XFG.
3.3 Deep Graph Neural Network Learning
We leverage a recent proposed graph neural architecture [34] comprising a graph convolutional layer, a graph pooling layer, and a graph readout layer to perform the graph classification task for XFGs. To achieve an ideal performance and demonstrate the generality of our approach, we instantiate our graph convolutional layer by feeding the XFGs into three state-of-the-art deep graph neural networks to train different prediction models and choose the best one. Note that these graph neural networks use the same graph pooling and readout layers, while having different graph convolutional layers. We first introduce our overall graph neural network architecture to show the full pipeline of our model. Then, we describe in detail how we adopt these networks in our convolutional layer and conduct the neural network training process. A detailed exposition of the graph convolutional layers is presented in appendix A.1.
3.3.1 Overall Graph Neural Network Architecture. Figure 7 shows a general structure of our neural network, consisting of several blocks of interleaved graph convolutional/pooling layers
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:11
Fig. 7. A general architecture of our graph neural network.
(conv-pool block); a graph readout layer (JK-net-style [35, 36] summary), which integrates features at different graph scales; and a multilayer perceptron (MLP) [37] for the final prediction. Note that the number of conv-pool blocks is set according to the size of our dataset. Graph convolutional layer (GCL). The general propagation rule that guides the feature transformation for the graph convolutional layer is shown here:
f (l ) (v ) = σ f (l−1) (v ) · W(l )
1+
∑
ω ∈N (v )
f (l−1) (ω ) · W(l )
2 , (4)
where f (l ) (v ) ∈ R1×d (l) means the output feature of node v in the lth layer, d (l ) represents the dimension, and N (v ) means the adjacent nodes of node v. W(l ) ∈ Rd (l−1)×d (l) means the weight matrices of the lth layer. Note that the weight matrices guiding the feature transformation of a node itself (W1) and its neighbors (W2) can be different according to the principle proposed by some graph neural networks (e.g., k-GNNs [25]). σ (·) is the activation function. Graph pooling layer (GPL). The objective of the graph pooling layer is to reduce the size of the original graph by applying a pooling ratio, k ∈ (0, 1], meaning that a graph with N nodes will have kN nodes after such a pooling layer, thereby reducing the amount of subsequent computation in the network. It simply drops N − kN nodes after each pooling operation based on a projection score against a learnable vector, p. The projection scores are also used as gating values to enable gradients to flow into p; therefore, for the remaining nodes, the extent of feature retention will be proportional to their scores. F = { f (0), f (1), . . . , f (N )} ∈ RN ×d means the matrix of node features. We use A to denote the adjacency matrix representing a graph. The transformation principle of the graph pooling layer that computes a pooled graph, (F′, A′), from an input graph, (F, A), can be formally expressed as
y = Fp p
i = top−k (y, k ) F′ = (F tanh y)i A′ = Ai,i . (5)
‖·‖ stands for the L2 norm, top−k selects the top−k indices from a given input vector, means (broadcasted) elementwise multiplication, and ·i is an indexing operation that takes nodes at indices specified by i. Graph readout layer (GRL). In order to produce a fixed-length representation for the entire graph so as to conduct the graph classification task, a graph readout layer is introduced to flatten features of all the nodes. Following traditional convolutional neural networks (CNNs), we perform global average pooling and global max pooling to strengthen the performance of our representation. Moreover, inspired by the JK-net architecture [35, 36], this summarization is performed after each block of interleaved graph convolutional/pooling layers (conv-pool block) and finally all of the summarizations are aggregated together. The output graph vector s (l ) ∈ R1×d (l) of
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:12 X. Cheng et al.
Fig. 8. An example to illustrate the full pipeline of our general graph neural network architecture.
the lth conv-pool block, (F(l ), A(l ) ), can be formally expressed as
s (l ) = 1
N (l )
N (l )
∑
i =1
f (l ) (i )||max N (l)
j=1 f (l ) (j ). (6)
Here, f (l ) (i) stands for the feature vector of node i, N (l ) means the number of nodes on the graph, and || denotes the concatenation operation. Finally, the summary vector of the graph is computed
as the sum of all those summaries (i.e., s = ∑L
l=1 s (l )). Note that it is significant to aggregate across conv-pool blocks to preserve information at different scales of processing. The vector representing the entire graph is then fed into a multilayer perceptron (MLP) [37] for the final prediction. The predicted distribution of the model q(lbi ) is computed using a softmax function, i.e., the dot product between the graph vector s and the vector representation lbi of each label lbi ∈ lb:
q(lbi ) = exp(s · lbi)
∑
lbj ∈Y exp(s · lbj ) . (7)
Example 3.4. Figure 8 exemplifies the full pipeline of our graph neural network, consisting of two interleaved graph convolutional/pooling layers (GCL1/GPL1, GCL2/GPL2) for graph feature abstraction, a GRL for summarization and flattening, and a multilayer perceptron for prediction. Starting from the input graph in (a), which consists of nine nodes (Nodes 1–9) with their links (1->5, 2->1, 2->5, 3->2, 3->5, 4->7, 5->4, 5->8, 5->9, 6->5, 6->7), the graph is then fed into the convolutional layer, which transforms the feature of each node by considering its predecessors and itself. For example, the vector representation of Node 5 (highlighted in red) after GCL1 in (b) is calculated over its predecessors (Node 1, 2, 3, 6) and itself based on Equation (4). Afterward, the graph passes through GPL1, reducing its size from 9 to 7, with Node 6 and Node 9 together with their links with other nodes (Edges 5->9, 6->5, 6->7) removed according to Equation (5), to produce the graph in (c). Note that the graph pooling ratio k here is set to 0.7. Similarly, the size of the graph is further abstracted and reduced through GCL2 and GPL2, respectively. A JKnet-style summary [35, 36] is subsequently applied to the graph after GPL1 in (c) and that after GPL2 in (e) by summing the concatenation of the mean and max pooling of all the nodes of each graph, respectively (Equation (6)), to generate the vector representation of the entire graph, which is finally used to predict the category of the input graph.
3.3.2 Graph Neural Network Training. Figure 9 shows a general structure for training our neural networks. A batch of XFG with its edge and code token (node) embedding is input into the graph
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:13
Fig. 9. The structure of the graph neural network in DeepWukong.
neural network, consisting of several blocks of interleaved graph convolutional/pooling layers (conv-pool block), a graph readout layer, and a MLP. GNN input. The inputs of the neural network are a batch of XFG obtained from the previous steps, which can be divided into two parts: (1) the edges (i.e., structured information) and (2) the code tokens (node) embedding (i.e., unstructured information). GNN learning. The inputs are fed into the above-introduced graph neural network. In the graph convolutional layer, features are extracted from the node embedding and the edges of the input XFG. After each graph convolutional layer, we use top−k pooling [34] to reduce the size of the XFG. In our detection system, there can be multiple conv-pool blocks. A graph readout layer realized by the concatenate of mean pooling and max pooling is applied after each conv-pool block to flatten multiple-node vectors of the graph into one graph vector. We then use a hidden fully connected layer after that, which allows high-order relationships between the features to be detected. Finally, a softmax layer is used to output the probabilities of class labels. It is worth noting that most of the parameters in the neural network are updated automatically by back-propagation during training. At last, we get a well-trained graph neural network model encoded with vulnerability patterns after GNN learning. We then use the model for further vulnerability detection.
4 EXPERIMENTAL EVALUATION
We seek to evaluate the effectiveness of DeepWukong on detecting the top 10 most common C/C++ vulnerabilities, compared with four traditional static vulnerability detectors and three stateof-the-art deep-learning-based approaches. To this end, we first introduce our dataset extracted from real-world vulnerabilities and then depict how they are labeled (Section 4.1). Next, we detail our experimental setup and model training (Section 4.2). Finally, we present our experimental results and observations (Section 5).
4.1 Dataset
4.1.1 Target Vulnerabilities. We have established our dataset based on two sources: (1) Software Assurance Reference Dataset (SARD) [22], a widely used vulnerability database, and (2) two real-world open-source projects (lua and redis). The statistics of the vulnerable programs in our dataset are shown in Table 2. SARD. We have harvested a comprehensive vulnerability benchmark dataset from SARD [22], which hosts a large number of known real-world security flaws. It is widely used to evaluate the performance of vulnerability detection approaches in our community [18, 38, 39]. In the SARD dataset, each program (i.e., test case) corresponds to one or more CWE IDs, as multiple types of
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:14 X. Cheng et al.
Table 2. The Statistics of Our Datasets
Vulnerability Category #LOI #Pointer #Object #Call |V| (ICFG) |E| (ICFG) |V| (VFG) |E| (VFG) CWE119 7,811,996 2,992,105 597,759 84,857 2,657,488 2,962,382 2,333,565 2,526,699 CWE20 7,728,477 3,205,748 601,656 65,394 2,828,956 3,133,667 2,420,491 2,471,687 CWE125 1,528,454 595,807 118,084 16,704 532,091 595,730 466,155 479,682 CWE190 1,611,932 653,629 108,248 18,250 612,164 690,579 496,601 512,800 CWE22 7,728,477 3,205,748 601,656 65,394 2,828,956 3,133,667 2,420,491 2,471,687 CWE399 5,552,545 1,895,244 359,892 63,500 1,780,518 2,013,559 1,419,467 1,566,615 CWE787 5,525,537 2,129,251 425,057 60,567 1,884,266 2,098,410 1,663,824 1,813,045 CWE254 14,576,245 5,079,282 965,625 151,987 4,535,930 5,096,619 3,898,307 4,102,872 CWE400 1,210,852 458,445 71,088 19,654 428,373 495,526 331,910 337,427 CWE78 703,660 290,148 50,478 5,383 242,148 263,665 217,122 200,428
lua 68,222 52,585 1,869 4,764 59,055 72,829 100,701 106,830 redis 735,275 401,520 14,461 119,216 45,737 56,505 654,495 778,854
Total 50,067,406 18,337,496 3,424,166 625,006 16,118,260 18,066,536 14,443,657 15,342,334
#LOI denotes the number of lines of LLVM instructions. #Pointer, #Object, #Call represent the numbers of pointers, objects and method calls, |V| (ICFG), |E| (ICFG), |V| (VFG) and |E| (VFG) are the numbers of ICFG nodes, ICFG edges, VFG nodes and VFG edges, respectively.
vulnerabilities could be identified in a program. We seek to study the top 10 most common C/C++ vulnerabilities as aforementioned in Section 1. Thus, we have implemented a crawler to harvest all the available programs related to the following vulnerabilities:
(1) CWE119: Improper Restriction of Operations within the Bounds of a Memory Buffer. The program reads from or writes to a memory location that is outside of the intended boundary of the memory buffer. (2) CWE20: Improper Input Validation. The program does not validate or incorrectly validates input that can affect the control-flow or data-flow of a program. (3) CWE125: Out-of-Bounds Read. The program reads data past the end, or before the beginning, of the intended buffer. (4) CWE190: Integer Overflow or Wraparound. The program performs a calculation that can produce an integer overflow or wraparound, when the logic assumes that the resulting value will always be larger than the original value.
(5) CWE22: Improper Limitation of a Pathname to a Restricted Directory. The program uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the software does not properly neutralize special elements within the pathname, which can cause the pathname to resolve to a location that is outside of the restricted directory. (6) CWE399: Resource Management Errors. It is related to improper management of system resources. (7) CWE787: Out-of-Bounds Write. The program writes data past the end, or before the beginning, of the intended buffer. (8) CWE254: Security Features. It is related to security-related operations, e.g., authentication, access control, confidentiality, cryptography, and privilege management.
(9) CWE400: Uncontrolled Resource Consumption. The program does not properly control the allocation and maintenance of a limited resource, thereby enabling an actor to
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:15
influence the amount of resources consumed, eventually leading to the exhaustion of available resources.
(10) CWE78: Improper Neutralization of Special Elements. The vulnerable program constructs all or part of an OS command using externally influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component.
Real-world open-source projects. To evaluate our approach on complex real-world opensource projects, we have further collected recent security-related commits of two open-source projects: redis-5.0.85 (a well-known database system server) and lua-5.3.46 (a widely used script language).
4.1.2 Labeling the Benchmark. As the vulnerabilities collected from SARD and real-world projects have different formats, we first present the labeling methods of them, respectively, and then show how we label XFG and method for further training and detecting. Labeling SARD. The programs we collect from SARD have already been labeled at the statement level as “good” (i.e., containing no security defect), “bad” (i.e., containing one or more specific security defects), or “mixed” (i.e., containing security defects and their fixed safe patches) using corresponding CWE IDs. Labeling real-world open-source projects. To label the benchmark in open-source projects, we adopt the methodology proposed by Zhou and Sharma [40]. To ensure the quality of our labeling process for the open-source projects, (1) we first extract bug-fixing commits by excluding the commits whose messages do not contain any bug-related keywords such as “bug,” “crash,” and “memory error,” and then (2) we manually check the modifications of each bug-fixing commit. Based on how the bug is fixed, we label safe statements as “0” from the fixed commit and vulnerable statements as “1” from its corresponding vulnerable commit. It took 720 hours for three experienced software engineers to carefully examine all the bug-related commits of our collected projects. Labeling XFG and method. Our vulnerability detection is performed at the XFG level rather than the method or file level, i.e., reporting whether an XFG (a program slice) is vulnerable or not. As described in Section 3.1, we first conduct program slicing for each program to generate a number of slices (i.e., XFG) provided that one program may have multiple points of interest, which determine the number of XFGs. Then, we further utilize the above-mentioned statementlevel labeling to label the vulnerable/safe XFG. Based on the labeling method of SARD, if a sample is extracted from a “good” program, we label it as “0” (i.e., safe), and if it is from a “bad” or “mixed” program, we label it as “1” (i.e., vulnerable) as long as it contains one vulnerable statement labeled by SARD, and “0” (i.e., safe) otherwise. Similarly, the XFG extracted from real-world open-source projects is considered to be vulnerable as long as it covers at least one statement of vulnerable code. Note that some existing learning-based approaches [15, 16] perform vulnerability detection at the method level; we further process the sample code and flag the vulnerabilities also at the method level (i.e., the method is marked as vulnerable if there is a vulnerable statement in the method) in order to compare with these existing approaches (cf. Section 5.3).
Example 4.1. Take the code fragment7 in Figure 10 as an example; it is an example to demonstrate the principle of CWE119. The figure depicts the content of source code and its label (i.e., “bad” at Statement 5) with the corresponding CWE ID (i.e., CWE119). Two XFGs extracted from the code
5https://redis.io/. 6https://www.lua.org/. 7https://cwe.mitre.org/data/definitions/119.html.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:16 X. Cheng et al.
Fig. 10. An example to show the labeling process of XFG. The source code shown on the left side is an example of CWE119, which shows the location (i.e., statement 5) of the weaknesses included. The XFGs generated from the source code are presented on the right side including one “0” (i.e., safe) XFG (without statement 5) and one “1” (i.e., vulnerable) XFG (with statement 5).
fragment are shown on the right side. The program points of interest of them are Statements 5 and 7, respectively. Accordingly, we label the XFG on the right as “1” (i.e., vulnerable) as it contains a vulnerable statement at Statement 5, and the XFG on the left as “0” (i.e., safe), because all the statements (Statements 1, 3, 6, 7) are not vulnerable.
4.1.3 Handling Duplicate/Conflict Samples. It is possible that there exist duplicate XFGs because different programs may have the same code logic leading to the same sliced statements and structure. The duplicate samples could lead to the overfitting problem and artificially inflate performance metrics because training data can leak into validation. We remove duplicate XFGs in our dataset by comparing their nodes, which are sorted by line number, and edges in space and time proportional to the size of an XFG. It is also possible that a few XFGs are labeled with both “1” and “0” because of the mislabeling from SARD, as mentioned by previous work [18]. In this situation, we simply remove all the conflict samples.
4.1.4 Distribution of the Benchmark. From the SARD dataset, we collect 102,696 and 1,408 vulnerable and safe programs, respectively (cf. Table 3). After program slicing, we have labeled 156,195 vulnerable XFGs and 409,262 safe XFGs. Each XFG contains 11 nodes (i.e., statements) on average, with respect to an average of 50 nodes and 217 statements for each ICFG and file of a program, respectively, which further indicates that our XFG is fine-grained. For the two real-world open-source projects redis and lua, we have collected 885 and 311 bug-fixing commits, respectively, from which we have produced and labeled 1,323 vulnerable XFGs and 2,848 safe XFGs (cf. Table 4). Each XFG contains 210 nodes (i.e., statements) on average with an average of 423 nodes and 1,004 statements for each ICFG and file of a program, respectively. Comparing with existing studies, the benchmarks we collected are large and comprehensive to perform evaluation.
4.2 Experimental Setup
Experimental environment and neural network configuration. The experiments are performed on a machine with NVIDIA GeForce GTX 1080 GPU and Intel Xeon E5-1620 CPU operating at 3.50GHz. The graph neural networks are implemented using PyTorch Geometric [41]. We perform experiments separately on each type of vulnerabilities, i.e., training a model for each of the 10 types of the vulnerabilities. We randomly choose 80% of the programs for training and
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:17
Table 3. Distribution of Labeled Samples from SARD
Vulnerability Category Granularity # Vulnerable Samples # Safe Samples # Total
CWE119
testcase 18,928 360 19,288 XFG 21,508 50,052 71,560 method 18,311 101,803 120,114 slice 8,853 11,700 20,553
CWE20
testcase 19,617 404 20,021 XFG 35,746 106,624 142,370 method 26,397 16,8245 194,642 slice 8,112 24,806 32,918
CWE125
testcase 3,645 0 3,645 XFG 3,677 8,187 11,864 method 3,536 20,818 24,354 slice 1,310 2,061 3,371
CWE190
testcase 1,052 149 1,201 XFG 2,555 6,076 8,631 method 3,759 24,994 28,753 slice 188 1,280 1,468
CWE22
testcase 2,764 5 2,769 XFG 5,165 6,857 12,022 method 4,692 22,141 26,833 slice 1,426 1,786 3,212
CWE399
testcase 12,367 24 12,391 XFG 14,060 38,001 52,061 method 12,669 86,043 98,712 slice 2,521 5,937 8,458
CWE787
testcase 12,946 48 12,994 XFG 14,419 31,535 45,954 method 12,477 62,670 75,147 slice 7,417 8,451 15,868
CWE254
testcase 27,776 409 28,185 XFG 49,569 134,232 183,801 method 38,603 229,942 268,545 slice 12,791 31,179 43,970
CWE400
testcase 1,872 0 1,872 XFG 6,677 22,671 29,348 method 3,875 31,629 35,504 slice 485 2,337 2,822
CWE78
testcase 1,729 9 1,738 XFG 2,819 5,027 7,846 method 4,719 26,429 31,148 slice 305 5,285 5,590
TOTAL
testcase 102,696 1,408 104,104 XFG 156,195 409,262 565,457 method 129,038 774,714 903,752 slice 43,408 94,822 138,230
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:18 X. Cheng et al.
Table 4. Distribution of Labeled Samples from Open-source Project
Project Granularity # Vulnerable Samples # Safe Samples # Total
redis
XFG 737 1,270 2,007 method 1,877 2,606 4,483 slice 512 340 852
lua
XFG 586 1,578 2,164 method 898 1,072 1,970 slice 601 987 1,588
TOTAL
XFG 1,323 2,848 4,171 method 2,775 3,678 6,453 slice 1,113 1,327 2,440
the remaining 20% for detecting. The neural networks are trained in a batch-wise fashion and the batch size is set to 64. We adopt a 10-fold cross-validation to train the neural network. The dimension of the vector representation of each node is set to 64. The dropout is set to 0.5 and the number of epochs is set to 50. The minibatch stochastic gradient descent together with ADAM [42] is used for training with the learning rate of 0.001. We use grid search to perform hyperparameter (e.g., batch size, dropout, learning rate) tuning in order to determine the optimal values for a given model. To prevent under- and over-fitting problems, we use a different number of convolutional layers on the dataset according to the size of vulnerability category (5 for CWE20 and CWE254; 4 for CWE119; 3 for CWE399, CWE787, and CWE400; and 2 for CWE125, CWE190, CWE22, CWE78, and open-source projects). The parameter k in k-GNNs is set to 3 and the graph pooling ratio is set to 0.8. The other parameters of our neural network are tuned in a standard method. All network weights and biases are randomly initialized using the default Torch initialization. Evaluation metrics. We apply six widely used metrics [43], including accuracy (ACC), falsepositive rate (FPR), false-negative rate (FNR), true-positive rate (TPR), Precision (P), F1 Score (F1), and AUC, to evaluate the performance of DeepWukong and the other competitors. We also adopt informedness (IFN) and markedness (MKN) [44], which are unbiased variants of recall and precision, respectively, to conduct our evaluation.
4.3 Research Questions
Our evaluation aims to answer the following four research questions:
RQ1 Can DeepWukong accurately detect vulnerabilities? In particular, we would like to further investigate (RQ1.1), can DeepWukong achieve consistently good performance across various types of vulnerabilities? and (RQ1.2), are there any performance differences when using three types of deep graph neural networks, and to what extent? RQ2 Can DeepWukong outperform traditional bug detection tools? RQ3 Can DeepWukong outperform existing deep-learning-based vulnerability detection approaches? RQ4 Can DeepWukong be applied to real-world open-source applications effectively? SARD is a security benchmark whose overall complexity may not be comparable to realworld applications; therefore, we further compare DeepWukong with the other stateof-the-art approaches mentioned based on the dataset built on real-world open-source applications.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:19
Table 5. A Comparison of GNNs (Best Results Shown in Bold)
Category GNN IFN FPR FNR MKN ACC F1 AUC
Training Time(s)
Testing Time(s)
CWE119
GCN 0.921 0.027 0.052 0.916 0.966 0.943 0.996 26,125.6 99.1 GAT 0.920 0.031 0.049 0.909 0.964 0.941 0.995 26,129.4 100.5 k-GNNs 0.932 0.023 0.045 0.926 0.970 0.950 0.996 26,140.7 103.9
CWE20
GCN 0.921 0.026 0.053 0.905 0.967 0.935 0.996 64,522.9 246.2 GAT 0.924 0.028 0.048 0.902 0.967 0.935 0.995 64,481.4 238.2 k-GNNs 0.933 0.023 0.044 0.918 0.972 0.944 0.997 64,566.4 257.7
CWE125
GCN 0.926 0.029 0.046 0.917 0.966 0.946 0.995 2,443.9 10.1 GAT 0.894 0.039 0.067 0.885 0.953 0.924 0.989 2,443.1 10.1 k-GNNs 0.954 0.022 0.025 0.942 0.977 0.964 0.997 2,446.2 10.6
CWE190
GCN 0.869 0.036 0.095 0.874 0.946 0.909 0.988 1,955.2 7.8 GAT 0.900 0.033 0.066 0.895 0.957 0.928 0.991 1,953.6 7.6 k-GNNs 0.913 0.021 0.067 0.923 0.966 0.941 0.995 1,954.1 7.6
CWE22
GCN 0.958 0.021 0.021 0.958 0.979 0.976 0.998 2,198.9 7.9 GAT 0.969 0.016 0.015 0.967 0.984 0.982 0.999 2,186.3 6.5 k-GNNs 0.979 0.011 0.011 0.978 0.989 0.988 0.999 2,203.5 8.6
CWE399
GCN 0.907 0.026 0.067 0.905 0.963 0.931 0.994 15,614.1 59.3 GAT 0.848 0.036 0.116 0.859 0.943 0.892 0.987 15,602.6 57.9 k-GNNs 0.929 0.020 0.052 0.928 0.972 0.947 0.996 15,611.8 58.9
CWE787
GCN 0.913 0.025 0.063 0.917 0.963 0.941 0.995 13,245.1 52.9 GAT 0.893 0.028 0.079 0.903 0.956 0.930 0.992 13,242.8 52.7 k-GNNs 0.938 0.018 0.044 0.941 0.974 0.958 0.997 13,262.7 56.1
CWE254
GCN 0.905 0.027 0.069 0.903 0.962 0.930 0.995 83,234.9 319.7 GAT 0.903 0.025 0.056 0.922 0.964 0.947 0.994 83,150.6 317.9 k-GNNs 0.925 0.027 0.047 0.910 0.967 0.940 0.997 83,248.5 321.1
CWE400
GCN 0.936 0.015 0.049 0.933 0.977 0.949 0.967 11,627.5 44.4 GAT 0.933 0.016 0.052 0.934 0.976 0.949 0.996 11,620.1 41.7 k-GNNs 0.963 0.011 0.025 0.951 0.985 0.967 0.998 11,631.7 47.5
CWE78
GCN 0.918 0.043 0.039 0.905 0.959 0.943 0.995 1,458.9 5.9 GAT 0.907 0.043 0.050 0.899 0.954 0.937 0.994 1,458.6 6.1 k-GNNs 0.932 0.026 0.042 0.930 0.968 0.956 0.996 1,461.7 6.4
AVERAGE
GCN 0.917 0.027 0.055 0.913 0.965 0.940 0.995 22242.7 85.3
GAT 0.909 0.029 0.061 0.905 0.962 0.935 0.993 22226.9 83.9 k-GNNs 0.940 0.020 0.040 0.935 0.974 0.956 0.997 22252.7 87.8
5 RESULTS AND ANALYSIS
5.1 RQ1: The Performance of DeepWukong
Overall result. Table 5 shows the overall results on the benchmark programs in terms of the aforementioned evaluation metrics. In general, DeepWukong achieves very promising results. On average, the accuracy is 97.4% (ranging from 96.6% to 98.9%) and the F1 Score is 95.6% (ranging from 94.0% to 98.8%), across all the top 10 vulnerabilities. Though it is generally time-consuming for training, bug prediction and detection are quite fast. For example, the training time of the largest dataset (CWE254) is 83,248.5s, while it only takes 321.1s for detection.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:20 X. Cheng et al.
5.1.1 RQ1.1: Detection Result across Vulnerabilities. As shown in Table 5, the performance differences when detecting different types of vulnerabilities are marginal. Our tool performs the best on CWE22 vulnerability, with the F1 Score of 98.8%. DeepWukong achieves the worst result on CWE254 and CWE190, with a good enough F1 Score of 94.0% and 94.1%, respectively. We further manually examine a number of exceptional cases to explore the reasons leading to the difference (false positives and false negatives). We have identified the following two main reasons leading to the inaccurate prediction. First, the selected program points of interest are not perfect. We take advantage of program slicing to generate the XFG, seeking to cover as much vulnerable code as possible, and preserve the semantics of the vulnerabilities. However, it is quite possible that we may lose some vulnerabilities that are not associated with the program points of interest we select (cf. Section 3.1.2). Nevertheless, we have extended the existing work on selecting program points of interest and have a broader coverage than state-of-the-art work [18]. Second, some kinds of vulnerabilities may exhibit quite different behaviors (i.e., manifesting as different patterns on control- and data-flow), which makes it hard for us to learn their patterns, especially when considering the limited number of training samples. Nevertheless, the performance of DeepWukong is already good enough across all kinds of vulnerabilities that we considered in our evaluation.
ANSWER: DeepWukong is effective in automatically learning high-level semantic features from graph embedding, which shows very good performance in detecting all the top 10 vulnerabilities. A few exceptional cases are introduced by the program points of interest selection process and the limited number of training samples.
5.1.2 RQ1.2: A Comparison of Graph Neural Networks. Table 5 compares the results of different GNNs on the XFG classification. k-GNN performs the best among the three networks, although the other two networks have close results. There are mainly two reasons behind this. First, we perform vulnerability detection at the XFG level. An XFG (slice) contains several program statements, which is more fine-grained than the approach working at the method level. Thus, XFG is able to precisely preserve code features (control- and data-flows) only for bug-relevant code statements. Theoretically, k-GNN [25] works better with fine-grained structures of a given graph than the other two networks, which was shown by previous studies [25, 45]. In addition, GCN and GAT consider each node separately at the same level, while k-GNN acts better by capturing the subgraph structured information (e.g., “if-else” code block) that is not visible at the node level. As implied by the similar time complexity of these models, the training and testing time costs are close among these neural networks considering each type of vulnerability.
ANSWER: The performance of DeepWukong is not tied to a particular graph neural network, as all the three networks have shown promising results. k-GNN outperforms the other two graph neural networks slightly, as it can better capture both the structured and unstructured information of XFG.
5.2 RQ2: DeepWukong vs. Traditional Vulnerability Detection Tools
To answer the second research question, we select the four most popular open-source tools, i.e., Flawfinder [5], RATS [8], Clang Static Analyzer [2], and Infer [6], as the baselines for our comparison. These tools are widely used in the software engineering community for static bug detection [46, 47]. They claim that they can cover a wide range of bugs. By looking into the source code of these tools, we find that these frameworks can detect vulnerabilities based on predefined detecting rules. In general, they are reported to perform well on low-level well-defined bugs, e.g., buffer overflow, null pointer dereference, and format string problems.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:21
Fig. 11. Comparing DeepWukong with the state-of-the-art vulnerability detection approaches. The 10 subfigures represent the 10 types of vulnerabilities. For a given vulnerability, each cell represents the performance of the approach (y-axis) under the corresponding metrics (x-axis). The darker the cell (the higher the value), the better the performance. Note that, for the FPR and FNR, we present their additive inverse here, which represents 1-FPR and 1-FNR separately. Note that the minimum negative value we observed is larger than -0.3, so the range is set to [-0.3,1].
Table 6. Runtime Costs Comparing DeepWukong with State-of-the-Art Vulnerability Detection Approaches on CWE254
Time RATS Flawfinder
Clang Static Analyzer Infer Token-based VGDetector Vuldeepecker DeepWukong
Training time(s)
N/A N/A N/A N/A 135,973.1 96,388.1 40,072.2 83,248.5
Testing time(s)
488.7 451.4 25,584.8 28,537.7 540.6 383.1 172.1 321.1
N/A indicates the phase is not applicable to the approach.
Comparison method. To conduct a fair comparison, we have applied all these frameworks to detect vulnerabilities in our labeled benchmark at the XFG level; i.e., predicting an XFG is vulnerable if it contains at least one vulnerable statement flagged by these tools. Result. As shown in Figure 11, DeepWukong outperforms all these four tools with regard to the evaluation metrics. It is interesting to see that these static analysis tools have either high false positives or high false negatives. Taking CWE22 vulnerability as an example, over 83% of the samples reported by Flawfinder are false positives. For the CWE119 vulnerability, over 98% of the samples reported by RATS are false negatives. Among the four traditional tools, RATS and Flawfinder perform the worst. RATS has a high false-negative rate of 90% on average, while Flawfinder has a high false-positive rate of 64% on average. Clang Static Analyzer also poses a high false-negative rate of 79% on average, as the detecting rules are not complete and cannot detect high-level bugs. Infer, which relies on Separation logic [48] and Bi-abduction [49], performs relatively better, with a false-negative rate at 56% and a false-positive rate at 40%. As for the runtime performance (see Table 6), the runtime cost by RATS on CWE254 is 488.7s, similar to that of Flawfinder with a relatively lower figure standing at 451.4s, while Clang Static Analyzer and Infer spend significantly longer time, at 25,584.8s and 28,537.7s, respectively. DeepWukong uses 83,248.51s on training and 321.1s on testing.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:22 X. Cheng et al.
Analysis. This result suggests that these tools are hard to be adopted to detect various highlevel software vulnerabilities, despite the comparatively low runtime cost. The main reason leading to the result is that they highly rely on well-defined bug patterns or specifications. By manually checking their existing detecting rules, we find that most of the rules are quite simple and the number of rules is also limited. The real-world vulnerabilities, especially for emerging vulnerabilities, are far more complicated than the simple rules defined by these detection tools, which will greatly limit the usage scenarios of the traditional rule-based detection tools, as they are purely relying on experienced human experts and developers to craft the sophisticated analyzers.
ANSWER: Our experiment results suggest that the traditional rule-based detection approaches are not applicable to detecting real-world complicated vulnerabilities, while DeepWukong, which embeds control- and data-flow information via deep learning, is effective in pinpointing vulnerabilities in a general manner without the knowledge of any predefined anti-patterns.
5.3 RQ3: DeepWukong vs. Existing Deep-Learning-Based Approaches
5.3.1 Existing Deep-Learning-Based Approaches. We first summarize the following three representative deep-learning-based vulnerability detection approaches. Token-based embedding [16] was proposed to detect vulnerabilities by representing the source code as sequential tokens. It first generates a token sequence for each method and then embeds raw-text information via deep learning. As this framework is not open source, we have reimplemented this detection framework strictly following the approach described in White et al. [16] for comparison. It is worth mentioning that there exists duplicate samples in the dataset primarily due to shared dependencies of different testcases, so we perform a comparative experiment with or without duplicate methods by applying an LSTM neural network, as shown in Table 7. Taking CWE20, which contains the number of samples above average, as an example, we observed a significant decline after removing all duplicates, with the F1 Score dropping from 77.3% to 58.4%. Regarding accuracy, both figures are relatively high (above 90%) because of the imbalanced distribution of the dataset: the number of safe methods, at 774,714, is more than six times as many as that of vulnerable methods, at 129,038 (a random guess can achieve an accuracy of above 85%). As this framework performs detection at the method level, after removing all duplicates, we have applied it to 903K methods that we labeled (cf. Table 3) to train the classifier and perform bug prediction. VGDetector [15] extracts a control-flow graph of source code and uses graph convolutional network [23] to embed it in a compact and low-dimensional representation. It claims to be well performed on detecting control-flow-related bugs. Note that this framework only works at the method level; thus, we have applied it to all of the 903K CFGs we extract and label (cf. Table 3) to perform the model training and prediction. Vuldeepecker [18] is a recent approach that detects vulnerabilities in source code, which relies on data-dependence information to represent the source code. It first locates bug-related APIs and then extracts the data-flow-related program slices based on the parameters of these APIs, thus generating code slices by assembling API-related program slices [18] to enforce accurate bug detection. It uses code slice to pinpoint bugs, and the number of generated code slices depends on the number of APIs existing in the source code. Unfortunately, this framework is not open source. We have made our efforts to reproduce their results by reimplementing their approach by strictly following their algorithm [18]. Note that we have also identified that the dataset8 released by the author contains lots of duplicate samples (roughly 50% of the samples are duplicates), which
8https://github.com/CGCL-codes/VulDeePecker.
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:23
Table 7. A Comparative Experiment on Dataset with and without the Duplicate Samples Using Token-based Embedding
Category Duplicate Samples INF FPR FNR MKN ACC F1 CWE119 Without 0.332 0.017 0.651 0.690 0.887 0.481
With 0.605 0.032 0.312 0.752 0.915 0.728 CWE20 Without 0.434 0.012 0.554 0.769 0.912 0.584
With 0.727 0.024 0.249 0.765 0.945 0.773 CWE125 Without 0.217 0.003 0.780 0.812 0.883 0.351
With 0.757 0.012 0.231 0.826 0.925 0.806 CWE190 Without 0.557 0.014 0.429 0.808 0.932 0.682
With 0.770 0.019 0.211 0.805 0.948 0.809 CWE22 Without 0.479 0.004 0.517 0.851 0.906 0.643
With 0.733 0.015 0.252 0.836 0.912 0.804 CWE399 Without 0.561 0.012 0.427 0.820 0.935 0.693
With 0.834 0.031 0.135 0.837 0.942 0.867 CWE787 Without 0.325 0.015 0.660 0.705 0.879 0.480
With 0.544 0.035 0.421 0.689 0.913 0.688 CWE254 Without 0.323 0.017 0.660 0.690 0.878 0.480
With 0.569 0.019 0.412 0.726 0.902 0.666 CWE400 Without 0.534 0.009 0.457 0.831 0.942 0.669
With 0.773 0.015 0.212 0.818 0.943 0.813 CWE78 Without 0.499 0.004 0.497 0.873 0.921 0.659
With 0.756 0.012 0.232 0.845 0.933 0.815
inflate performance metrics, i.e., overfitting problem. We evaluate Vuldeepecker’s performance on the dataset with and without removing the duplicate samples. Table 8 shows the results of Vuldeepecker with and without duplicate samples. When including the duplicate samples, our implementation achieves similar results as that reported in their paper, which suggests that our reproduced framework correctly follows their algorithm. However, it is surprising to see that, after removing duplicate samples, the performance of Vuldeepecker decreases significantly; e.g., the F1 Score decreases from 92% to 72% for CWE119, and decreases from 95% to 66% for CWE399. More surprisingly, the F1 Score decreases by over 34% to 58% for CWE190. To conduct a fair comparison, we have removed all the duplicate samples to use the same experimental setup as DeepWukong. We have extracted 138,230 slices (cf. Table 3) to evaluate the performance of Vuldeepecker.
5.3.2 Result. As shown in Figure 11, DeepWukong outperforms all of the three referred deeplearning-based approaches when evaluating the 10 types of vulnerabilities against the evaluation metrics. Taking the CWE254 vulnerability as an example, the false-negative rate of the Token-based approach is roughly 66%, while the false-negative rate of our DeepWukong is only 5%. Regarding the result of the F1 Score, DeepWukong is roughly 46% higher than the Token-based approach. As for VGDetector, taking CWE399 as an example, the F1 Score of VGDetector is only 70%, roughly 25% lower than DeepWukong. The false-negative rate of VGDetector reaches 43%, which is 38% higher than our approach, showing that DeepWukong can precisely detect more bugs. Comparing with Vuldeepecker, taking CWE190 as an example, Vuldeepecker nearly misses half of all the
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:24 X. Cheng et al.
Table 8. A Comparative Experiment on Datasets with and without the Duplicate Samples Using Vuldeepecker
Category Duplicate Samples TPR FPR FNR P ACC F1 CWE119 Without 0.766 0.359 0.234 0.681 0.704 0.721
With 0.956 0.124 0.044 0.885 0.916 0.919 CWE20 Without 0.850 0.380 0.150 0.691 0.740 0.772
With 0.968 0.134 0.032 0.911 0.946 0.939 CWE125 Without 0.741 0.492 0.259 0.595 0.629 0.662
With 0.938 0.156 0.062 0.921 0.921 0.929 CWE190 Without 0.529 0.292 0.471 0.652 0.623 0.582
With 0.958 0.196 0.042 0.891 0.912 0.923 CWE22 Without 0.952 0.052 0.048 0.954 0.953 0.951
With 0.978 0.036 0.022 0.971 0.982 0.974 CWE399 Without 0.615 0.259 0.385 0.950 0.704 0.656
With 0.957 0.059 0.043 0.942 0.948 0.949 CWE787 Without 0.652 0.231 0.348 0.740 0.714 0.692
With 0.948 0.136 0.052 0.921 0.898 0.934 CWE254 Without 0.752 0.161 0.248 0.822 0.794 0.781
With 0.936 0.146 0.064 0.932 0.917 0.934 CWE400 Without 0.732 0.202 0.268 0.782 0.764 0.751
With 0.918 0.176 0.082 0.952 0.942 0.935 CWE78 Without 0.923 0.122 0.077 0.892 0.902 0.913
With 0.965 0.135 0.035 0.922 0.931 0.943
vulnerabilities with the false-negative rate of 47% and the F1 Score is only 58%. DeepWukong is roughly 36% higher than Vuldeepecker for the F1 Score and 40% lower for the false-negative rate. For the running time shown in Table 6, all these approaches generally have a high runtime cost for training but a low runtime cost for prediction. In particular, DeepWukong and Vuldeepecker take less time than the other two tools.
5.3.3 Analysis. The experimental results suggest that representing the source code using a single code feature, such as raw text (e.g., Token-based embedding), control-flow (e.g., VGDetector), or data-flow (e.g., Vuldeepecker), is not enough to detect a wide variety of vulnerabilities.
DeepWukong vs. Token-based Approach. Token-based embedding considers each method of a program as a single big code block (i.e., sequential tokens), while DeepWukong embeds both the textual and structured information of code. The real-world programs are complex, and only considering the code as a plain text will miss its data- and control-flow information and cannot precisely capture code semantics. DeepWukong vs. VGDetector. Using control-flow alone to represent source code is also not enough to detect all vulnerabilities. Compared with Token-based embedding, VGDetector splits source code into basic blocks and adds control-flow edges between basic blocks, but VGDetector does not conduct data-flow analysis, making the representation less comprehensive. It mainly focuses on dealing with control-flow-related vulnerabilities but cannot precisely capture data dependence, which is essential for vulnerabilities such as buffer overflows. DeepWukong vs. Vuldeepecker. Vuldeepecker [18] only considers data-flows while ignoring control-flow information. It claims to perform well on buffer errors and resource management errors because those vulnerabilities are highly related to data-flow. However, the approach is
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:25
Fig. 12. Comparing DeepWukong with our baselines on real-world complex applications. Subfigure (1) represents redis dataset, subfigure (2) represents lua dataset, and subfigure (3) represents the mixed dataset of redis and lua. For a given vulnerability, each cell represents the performance of the approach (y-axis) under the corresponding metrics (x-axis). The darker the cell, the better the performance. Note that we did not observe negative values for IFN and FNR, so the range is set to [0,1].
shown ineffective in detecting vulnerabilities related to control-flows such as Insufficient Control Flow Management vulnerability. Vuldeepecker also failed to capture the structured information, which can lead to imprecise code embedding. XFG designed in our approach could capture both interprocedural control- and data-flow information of a program. These comprehensive semantic features are highly related to vulnerability patterns, which can be used to detect both low-level vulnerabilities like memory errors and highlevel ones such as business logic errors.
ANSWER: DeepWukong preserves the comprehensive code features (i.e., interprocedural control- and data-flow) in the graph embedding, which is able to capture the common characteristics of vulnerabilities and greatly improve the performance of existing deep-learning-based approaches.
5.4 RQ4: DeepWukong on Real-World Open-Source Applications
Comparison method. To evaluate whether DeepWukong can be applied to real-world complex projects, we evaluate DeepWukong using two real-world open-source applications (Table 4). We compare DeepWukong with our baselines by randomly choosing 80% of the XFGs generated from these two open-source projects for training and the remaining XFGs for detection. We used the same comparison method described in Section 5.2 to compare DeepWukong with traditional bug detectors. Similarly, we also compared DeepWukong with the existing deep-learning-based approaches introduced in Section 5.3. To further understand whether DeepWukong can successfully discover real-world emerging vulnerabilities that are not in the SARD dataset, we evaluate DeepWukong by choosing the latest five security-related commits of each open-source project, from which we randomly select 50 vulnerable XFGs, and then input those samples into the model trained from SARD. Result and analysis. Figure 12 compares DeepWukong with all seven vulnerability detection approaches by using our vulnerable programs extracted from the two real-world projects. Obviously, DeepWukong outperforms all the other approaches with an average improvement of 61%, 57%, and 42% in terms of informedness, markedness, and F1 Score, respectively. Traditional vulnerability detection tools, not surprisingly, have high false-positive and falsenegative rates, showing that they cannot be used to tackle a wide variety of vulnerabilities in the wild, particularly those without well-defined specifications/patterns. The performance of the existing deep-learning-based approaches is also poorer than DeepWukong. The false-negative
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:26 X. Cheng et al.
rate of the Token-based approach on redis is 59%, which is about seven times higher than that of DeepWukong (8%). VGDetector reports a markedness of 64% on the mixed dataset, while the score for DeepWukong is 21% higher at 85%. As for Vuldeepecker, the F1 Score on the mixed dataset is merely 64% compared to 90% by DeepWukong. When detecting vulnerabilities in the two open-source projects using the model trained from SARD, we achieve an accuracy of 86%, demonstrating that DeepWukong is able to precisely capture real-world emerging vulnerabilities.
ANSWER: DeepWukong can effectively detect vulnerabilities in real-world open-source applications that require comprehensive code features and significantly boost the performance of both traditional and deep-learning-based vulnerability detecting approaches.
6 THREATS TO VALIDITY
First, the vulnerability labeling in this work might not be perfect. It is possible that some samples are mislabeled. Here, we trust the labeling results of SARD since they are labeled by domain experts. We also try our best to conduct the dataset labeling for the vulnerable patches from two real-world projects, which takes 720 hours by three experienced engineers. Second, our framework performs program slicing based on function calls and operator statements as the program points of interest. As aforementioned, it might not be perfect, as we may miss some corner cases. One way to improve our approach is to identify other types of program points of interest and filter irrelevant program points of interest to further eliminate possible noises in the training phase. Third, our experiments are limited to the top 10 vulnerabilities in C/C++ programs. However, it is easy to extend our methodology to support more vulnerabilities and other programming languages. Last, our approach only considers three state-of-the-art graph neural networks for code embedding; it is interesting to further explore more types of neural networks.
7 RELATED WORK
Static vulnerability detection. There are quite a few traditional static program analysis frameworks developed to process source code and report potential vulnerabilities in all kinds of software engineering systems (e.g., Clang Static Analyzer [2], Infer [6], SVF [10], Coverity [3], Fortify [4], Flawfinder [5], ITS4 [7], RATS [8], Checkmarx [9]). Besides, a number of academic research works [50–52] seek to detect specific vulnerabilities (mainly of memory errors and information leaks). These traditional approaches heavily rely on conventional static analysis theories (e.g., data-flow, abstract interpretation, and taint analysis) and need human experts to define effective detecting rules. The number of rules is limited and cannot cover all of the vulnerability patterns. As a result, they often suffer from high false positives and false negatives when detecting complex programs, as indicated in our evaluation (cf. Section 5.2). Similarity-based vulnerability detection. There is a line of research for detecting vulnerabilities by applying similarity analysis (e.g., code clone bugs) [53–55]. They normally represent the code fragment into an abstract representation and compute the similarity between pairs of abstractions. Then they set a similarity threshold and consider the target code as vulnerable if the similarity between the target code fragment and vulnerable ones is above the threshold. This method still requires humans to select appropriate code and extract features from it for comparison.
Machine -learning-based vulnerability detection. There is another line of research recently for automatically detecting vulnerabilities using machine learning. DeepBugs [56] represents code via text vector for detecting name-based bugs. Grieco et al. [11] uses lightweight static and dynamic features to detect memory corruption. Neuhaus et al. [12] use support vector machines (SVMs) to analyze code from Red hat packages. Yan et al. [57] perform machine-learning-guided type state
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:27
analysis for detecting use-after-frees. VGDetector [15] uses control-flow graph and graph convolutional network to detect control-flow-related vulnerabilities. Vuldeepecker [18] applies code embedding using data-flow information of a program for detecting resource management errors and buffer overflows. All these solutions can only detect specific well-defined vulnerabilities. Compared with these approaches that mainly focus on detecting limited types of bugs, our approach focuses on detecting multiple vulnerabilities (both low level and high level) with low false positive and negative rates. Machine learning for program analysis. A number of studies have been proposed to combine machine learning with program analysis to perform better code analysis. The existing approaches have defined sets of features that prove to be useful representing code. They can be mainly divided into four categories. There are approaches that extract features based on texts [58–61], tokens [62–65], ASTs [21, 66–68], and graphs [69–74]. For example, Wang et al. [67] proposed learning from token vectors extracted from Abstract Syntax Trees (ASTs) and use it to realize software defect prediction. White et al. [75] used a stream of identifiers and literal types to automatically encode code-for-code clone detection. Sui et al. [74] utilized a bag of paths from an interprocedural context-sensitive alias-aware value-flow-graph for code classification and summarization. These studies may have a correlation with part of our work, but with different goals. Most of them are focused on code clone detection and defect prediction, which is different from vulnerability detection in general. Compared with those code representations, XFG is more fine-grained (code statements vs. program level) and preserves as much high-level semantic information as possible. Nevertheless, the different code representation approaches might be a complement to our work. Devign [17] is a very recent work on deep-learning-based vulnerability detection. There are two major differences between Devign and DeepWukong. Their approach aims at pinpointing bugs at the method level, while DeepWukong has a finer granularity (i.e., at program slice level). Their analysis is intraprocedural, which does not support interprocedural analysis, while DeepWukong has conducted a more precise interprocedural analysis, which is essential for real-world programs since function calls are quite common in modern software projects.
8 CONCLUSION
In this article, we present DeepWukong, a new deep-learning-based approach that embeds both textual and code-structured features into an effective representation to support detection of a wide range of vulnerabilities. DeepWukong first performs program slicing to extract fine-grained but complicated semantic features, and then combines with graph neural networks to produce compact and low-dimensional representation. We have applied DeepWukong to over 100K vulnerable programs for the 10 most popular C/C++ vulnerabilities and 2 real-world open-source projects, and demonstrate that DeepWukong outperforms several state-of-the-art approaches, including traditional vulnerability detectors and deep-learning-based approaches.
APPENDIX
A GRAPH CONVOLUTIONAL LAYER
This section introduces the three graph convolutional networks we used in our graph convolutional layer.
A.1 Graph Convolutional Network (GCN)
GCN [23] scales linearly with respect to the number of graph edges. It learns hidden layer representations that encode both local graph structures and features of nodes. It introduces a simple and well-behaved layer-wise propagation rule for neural network models that operate directly
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:28 X. Cheng et al.
Fig. 13. Illustration of the layer-wise propagation rule of GCN.
on graphs. During our implementation, we consider a multilayer GCN with the following layer-wise propagation rule (illustrated by Figure 13):
F(l ) = σ
(D ̃− 1
2 A ̃D ̃− 1
2 F(l −1)W(l ) ) . (8)
F(l ) = { f (l ) (0), f (l ) (1), . . . , f (l ) (N )} ∈ RN ×d (l) means the matrix of activations in the lth layer of the neural network. For the first layer, F(0) = { f (0) (0), f (0) (1), . . . , f (0) (N )} is the node feature matrix of the graph.  ̃A = A + IN represents the adjacency matrix of the graph with added self
connections, where IN is the identity matrix. D ̃ii = ∑
j A ̃ij and W(l ) is the trainable weight matrix for the lth layer. We choose Rectified Linear activation function σ (·) = ReLU (·) = max (0, ·) as our activation function.
A.2 Graph Attention Network (GAT)
GAT [24] is a graph neural network that adopts self-attentional layers, and thus could encode the graph structured by specifying different weights to different neighbors without additional costs. Our approach is branch-aware and the neighboring relationship when meeting a branch condition different from the linearly executing circumstances, requiring (implicitly) different processing. GAT is good at recognizing the different structures of XFG by attending over the features of node neighborhoods. For the lth layer, we first do a shared linear transformation parameterized by a weight matrix, W(l ), for every node. Then the attention coefficients are computed by a shared attentional mechanism, a(l ) : Rd (l−1)×d (l) → R. We only compute attention coefficients e (l )
ij for nodes j ∈ N (i), where N (i) is the neighbor nodes set of node i, indicating the importance of node j’s features to node i:
e (l )
ij = a(l ) (
f (l−1) (i ) · W(l ), f (l−1) (j ) · W(l ) )
= LeakyReLU (
[
f (l−1) (i ) · W(l ) ‖ f (l−1) (j ) · W(l ) ]
a(l ) ). (9)
Here ‖ is the concatenation operation. The attention mechanism a(l ) is a single-layer feedforward network, parameterized by a weight vector a(l ) ∈ R2d (l) , and then applying the LeakyReLU nonlinearity function. In order to make coefficients easily comparable across different nodes, we normalize them across all choices of node j by applying the softmax function (illustrated by Figure 14 (left)):
α (l )
ij = exp (e (l )
ij )
∑
k ∈N (i ) exp (e (l )
ik )
= exp (LeakyReLU ([f (l−1) (i )W(l ) ‖ f (l−1) (j )W(l )]a(l ) ))
∑
k ∈N (i) exp (LeakyReLU ([f (l−1) (i )W(l ) ‖ f (l−1) (k )W(l )]a(l ) ))
. (10)
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:29
Fig. 14. Illustration of the attention mechanism (left) and propagation rule (right) of GAT.
Fig. 15. Illustration of the neighbouring of 1-GNN, 2-GNN, and 3-GNN.
Then, we consider a multilayer GAT with the following layer-wise propagation rule, where σ (·) denotes an activation function (illustrated by Figure 14 (right)):
f (l ) (i ) = σ
∑
j ∈N (i )
α (l )
ij f (l−1) (j ) · W(l ) . (11)
A.3 k-dimensional GNN (k-GNN)
k-GNN [25] is a recent neural network improving handling of higher-order graph structure by taking multiple scales into account. The node of XFG is a single statement of the program, making XFG a molecule graph because the size of each node is tiny. In addition, a program can be divided into multiple code blocks (e.g., if-else block, loop block) and each block can be separately considered as a logic unit. The program performs message passing between those code blocks through control or data dependence, making the graph representation of the program (i.e., XFG) have a higher-order graphic structure. k-GNN [25] works well with the fine- and coarse-grained structures of a given graph and can capture the code block structured information that is not visible at the single statement level. k-GNNs have the same expressiveness as the k-dimensional Weisfeiler-Leman graph isomorphism heuristic. We denote the set of nodes and the set of edges of G by V (G) and E (G), respectively. The number of nodes is N = |V (G)|. For a given k, [V (G)]k = {U ⊆ V (G)| |U | = k } over V (G) represents all k-element subsets. Let s = {s1, . . . , sk } be a k-set in [V (G)]k . The neighborhood of s is defined as (illustrated by Figure 15)
N (s) = {
t ∈ [V (G)]k | |s ∩ t | = k − 1
}. (12)
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:30 X. Cheng et al.
Further, the local neighborhood NL (s) consists of all t ∈ N (s) such that (v, w ) ∈ E (G) for the unique v ∈ s \ t and the unique w ∈ t \ s. The global neighborhood NG (s) then is defined as N (s) \ NL (s ).
In each k-GNN layer l ≥ 0, we compute a feature vector f (l )
k (s) for each k-set s in [V (G)]k . For
l = 0, we set f (0)
k (s) by pooling the feature vectors of each node in the k-set. In each layer l > 0,
we compute new features by
f (l )
k (s ) = σ f (l−1)
k (s ) · W(l )
1+
∑
u ∈NL (s )∪NG (s )
f (l −1)
k (u) · W(l )
2 . (13)
Moreover, to scale k-GNNs to larger datasets and to prevent overfitting, we omit the global neighborhood of s; i.e., the final propagation rule is
f (l )
k,L (s ) = σ f (l−1)
k,L (s ) · W(l )
1+
∑
u ∈NL (s )
f (l −1)
k,L (u ) · W(l )
2 . (14)
REFERENCES
[1] American Information Technology Laboratory. 2020. National Vulnerability Database. https://nvd.nist.gov. [2] Apple Inc. 2020. Clang static analyzer. https://clang-analyzer.llvm.org/scan-build.html. [3] Synopsys. 2020. Coverity. https://scan.coverity.com/. [4] Micro Focus. 2020. HP Fortify. https://www.hpfod.com/. [5] David A. Wheeler. 2020. Flawfinder. https://dwheeler.com/flawfinder/. [6] Facebook. 2020. Infer. https://fbinfer.com/. [7] J. Viega, J. T. Bloch, Y. Kohno, and G. McGraw. 2000. ITS4: A static vulnerability scanner for C and C++ code. In Proceedings 16th Annual Computer Security Applications Conference (ACSAC’00). 257–267. DOI:http://dx.doi.org/10. 1109/ACSAC.2000.898880 [8] Inc Secure Software. 2014. RATS. https://code.google.com/archive/p/rough-auditing-tool-for-security/. [9] Israel. 2020. Checkmarx. https://www.checkmarx.com/. [10] Yulei Sui and Jingling Xue. 2016. SVF: Interprocedural Static Value-Flow Analysis in LLVM. https://github.com/unswcorg/SVF. In Proceedings of the 25th International Conference on Compiler Construction (CC’16). 265–266.
[11] Gustavo Grieco, Guillermo Luis Grinblat, Lucas Uzal, Sanjay Rawat, Josselin Feist, and Laurent Mounier. 2016. Toward large-scale vulnerability discovery using machine learning. In Proceedings of the 6th ACM Conference on Data and Application Security and Privacy (CODASPY’16). ACM, New York, NY, 85–96. DOI:http://dx.doi.org/10.1145/2857705. 2857720 [12] Stephan Neuhaus and Thomas Zimmermann. 2009. The beauty and the beast: Vulnerabilities in red hat’s packages. In Proceedings of the 2009 USENIX Annual Technical Conference (USENIX ATC’09).
[13] Stephan Neuhaus, Thomas Zimmermann, Christian Holler, and Andreas Zeller. 2007. Predicting vulnerable software components. In Proceedings of the 14th ACM Conference on Computer and Communications Security (CCS’07). Association for Computing Machinery, New York, NY, 529–540. DOI:http://dx.doi.org/10.1145/1315245.1315311 [14] Yonghee Shin, Andrew Meneely, Laurie Williams, and Jason A. Osborne. 2011. Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities. IEEE Trans. Software Eng. 37 (2011), 772–787. DOI:http://dx.doi.org/10.1109/TSE.2010.81 [15] X. Cheng, H. Wang, J. Hua, M. Zhang, G. Xu, L. Yi, and Y. Sui. 2019. Static detection of control-flow-related vulnerabilities using graph embedding. In 2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS’19). 41–50. DOI:http://dx.doi.org/10.1109/ICECCS.2019.00012 [16] Martin White, Christopher Vendome, Mario Linares-Vásquez, and Denys Poshyvanyk. 2015. Toward deep learning software repositories. In Proceedings of the 12th Working Conference on Mining Software Repositories (MSR’15). IEEE Press, Piscataway, NJ, 334–345. http://dl.acm.org/citation.cfm?id=2820518.2820559 [17] YaQin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019 (NeurIPS’19).
[18] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong. 2018. VulDeePecker: A deep learning-based system for vulnerability detection. In The Network and Distributed System Security Symposium (NDSS’18).
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:31
[19] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (NIPS’13). Curran Associates Inc., 3111–3119. http://dl.acm.org/citation.cfm?id=2999792. 2999959 [20] Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31th International Conference on Machine Learning (ICML’14), Vol. 32. JMLR.org, 1188–1196. http://dblp.uni-trier.de/ db/conf/icml/icml2014.html#LeM14 [21] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2017. Learning to represent programs with graphs. CoRR abs/1711.00740 (2017). arxiv:1711.00740 http://arxiv.org/abs/1711.00740 [22] American Information Technology Laboratory. 2017. Software Assurance Reference Dataset. https://samate.nist.gov/ SARD/index.php. [23] Thomas N. Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. CoRR abs/1609.02907 (2016). arxiv:1609.02907 http://arxiv.org/abs/1609.02907 [24] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In International Conference on Learning Representations. https://openreview.net/forum? id=rJXMpikCZ. Accepted as poster. [25] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2018. Weisfeiler and Leman Go Neural: Higher-order graph neural networks. CoRR abs/1810.02244 (2018). arxiv:1810.02244 http://arxiv.org/abs/1810.02244 [26] Keshav Pingali and Gianfranco Bilardi. 1995. APT: A data structure for optimal control dependence computation. In Proceedings of the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation (PLDI’95). ACM, New York, NY, 32–46. DOI:http://dx.doi.org/10.1145/207110.207114 [27] Ben Hardekopf and Calvin Lin. 2011. Flow-sensitive pointer analysis for millions of lines of code. In Proceedings of the 9th Annual IEEE/ACM International Symposium on Code Generation and Optimization (CGO’11). IEEE Computer Society, 289–298. [28] Crispin Cowan, Calton Pu, Dave Maier, Heather Hintony, Jonathan Walpole, Peat Bakke, Steve Beattie, Aaron Grier, Perry Wagle, and Qian Zhang. 1998. StackGuard: Automatic adaptive detection and prevention of buffer-overflow attacks. In Proceedings of the 7th Conference on USENIX Security Symposium - Volume 7 (SSYM’98). USENIX Association, Berkeley, CA, 5–5. http://dl.acm.org/citation.cfm?id=1267549.1267554 [29] C. Cowan, F. Wagle, Calton Pu, S. Beattie, and J. Walpole. 2000. Buffer overflows: Attacks and defenses for the vulnerability of the decade. In Proceedings DARPA Information Survivability Conference and Exposition (DISCEX’00), Vol. 2. 119–129. DOI:http://dx.doi.org/10.1109/DISCEX.2000.821514 [30] Terence Parr. 2014. Antlr. https://www.antlr.org/.
[31] Mark Weiser. 1981. Program slicing. In Proceedings of the 5th International Conference on Software Engineering (ICSE’81). IEEE Press, 439–449. [32] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish K. Shevade. 2017. DeepFix: Fixing common C language errors by deep learning. In Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI’17). 1345–1351. http: //aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603 [33] H. Robbins. 2007. A stochastic approximation method. Annals of Mathematical Statistics 22 (2007), 400–407. [34] Caˇtaˇline Cangea, Petar Veličković, Nikola Jovanović, Thomas Kipf, and Pietro Liò. 2018. Towards Sparse Hierarchical Graph Classifiers. arxiv:stat.ML/1811.01287 [35] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs with jumping knowledge networks. CoRR abs/1806.03536 (2018). arxiv:1806.03536 http://arxiv.org/abs/1806.03536 [36] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural networks?CoRR abs/1810.00826 (2018). arxiv:1810.00826 http://arxiv.org/abs/1810.00826 [37] H. Ramchoun, M. A. Janati Idrissi, Y. Ghanou, and M. Ettaouil. 2017. Multilayer perceptron: Architecture optimization and training with mixed activation functions. In Proceedings of the 2nd International Conference on Big Data, Cloud and Applications (BDCA’17). Association for Computing Machinery, New York, NY, Article 71, 6 pages. DOI: http://dx.doi.org/10.1145/3090354.3090427 [38] Shadi A. Aljawarneh, Ali Alawneh, and Reem Jaradat. 2017. Cloud security engineering: Early stages of SDLC. Future Generation Computer Systems 74 (2017), 385–392. DOI:http://dx.doi.org/10.1016/j.future.2016.10.005 [39] H. H. AlBreiki and Q. H. Mahmoud. 2014. Evaluation of static analysis tools for software security. In 2014 10th International Conference on Innovations in Information Technology (IIT’14). 93–98. DOI:http://dx.doi.org/10.1109/ INNOVATIONS.2014.6987569 [40] Yaqin Zhou and Asankhaya Sharma. 2017. Automated identification of security issues from commit messages and bug reports. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE’17). Association for Computing Machinery, New York, NY, 914–919. DOI:http://dx.doi.org/10.1145/3106237.3117771
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


38:32 X. Cheng et al.
[41] Matthias Fey and Jan E. Lenssen. 2019. Fast graph representation learning with PyTorch geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds.
[42] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations (ICLR’15), Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http:// arxiv.org/abs/1412.6980 [43] Marcus Pendleton, Richard Garcia-Lebron, Jin-Hee Cho, and Shouhuai Xu. 2016. A survey on systems security metrics. ACM Computing Surveys 49, 4, Article 62 (Dec. 2016), 35 pages. DOI:http://dx.doi.org/10.1145/3005714 [44] D. M. W. Powers. 2011. Evaluation: From precision, recall and F-measure to ROC, informedness, markedness & correlation. Journal of Machine Learning Technologies 2, 1 (2011), 37–63.
[45] Petra Mutzel. 2019. Algorithmic data science (invited talk). In 36th International Symposium on Theoretical Aspects of Computer Science (STACS’19) (Leibniz International Proceedings in Informatics (LIPIcs)), Rolf Niedermeier and Christophe Paul (Eds.), Vol. 126. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany, 3:1–3:15. DOI:http://dx.doi.org/10.4230/LIPIcs.STACS.2019.3 [46] Yves Younan, Wouter Joosen, and Frank Piessens. 2004. Code Injection in C and C++: A Survey of Vulnerabilities and Countermeasures. Technical Report. Departement Computerwetenschappen, Katholieke Universiteit Leuven. [47] 2000. The UC Davis Reducing Software Security Risk through an Integrated Approach Project. Computer Security Laboratory, Department of Computer Science, University of California at Davis. http://seclab.cs.ucdavis.edu/projects/ testing/. [48] Josh Berdine, Cristiano Calcagno, and Peter W. O’Hearn. 2006. Smallfoot: Modular automatic assertion checking with separation logic. In Formal Methods for Components and Objects, Frank S. de Boer, Marcello M. Bonsangue, Susanne Graf, and Willem-Paul de Roever (Eds.). Springer, Berlin,115–137. [49] Cristiano Calcagno, Dino Distefano, Peter O’Hearn, and Hongseok Yang. 2009. Compositional shape analysis by means of bi-abduction. In Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL’09). ACM, New York, NY, 289–300. DOI:http://dx.doi.org/10.1145/1480881.1480917 [50] Fabian Yamaguchi, Christian Wressnegger, Hugo Gascon, and Konrad Rieck. 2013. Chucky: Exposing missing checks in source code for vulnerability discovery. In Proceedings of the 2013 ACM SIGSAC Conference on Computer & Communications Security (CCS’13). ACM, New York, NY, 499–510. DOI:http://dx.doi.org/10.1145/2508859.2516665 [51] F. Yamaguchi, A. Maier, H. Gascon, and K. Rieck. 2015. Automatic inference of search patterns for taint-style vulnerabilities. In 2015 IEEE Symposium on Security and Privacy (SP’15). 797–812. DOI:http://dx.doi.org/10.1109/SP.2015.54 [52] M. Backes, B. Köpf, and A. Rybalchenko. 2009. Automatic discovery and quantification of information leaks. In 2009 IEEE Symposium on Security and Privacy (SP’09). 141–153. DOI:http://dx.doi.org/10.1109/SP.2009.18 [53] J. Jang, A. Agrawal, and D. Brumley. 2012. ReDeBug: Finding unpatched code clones in entire OS distributions. In 2012 IEEE Symposium on Security and Privacy (SP’12). 48–62. DOI:http://dx.doi.org/10.1109/SP.2012.13
[54] S. Kim, S. Woo, H. Lee, and H. Oh. 2017. VUDDY: A scalable approach for vulnerable code clone discovery. In 2017 IEEE Symposium on Security and Privacy (SP’17). 595–614. DOI:http://dx.doi.org/10.1109/SP.2017.62 [55] Nam H. Pham, Tung Thanh Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen. 2010. Detection of recurring software vulnerabilities. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering (ASE’10). ACM, New York, NY, 447–456. DOI:http://dx.doi.org/10.1145/1858996.1859089 [56] Michael Pradel and Koushik Sen. 2018. DeepBugs: A learning approach to name-based bug detection. Proceedings of the ACM on Programming Languages 2, OOPSLA, Article 147 (Oct. 2018), 25 pages. DOI:http://dx.doi.org/10.1145/ 3276517 [57] Hua Yan, Yulei Sui, Shiping Chen, and Jingling Xue. 2017. Machine-learning-guided typestate analysis for static useafter-free detection. In Proceedings of the 33rd Annual Computer Security Applications Conference (ACSAC’17). ACM, 42–54. [58] J. Howard Johnson. 1993. Identifying redundancy in source code using fingerprints. In Proceedings of the 1993 Conference of the Centre for Advanced Studies on Collaborative Research: Software Engineering - Volume 1 (CASCON’93). IBM Press, 171–183. http://dl.acm.org/citation.cfm?id=962289.962305 [59] J. Howard Johnson. 1994. Substring matching for clone detection and change tracking. In Proceedings 1994 International Conference on Software Maintenance. 120–126. DOI:http://dx.doi.org/10.1109/ICSM.1994.336783 [60] J. Howard Johnson. 1994. Visualizing textual redundancy in legacy source. In Proceedings of the 1994 Conference of the Centre for Advanced Studies on Collaborative Research (CASCON’94). IBM Press, 32–. http://dl.acm.org/citation.cfm? id=782185.782217 [61] C. K. Roy and J. R. Cordy. 2008. NICAD: Accurate detection of near-miss intentional clones using flexible prettyprinting and code normalization. In 2008 16th IEEE International Conference on Program Comprehension (ICPC’08). 172–181. DOI:http://dx.doi.org/10.1109/ICPC.2008.41
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.


DeepWukong: Statically Detecting Software Vulnerabilities 38:33
[62] T. Kamiya, S. Kusumoto, and K. Inoue. 2002. CCFinder: A multilinguistic token-based code clone detection system for large scale source code. IEEE Transactions on Software Engineering 28, 7 (2002), 654–670. DOI:http://dx.doi.org/10. 1109/tse.2002.1019480 [63] Z. Li, S. Lu, S. Myagmar, and Y. Zhou. 2006. CP-miner: Finding copy-paste and related bugs in large-scale software code. IEEE Transactions on Software Engineering 32, 3 (March 2006), 176–192. DOI:http://dx.doi.org/10.1109/TSE.2006. 28 [64] H. Sajnani and C. Lopes. 2013. A parallel and efficient approach to large scale clone detection. In 2013 7th International Workshop on Software Clones (IWSC’13). 46–52. DOI:http://dx.doi.org/10.1109/IWSC.2013.6613042 [65] Hitesh Sajnani, Vaibhav Saini, Jeffrey Svajlenko, Chanchal K. Roy, and Cristina V. Lopes. 2016. SourcererCC: Scaling code clone detection to big-code. In Proceedings of the 38th International Conference on Software Engineering (ICSE’16). ACM, New York, NY, 1157–1168. DOI:http://dx.doi.org/10.1145/2884781.2884877 [66] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. A novel neural source code representation based on abstract syntax tree. In Proceedings of the 41st International Conference on Software Engineering (ICSE’19). IEEE Press, Piscataway, NJ, 783–794. DOI:http://dx.doi.org/10.1109/ICSE.2019.00086 [67] Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic features for defect prediction. In Proceedings of the 38th International Conference on Software Engineering (ICSE’16). ACM, New York, NY, 297–308. DOI:http://dx.doi.org/10.1145/2884781.2884804 [68] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. Code2Vec: Learning distributed representations of code. Proceedings of the ACM on Programming Languages 3, POPL, Article 40 (Jan. 2019), 29 pages. DOI:http://dx.doi. org/10.1145/3290353 [69] Kai Chen, Peng Liu, and Yingjun Zhang. 2014. Achieving accuracy and scalability simultaneously in detecting application clones on Android markets. In Proceedings of the 36th International Conference on Software Engineering (ICSE’14). ACM, New York, NY, 175–186. DOI:http://dx.doi.org/10.1145/2568225.2568286 [70] Mark Gabel, Lingxiao Jiang, and Zhendong Su. 2008. Scalable detection of semantic clones. In Proceedings of the 30th International Conference on Software Engineering (ICSE’08). ACM, New York, NY, 321–330. DOI:http://dx.doi.org/10. 1145/1368088.1368132 [71] Raghavan Komondoor and Susan Horwitz. 2001. Using slicing to identify duplication in source code. In Static Analysis, Patrick Cousot (Ed.). Springer, Berlin, 40–56. [72] Jens Krinke. 2001. Identifying similar code with program dependence graphs. In Proceedings of the 8th Working Conference on Reverse Engineering (WCRE’01). IEEE Computer Society, Washington, DC, 301–. http://dl.acm.org/citation. cfm?id=832308.837142 [73] Chao Liu, Fen Chen, Jiawei Han, and Philip Yu. 2006. GPLAG: Detection of software plagiarism by program dependence graph analysis. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’06). 872–881. DOI:http://dx.doi.org/10.1145/1150402.1150522 [74] Yulei Sui, Xiao Cheng, Guanqin Zhang, and Haoyu Wang. 2020. Flow2Vec: Value-flow-based precise code embedding. Proceedings of the ACM on Programming Languages 4, OOPSLA, Article 233 (Nov. 2020), 27 pages. DOI:http://dx.doi. org/10.1145/3428301 [75] Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk. 2016. Deep learning code fragments for code clone detection. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering (ASE’16). ACM, New York, NY, 87–98. DOI:http://dx.doi.org/10.1145/2970276.2970326
Received January 2020; revised November 2020; accepted November 2020
ACM Transactions on Software Engineering and Methodology, Vol. 30, No. 3, Article 38. Pub. date: April 2021.