111
A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning
NIMA SHIRI HARZEVILI, York University, Canada ALVINE BOAYE BELLE, York University, Canada JUNJIE WANG, Institute of Software, Chinese Academy of Sciences, China SONG WANG, York University, Canada
ZHEN MING (JACK) JIANG, York University, Canada NACHIAPPAN NAGAPPAN, Meta, USA
Software vulnerability detection is critical in software security because it identifies potential bugs in software systems, enabling immediate remediation and mitigation measures to be implemented before they may be exploited. Automatic vulnerability identification is important because it can evaluate large codebases more efficiently than manual code auditing. Many Machine Learning (ML) and Deep Learning (DL) based models for detecting vulnerabilities in source code have been presented in recent years. However, a survey that summarises, classifies, and analyses the application of ML/DL models for vulnerability detection is missing. It may be difficult to discover gaps in existing research and potential for future improvement without a comprehensive survey. This could result in essential areas of research being overlooked or under-represented, leading to a skewed understanding of the state of the art in vulnerability detection. This work address that gap by presenting a systematic survey to characterize various features of ML/DL-based source code level software vulnerability detection approaches via five primary research questions (RQs). Specifically, our RQ1 examines the trend of publications that leverage ML/DL for vulnerability detection, including the evolution of research and the distribution of publication venues. RQ2 describes vulnerability datasets used by existing ML/DL-based models, including their sources, types, and representations, as well as analyses of the embedding techniques used by these approaches. RQ3 explores the model architectures and design assumptions of ML/DL-based vulnerability detection approaches. RQ4 summarises the type and frequency of vulnerabilities that are covered by existing studies. Lastly, RQ5 presents a list of current challenges to be researched and an outline of a potential research roadmap that highlights crucial opportunities for future work.
CCS Concepts: • Security and privacy → Software security engineering.
Additional Key Words and Phrases: source code, software security, software vulnerability detection, software bug detection, machine learning, deep learning
ACM Reference Format:
Nima Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan. 2018. A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning. J. ACM 37, 4, Article 111 (August 2018), 37 pages. https://doi.org/XXXXXXX.XXXXXXX
Authors’ addresses: Nima Shiri harzevili, nshiri@yorku.ca, York University, 4700 Keele St., North York, Ontario, Canada, M3J 1P3; Alvine Boaye Belle, York University, 4700 Keele St., North York, Canada, alvine.belle@lassonde.yorku.ca; Junjie Wang, Institute of Software, Chinese Academy of Sciences, Beijing, China, junjie@iscas.ac.cn; Song Wang, York University, 4700 Keele St., North York, Canada, wangsong@yorku.ca; Zhen Ming (Jack) Jiang, York University, 4700 Keele St., North York, Canada, zmjiang@eecs.yorku.ca; Nachiappan Nagappan, Meta, Seattle, USA, nachiappan.nagappan@gmail.com.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2018 Association for Computing Machinery. 0004-5411/2018/8-ART111 $15.00 https://doi.org/XXXXXXX.XXXXXXX
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
arXiv:2306.11673v1 [cs.SE] 20 Jun 2023


11N1:i2ma Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
1 INTRODUCTION
Automatic detection of software security vulnerabilities is a critical component of assuring software security. Machine Learning (ML) and Deep Learning (DL) breakthroughs have sparked great interest in employing these models to discover software vulnerabilities in general software systems.. [28, 79, 84, 137, 145]. ML/DL models excel at discovering subtle patterns and correlations from large datasets. They can automatically extract meaningful features from raw data, such as source code, and identify hidden patterns that may indicate software vulnerabilities. This capability is crucial in vulnerability detection, as vulnerabilities often involve subtle code characteristics and dependencies. Also, ML/DL models can handle a wide range of data types and formats, including source code [32, 33, 64, 95, 128, 134, 138, 144], textual information [60], and numerical features such as commit characteristics [113, 147]. They can process and analyze these data representations to detect vulnerabilities effectively. This flexibility allows researchers to leverage various sources of data and incorporate different features for comprehensive vulnerability detection. The overall process to leverage ML/DL models for software vulnerability detection is as follows: Data collection: The first step toward building a vulnerability detection model is to collect relevant vulnerable data for training the models. There are multiple sources for vulnerability detection datasets (we elaborate on this in RQ2), the researchers either use benchmark data [17, 42, 49, 76, 87, 88, 140, 147, 157, 158, 160] or collect from the open source [25, 45, 110, 114, 120] based on the requirements and the type of vulnerabilities. Data representation: Once the data is collected, it needs to be preprocessed to prepare it for training. The preprocess includes using appropriate representation techniques, i.e., graph/tree representation [17, 30, 38, 49, 82, 87, 90, 91, 97, 140, 153, 156, 157, 160], token representation [25, 42, 55, 58, 62, 76, 124, 158, 161], or using commit characteristics. Embedding: This step involves converting the source code representation into numerical format [17, 45, 49, 55, 58, 62, 76, 91, 114, 124, 161](vectors or embeddings) that can be utilized by machine learning or deep learning models for vulnerability detection. Model selection and architecture design: An suitable ML/DL model must be chosen based on the software vulnerability detection task. This can include everything from simple ML algorithms like SVM or Random Forests [24, 123, 155] to more advanced DL architectures like CNNs [60, 62, 145] or RNNs. The architecture of the model is intended to extract significant characteristics and patterns from the input data. Training: In the training phase, the vulnerability detection dataset is separated into training and validation sets, and the model learns from labeled data. The model’s parameters are updated iteratively depending on the prediction errors, using optimization techniques such as gradient descent. Evaluation and validation: Once the training is finished, the model’s performance is evaluated using a separate test dataset. Various metrics such as accuracy, precision, recall, and F1 score are calculated to assess the model’s effectiveness in detecting vulnerabilities. The model may also be validated against real-world vulnerabilities to measure its practical utility. Although many studies have utilized ML/DL to detect software vulnerabilities, there has not been a comprehensive review to consolidate the various approaches and characteristics of these techniques. Conducting such a systematic survey would be beneficial for practitioners and researchers to gain a better understanding of the current state-of-the-art tools for vulnerability detection, and could serve as inspiration for future studies. This study conducts a detailed and comprehensive survey to review, analyze, describe, and classify vulnerability detection papers from different perspectives. We analyzed 67 articles published in 37 flagship SE journals and conferences from 2011 to 2022. We investigated the following research questions (RQs) in this study:
• RQ1: What is the trend of studies using ML/DL models for vulnerability detection? – RQ1.1. What are the trends of studies in software vulnerability detection studies over time? – RQ1.2. What is the distribution of the publication venues?
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:3
• RQ2: What are the characteristics of experiment datasets used in software vulnerability detection?
– RQ2.1. What is the source of data? – RQ2.2. What are the types of data used in primary studies? – RQ2.3. How input data are represented? – RQ2.4. How input data are embedded for feature space?
• RQ3. What are the different ML/DL models used for vulnerability detection? • RQ4. What are the most frequent type of vulnerabilities covered in these studies? • RQ5. What are possible challenges and open directions in software vulnerability detection?
This paper makes the following contributions:
• We thoroughly analyzed 67 relevant studies that used ML/DL techniques to detect security vulnerabilities regarding publication trends, distribution of publication venues, and types of contributions. • We conducted a comprehensive analysis to understand the dataset, the processing of data, data representation, model architecture, model interpretability, and the types of involved vulnerabilities of these ML/DL-based vulnerability detection techniques. • We provided a classification of ML/DL models used in vulnerability detection based on their architectures and analysis of technique selection strategy on these models. • We discuss distinct technical challenges of using ML/DL techniques in vulnerability detection and outline key future directions. • We have shared our results and analysis data as a replication package1 to allow other researchers easily follow this paper and extend it.
We believe that this work is useful for researchers and practitioners in the field of software engineering and cybersecurity, particularly those with an interest in software vulnerability detection and mitigation. In addition, the findings of our systematic survey may also be useful to policymakers, software vendors, and other stakeholders who are concerned with improving software security and reducing the risk of cyberattacks. These individuals may use the insights provided by the review to inform their decisions about software development, procurement, and risk management. The remaining part of this paper is organized as follows: Section 2 summarizes existing studies focusing on proposing a systematic survey for software vulnerability detection. Section 2 presents related work on systematic surveys for software vulnerability detection using ML/DL techniques. Section 3 presents the research methodology proposed in this paper for paper collection and criteria for including and excluding studies. Section 4 addresses research questions and corresponding results. Section 5 discusses the possible limitations of this systematic survey. Finally, section 6 discusses the conclusion and future directions.
2 BACKGROUND AND RELATED WORK
In this section of the paper, we first provide a background on the definition of vulnerability and the different steps in software vulnerability detection. Then we discuss the related surveys and highlight their differences compared to our survey.
2.1 Background
Software vulnerability management is now essential for guaranteeing the security and integrity of software systems [20, 43, 119, 135]. Given the increasing reliance on software for many critical processes such as financial transactions, the frequency of vulnerabilities poses serious risks.. [57, 99],
1https://colab.research.google.com/drive/1O42duwz34H3fRoyfA37EU6Ig2u16R1Lb?usp=sharing
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i4ma Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
autonomous driving [46, 100], and mission-critical systems [53, 56]. Software vulnerabilities can be exploited by malicious entities to gain unauthorized access, compromise sensitive information, or disrupt services if they go undetected or ignored. [56]. As a result, excellent software vulnerability management is crucial to handling these risks, preserving user privacy [5], maintaining system availability, and assuring software application trustworthiness [104]. By proactively detecting, analyzing, and remediating vulnerabilities, organizations may strengthen their software systems against changing cybersecurity threats [6] and adhere to industry best practices for safe software development and deployment. There are multiple steps in software vulnerability management including vulnerability detection [17], vulnerability analysis [73], and vulnerability remediation [77]. In the following subsections, we elaborate on each step in detail.
2.1.1 Vulnerability detection. Vulnerability detection is critical in the overall process of managing software vulnerabilities [17, 17, 28, 64, 87, 90, 95, 134, 136, 158, 160]. It comprises detecting and investigating possible security weaknesses in software systems that attackers may exploit. There are several traditional techniques commonly used for vulnerability detection: Manual Code Auditing [7, 18, 126, 127, 130]: In this method, human experts examine the source thoroughly with the goal of manually detecting coding flaws, unsafe procedures, and possible vulnerabilities. Manual code review is time-consuming and requires the knowledge of qualified developers or security analysts. However, it provides for a thorough grasp of the code and can reveal subtle bugs that automated tools may overlook. Static Analysis [39, 41, 75, 103, 129, 139]: Static analysis involves using automated tools to analyze the source code or compiled binaries without executing the software. It examines the code structure, identifies potential coding issues, and detects common vulnerabilities such as buffer overflows [56], injection attacks, and insecure data handling. Static analysis tools employ various techniques like data flow analysis, control flow analysis, and pattern matching to identify potential vulnerabilities. They can help scale vulnerability detection efforts by analyzing large codebases efficiently. Dynamic Analysis [15, 81, 106]: The goal of dynamic analysis is to evaluate the behavior of software while it is running. Running the software in a controlled environment or through automated tests while monitoring its execution and interactions with system resources is what it entails. Dynamic analysis can detect bugs in input validation [70], access control, and error handling. This approach can identify vulnerabilities that static analysis alone cannot detect by analyzing the real-time behavior of the software. However, the dynamic analysis may have constraints in terms of significant system overhead [149].
2.1.2 Vulnerability analysis. After the detection of vulnerabilities, the subsequent step in software vulnerability management is vulnerability analysis and assessment [44, 63, 78, 80, 101, 131, 148]. This step involves further examining the identified vulnerabilities to assess their severity, impact, and potential exploitability. Severity: Accurately assessing software vulnerabilities is vital for several reasons. Firstly, it allows organizations to prioritize their response based on the severity of the vulnerabilities. Severity refers to the potential impact a vulnerability could have if exploited [23, 73, 74, 133]. By accurately assessing the severity, organizations can focus their attention on high-severity vulnerabilities that pose significant risks to the security and functionality of the software system. Impact: Secondly, accurately assessing vulnerabilities helps determine the potential impact they may have on the organization [27, 47, 51, 65]. The term impact refers to the repercussions of exploiting a vulnerability, such as denial of service [56] or data breaches [1]. By understanding the potential impact, organizations can make informed decisions regarding the urgency and priority of remediation efforts.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:5
exploitability: Furthermore, accurately assessing vulnerabilities aids in understanding their potential exploitability [13, 21, 22]. This entails determining the possibility that an attacker will be successful in exploiting the vulnerability to infiltrate the software system. Organizations can estimate the amount of risk associated with each vulnerability and invest resources accordingly by evaluating criteria such as the ease of exploitation and the availability of exploit techniques.
2.1.3 Vulnerability remediation. The process of addressing detected software vulnerabilities by different techniques such as patching, code modification, and repairing is referred to as software vulnerability remediation [8, 16, 26, 117]. The fundamental goal of remediation is to eliminate or mitigate vulnerabilities in order to improve the software system’s security and dependability. One common approach to vulnerability remediation is applying patches provided by software vendors or open-source communities [50, 83, 141]. Patches are updates or fixes that address specific vulnerabilities or weaknesses identified in a software system.
2.1.4 ML/DL for software vulnerability detection. By utilizing data analysis, pattern recognition, and machine-driven learning for finding software security vulnerabilities, ML/DL approaches have revolutionized software vulnerability detection. [28, 79, 84, 137, 145]. These techniques improve the accuracy and efficiency of vulnerability detection, potentially allowing for automated detection, faster analysis, and the identification of previously undisclosed vulnerabilities. One common application of ML/DL in vulnerability detection is the classification of code snippets [32, 64, 128, 138], software binaries [61, 109, 116, 145], or code changes mined from open-source repositories such as GitHub or CVE [35, 60, 85, 94, 118, 123, 136, 154]. ML models can be trained on labeled datasets, where each sample represents a known vulnerability or non-vulnerability. These models then learn to generalize from the provided examples and classify new instances based on the patterns they have learned. This method allows for automatic vulnerability discovery without the need for manual examination, considerably lowering the time and effort necessary for analysis. ML/DL models for detecting software vulnerabilities have promising advantages over traditional methodologies. Each benefit is discussed in depth in the next paragraph. Automation: Automation is a significant advantage. ML models can automatically scan and analyze large codebases, network traffic logs, or system configurations, flagging potential vulnerabilities without requiring human intervention for each individual case [19]. This automation speeds up the detection process, allowing security teams to focus on verifying and mitigating vulnerabilities rather than manual analysis. Performance: ML/DL approaches offer faster analysis. Traditional vulnerability detection methods rely on manual inspection or the application of predefined rules [7, 18, 126, 127, 130]. In contrast, ML/DL approaches can evaluate enormous volumes of data in parallel and generate predictions fast, dramatically shortening the time necessary to find vulnerabilities. Detection effectiveness: ML/DL models can uncover previously unknown vulnerabilities, commonly known as zero-day vulnerabilities [10]. These models may uncover signs of vulnerabilities even when they have not been specifically trained on them by learning patterns and generalizing from labeled data. This capability improves the overall security posture by helping to identify and address unknown weaknesses in software before they are exploited by attackers [2].
2.2 Related work
There have been several existing survey papers on software vulnerabilities in the literature. In this section, we analyze the existing papers based on different aspects as shown in Table 1. The columns in the table represent different aspects of the surveys, such as the data source used, representation, feature embedding, ML/DL models, vulnerability types, and interpretability of ML/DL models. Data Source indicates whether the survey reviewed vulnerability detection data
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i6ma Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
Table 1. Comparison of contributions between our survey and the existing related surveys/reviews.
No Studies Data Source Representation Embedding Models Vulnerability Types Interpretability
1 Triet et al. [77] ✓ × ✓ ✓ × ×
2 Ghaffarian et al. [48] ✓ ✓ ✓ ✓ × ×
3 Lin et al. [89] ✓ ✓ ✓ ✓ × ×
4 Zeng et al. [151] ✓ ✓ ✓ ✓ × × 5 Semasaba et al. [125] ✓ ✓ × ✓ ✓ ×
6 Sun et al. [132] ✓ × × × ✓ ×
7 Kritikos et al. [71] ✓ × × × ✓ ×
8 Khan et al. [69] × × × × × ×
9 Nong et al. [111] × × × × × × 10 Chakraborty et al. [19] × × × × × ×
11 Liu et al. [93] × × × × × ×
12 Bi et al. [9] × × × × × × 12 Our survey ✓ ✓ ✓ ✓ ✓ ✓
sources. Representation discusses whether the survey considered source code representation in its analysis. Embedding deals with whether the survey is analyzed feature embedding in its analysis. The table also considers ML/DL models in the sixth column as ML Models. The table also checks whether the survey considers vulnerability types based on Common Weakness Enumeration (CWE) number. The last column indicates whether the survey takes into account the interpretability of ML/DL models. Ghaffarian et al. [48] is the closest survey to ours when it comes to data-driven security vulnerability detection. In their survey, they analyzed data-driven software vulnerability detection from various aspects including Data Sources, Representation, Embedding types, and different ML/DL models as shown in Table 1. However, there are a couple of differences compared to our work. Specifically, this work also surveys vulnerability detection from the following aspects: Comprehensive Coverage: Understanding the many sorts of vulnerabilities allows researchers to create and develop effective vulnerability detection models that can thoroughly discover security vulnerabilities. To guarantee that their detection systems cover as many vulnerability types as possible, researchers must be familiar with the various methods of attack and potential weaknesses in software systems. Customization of Detection Techniques: Different sorts of vulnerabilities necessitate distinct detection methods. To build specialized detection systems that can discover certain types of vulnerabilities, researchers must first understand the subtleties of each vulnerability type. Prioritization of Mitigation Efforts: Researchers can prioritize mitigation efforts depending on the severity and effect of each vulnerability by understanding the many types of vulnerabilities. Critical vulnerabilities that pose the greatest danger to the system or organization can be prioritized by researchers. Better Understanding of Attack Patterns: Understanding the different types of vulnerabilities provides researchers with insights into the different attack patterns used by attackers. This knowledge helps researchers design detection techniques that can detect not only known attack patterns but also new, unknown patterns. Interpretability refers to the ability to explain how a model makes a particular decision or prediction. This is particularly important in the context of software vulnerability detection because security researchers need to be able to understand why a model is flagging a particular piece of code as potentially vulnerable. Additionally, interpretability can help improve trust in the model’s predictions. If developers and security researchers can understand how a model is making its decisions, they are more likely to trust its output and take appropriate actions based on its recommendations. Triet et al. [77] reviewed data-driven vulnerability assessment and prioritization studies. They conduct a review of prior research on software assessment and prioritization that leverages machine learning and data mining methods. They examine various types of research in this area, discuss the strengths and weaknesses of each approach, and highlight some unresolved issues and potential
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:7
areas for future research. The major difference to ours is that we review vulnerability detection approaches while they survey assessment and prioritization techniques. Vulnerability detection, vulnerability assessment, and vulnerability prioritization are all important components of the vulnerability management life-cycle, but they involve different stages of the vulnerability management process. Our work focuses on Vulnerability detection which refers to the process of identifying potential vulnerabilities in software systems. The goal of vulnerability detection is to identify all vulnerabilities that exist within the system, regardless of their severity. Vulnerability assessment, on the other hand, involves evaluating the severity and potential impact of each identified vulnerability. This assessment can involve analyzing factors such as the likelihood of the vulnerability being exploited and the potential harm that could result. Vulnerability prioritization involves ranking the identified vulnerabilities based on their level of risk or criticality. This ranking is typically based on the results of the vulnerability assessment, as well as other factors such as the availability of resources to address the vulnerabilities. Lin et al. [89] examined the literature on using deep learning and neural network-based approaches for detecting software vulnerabilities. There are a couple of differences compared to our survey. First, the study of conventional source code representation techniques (Static code attributes) for software vulnerability detection. In our survey, we neglect to review such representation techniques for a couple of reasons. Static code attributes, such as code length or complexity, may not be effective for vulnerability detection because they do not capture the dynamic behavior of the code at runtime. Vulnerabilities can manifest themselves in unexpected ways that are not apparent in the static code, making it difficult to detect them through static analysis alone. Additionally, static code attributes may not be able to capture the context of the code, which is important for understanding how the code interacts with other components in a system. Finally, static analysis tools may produce a high rate of false positives, which can be time-consuming to verify and may cause developers to ignore important vulnerabilities. Second, we examine the trend analysis of papers published in software vulnerability detection in a journal and conference papers because it provides a comprehensive understanding of the publishing patterns in a particular field or area of research. The trend analysis can shed light on the distribution of research output across various publication venues and the shifting preferences of researchers and authors. This information can be useful for stakeholders such as publishers, academic institutions, and researchers in making strategic decisions related to publishing, funding, and research collaborations. Zeng et al. [151] discussed the increasing attention towards exploitable vulnerabilities in software and the development of vulnerability detection methods, specifically the application of ML techniques. The paper reviews 22 recent studies that use deep learning to detect vulnerabilities and identifies four game changers that have significantly impacted this field. The survey further compares the game changers based on different aspects in software vulnerability detection including data source, feature representation, DL models, and detection granularity. There are a couple of differences compared to our survey. First, we analyze the trend patterns of papers on software vulnerability detection that have been published in journals and conferences. This analysis helps us gain a thorough comprehension of the publication trends in a specific area of research or field. Second, we cover more aspects of software vulnerability detection. While they only cover data source, feature representation, DL models, and detection granularity, we cover more aspects including vulnerability types and interpretability of ML/DL models. Additionally, we provide a more granular analysis of different aspects. Kritikos et al. [71] and Sun et al. [132] focused on cybersecurity and aim to improve cyber resilience. Sun et al. [132] discussed the paradigm change in understanding and protecting against cyber threats from reactive detection to proactive prediction, with an emphasis on new research on cybersecurity incident prediction systems that use many types of data sources. Kritikos et al. [71]
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i8ma Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
discusses the challenges of migrating applications to the cloud and ensuring their security, with a focus on vulnerability management during the application lifecycle and the use of open-source tools and databases to better secure applications. While the topics of the two abstracts are different, they share a common goal of improving cybersecurity and resilience. Both highlight the importance of proactive measures to prevent or mitigate cyber threats, rather than relying solely on reactive detection and response. Additionally, both highlight the importance of utilizing various data sources and tools to improve cybersecurity measures. While both approaches aim to improve the security of applications, they differ in their focus and techniques used. They mainly focus on providing guidance and tools to support vulnerability management during the application lifecycle, while in our survey, we focus on software vulnerability detection using ML/DL techniques on source code which aim at automating the identification of vulnerabilities in the source code. Khan et al. [69] focused on Vulnerability Assessment, which is the process of finding and fixing vulnerabilities in a computer system before they can be exploited by hackers. This highlights the necessity for more studies into automated vulnerability mitigation strategies that can effectively secure software systems. On the other hand, vulnerability identification with ML/DL approaches on source code entails analyzing an application’s source code in order to spot security flaws. Instead of evaluating the safety of the entire system, this method concentrates on finding vulnerabilities in the code itself. Nong et al.[111] explored the open-science aspects of studies on software vulnerability detection and argued there is a dearth of research on problems of open science in software engineering, particularly with regard to software vulnerability detection. The authors conducted an exhaustive literature study and identify 55 relevant works that propose deep learning-based vulnerability detection approaches. They investigated open science aspects including availability, executability, reproducibility, and replicability. The study reveals that 25.5% of the examined approaches provide open-source tools. Furthermore, some open-source tools lack adequate documentation and thorough implementation, rendering them inoperable or unreplicable. The use of unbalanced or intentionally produced datasets causes the approaches’ performance to be overstated, rendering them unreplicable. Chakraborty et al. [19] investigated the performance of cutting-edge DL-based vulnerability prediction approaches in real-world vulnerability prediction scenarios. They find that the performance of the state-of-the-art DL-based techniques drops by more than 50 percent in real-world scenarios. They also discover problems with training data (for example, data duplication and an unrealistic distribution of vulnerable classes) and model selection (for example, simplistic token-based models). Existing DL-based approaches often learn unrelated artifacts instead of features related to the cause of vulnerabilities. The significant difference compared to our survey study is that in our work, we focus on the usage of ML/DL models for software vulnerability detection and characterize the different stages in the pipeline of vulnerability detection. On the other hand, they focus on the issues related to the usage of state-of-the-art DL models for software vulnerability detection. Liu et al. [93] discussed the increasing popularity of DL techniques in software engineering research due to their ability to address SE challenges without extensive manual feature engineering. The authors highlight two important factors often overlooked in DL studies: reproducibility and replicability. Reproducibility refers to whether other researchers can obtain the same results using the authors’ artifacts, while replicability refers to obtaining similar results with re-implemented artifacts and a different experimental setup. The major difference compared to our study is that we focus on the usage of ML/DL techniques in software vulnerability detection pipelines, while they emphasize replicability and reproducibility of the results reported in software engineering research studies.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:9
Bi et al. [9] emphasizes the importance of software vulnerability detection techniques as well as the absence of a systematic methodology to evaluate these approaches. The research is the first to look into and describe the current state of software vulnerability detection benchmarking. The assessment examines current literature on vulnerability detection benchmarking, including methodologies employed in technique-proposing publications and empirical research. The survey examines the difficulties associated with benchmarking software vulnerability detection approaches and suggests alternative solutions to these difficulties. They do not, however, give a characterization of datasets, representations, embedding techniques, and models employed in software vulnerability identification, unlike our work.
3 METHODOLOGY OF SYSTEMATIC SURVEY
3.1 Sources of Information
In this paper, we conducted an empirical study following [68, 115]. The purpose of this study is to collect and examine papers from the year 2011 to 2022 focusing on vulnerability detection across various programming languages and source codes using machine learning and deep learning techniques. The period between 2011 and 2022 is an appropriate time interval for reviewing datadriven vulnerability detection for several reasons: a) Increase in the volume and diversity of software vulnerabilities: Over the past decade, there has been a significant increase in the number and diversity of software vulnerabilities that have been discovered and reported 2. As of 2021, there exist 150,000 CVE records in the National Vulnerability Database (NVD)3. This increase has created a need for more sophisticated and effective methods for vulnerability detection, which has led to the development of new data-driven techniques. b) Advancements in ML/DL and data analytics: The past decade has seen significant advancements in machine learning, including the development of deep learning algorithms [52, 59], natural language processing techniques [34, 96], and other data-driven approaches that are highly effective in detecting software vulnerabilities. While we start collecting papers, we search for relevant research papers from four available databases, which are ScienceDirect, IEEE Xplore, ACM digital library, and Google Scholar.
3.2 Search Terms
From earlier work, we identify key phrases used in the search [77, 89, 125, 151] and our experience with the subject area. The following are the search terms:
vulnerability detection OR security vulnerability detection OR vulnerability detection using machine learning OR vulnerability detection using deep learning OR source code security bug prediction OR source code vulnerability detection OR source code bug prediction
3.3 Study Selection
The process of selecting studies to be included in our survey involves the following stages: (1) initially choosing studies based on their title, (2) selecting studies after reviewing their abstracts, and (3) making further selections after reading the full papers. Note that, the initial search results contain entries that are not related to security vulnerability detection. This might be caused by accidental keyword matching. We manually check each paper and remove these irrelevant papers to ensure the quality of our survey dataset. We also observe that there exist duplicate papers among search results since the same study could be indexed by multiple databases. We then discarded
2https://nvd.nist.gov/general/news 3https://nvd.nist.gov/general/brief-history
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i1m0a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
duplicate studies manually. To assist the selection of papers that have presented new ML or DLbased models for software vulnerability identification, we provide the following inclusion and exclusion criteria:
• The studies should have been peer-reviewed • The studies should have experimental results • The studies should employ an ML or DL technique • The studies improve existing data-driven vulnerability detection techniques • The input to ML/DL models should be either source code, commit, or byte-codes
Also, we have the following exclusion criteria to filter out irrelevant papers:
• Studies focusing on other engineering domains • Studies addressing static analysis, dynamic analysis, mutation testing, fault localization • Review papers • Studies focusing on vulnerability detection of web and Android applications • Studies belonging to one of the following categories: books, chapters, tutorials, technical reports • Studies using code similarity or clone detection tools • Studies focusing on malware detection on mobile devices, intrusion detection, and bug detection using static code attributes
Using these criteria, we narrow down our findings by examining each paper’s title, abstract, and contents to get the most relevant and high-quality research papers. To make human effort manageable, we developed a script to automatically get high-quality records close to the software vulnerability detection problem. To summarize, in the first stage, we began with a total of 3,154 papers obtained from the database search. From this initial pool, 880 papers were chosen for further evaluation in the second stage. During the second stage, these papers were reviewed based on their abstracts, resulting in the selection of 116 papers with relevant abstracts. Finally, in the third stage, after reading the full papers, 67 papers were ultimately chosen for inclusion in the study.
3.4 Study Quality Assessment
For each of the final selected studies, we answered the questions below to assess its quality:
• Is there a clearly stated research goal related to software vulnerability detection? • Is the proposed vulnerability detection approach used ML or DL techniques? • Is there a defined and repeatable technique? • Is there any explicit contribution to vulnerability detection? • Is there a clear methodology for validating the technique? • Are the subject projects selected for validation suitable for the research goals? • Are there control techniques or baselines to demonstrate the effectiveness of the vulnerability detection technique? • Are the evaluation metrics relevant (e.g., evaluate the effectiveness of the proposed technique) to the research objectives? • Do the results presented in the study align with the research objectives and are they presented in a clear and relevant manner?
3.5 Selection Verification
The process of creating a taxonomy for the selected 67 primary studies involves several steps. Initially, the lead author establishes a preliminary taxonomy that groups the studies together based on their research questions. This taxonomy provides a basic framework for organizing the studies in a meaningful and systematic manner. Next, the lead author expands the taxonomy by assigning new
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:11
Fig. 1. The workflow of our survey.
papers to the preliminary taxonomy. If a new paper cannot fit into any of the existing categories within the taxonomy, a new category is created that reflects the unique characteristics of that paper. To ensure the accuracy of the taxonomy, the second and third authors (who are not involved in the taxonomy creation process) randomly select 20 papers from the workflow and check the created taxonomies for any discrepancies. They then mark any disagreements they find, and all three authors discuss and resolve these disagreements. Initially, the disagreement rate was 30%, but after a second round of review and cross-checking of the papers, the authors were able to eliminate all disagreements.
4 RESULTS
We present our analyses and findings in this section to address the research questions we devised in Section 1.
4.1 RQ1. What is the trend of studies using ML/DL models for vulnerability detection?
To comprehend the trend of publications, we examined the publication dates along with the venues in which they were presented.
4.1.1 RQ1.1. What are the trends of studies in software vulnerability detection over time? Figure 2 demonstrates the publication trend of vulnerability detection studies published in eleven years, i.e., between 2011 and 2022. It is observable that the number of publications has increased gradually over the years. There is only one publication from 2011 to 2016, and the number of publications increased to 18 in 2021. However, there is a decrease in the number of publications in 2022 compared
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i1m2a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
2011 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022
0
5
10
15
20 14
18
6
99 6 11111
Year
Number of Publications
2011 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022
0
0.5
1
1 · 102−2· 104−2· 105−2· 107−2· 10−20.16 0.29 0.43 0.52
0.79
1
Year
Cumulative Distribution
Fig. 2. Publication trend of vulnerability detection studies.
Table 2. Conference publication venues for manual search.
No Acronym Full name
1 ICSE International Conference on Software Engineering 2 ECSE/FSE ACM SIGSOFT Symposium on the Foundation of Software Engineering 3 ASE IEEE/ACM International Conference on Automated Software Engineering 4 USENIX USENIX
5 OOPSLA Object-oriented Programming, Systems, Languages, and Applications 6 ISSTA ACM SIGSOFT International Symposium on Software Testing and Analysis 7 MSR IEEE Working Conference on Mining Software Repositories 8 SANER IEEE International Conference on Software Analysis, Evolution and Reengineering 9 ISSRE IEEE International Symposium on Software Reliability Engineering 10 ICSME IEEE International Conference on Software Maintenance and Evolution
11 IJCAI International Joint Conferences on Artificial Intelligence Organization 12 CCS ACM SIGSAC Conference on Computer and Communications Security 13 ICLR International Conference on Learning Representations 14 NIPS International Conference on Neural Information Processing Systems 15 MASCOT Modelling, Analysis, and Simulation of Computer and Telecommunication Systems 16 QRS IEEE International Conference on Software Security and Reliability 17 KDDM Pacific-Asia Conference on Knowledge Discovery and Data Mining 18 NDSS Network and Distributed Systems Security Symposium 19 ARES ACM International Conference on Availability, Reliability and Security 20 INFOCOM IEEE International Workshop on Security and Privacy in Big Data 21 ICTAI IEEE International Conference on Tools with Artificial Intelligence 22 ICDM IEEE International Conference on Data Mining 23 GLOBCOM IEEE Global Communications Conference
24 TrustCom IEEE International Conference on Trust, Security and Privacy in Computing and Communications 25 DSAA IEEE International Conference on Data Science and Advanced Analytic
to the previous year. We have also examined the cumulative number of publications shown in Figure 2. It is noticeable that the curve fitting the distribution shows a significant increase in slope between 2018 and 2022 suggesting that the use of ML/DL techniques for software vulnerability detection has become a prevalent trend since 2017, and a broad range of studies have utilized ML/DL models to address challenges in this field.
4.1.2 RQ1.2: What is the distribution of the publication venues? In this study, overall, we analyzed and reviewed 67 papers from various publication venues including 43 conference and symposium papers along with 24 journal papers. We have included the conference and journal acronyms and their complete names for reference in Table 2 and Table 3. Table 4 shows the distribution of primary studies for each publication venue. 64.1% of publications are published in conferences and symposiums while 35.8% of papers have been published as journal papers. It is observable that MSR, IJCAI, and ECSE/FSE are the most popular venues that have the highest number of primary studies, each of which contains 4 papers. Meanwhile, among the journals, TDSC and TSE include the highest number of studies, i.e., 6 and 4 studies respectively.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:13
Table 3. Journal publication venues for manual search.
No Journal Acronym Full Name
1 TSE IEEE Transaction on Software Engineering 2 TOSEM ACM Transaction on Software Engineering and Methodology 3 IST Information and Software Technology 4 ESM Empirical Software Engineering 5 JSS Journal of System and Software 6 TDSC IEEE Transaction on Dependable and Secure Computing 7 CSJ Computer and Security Journal 8 TIFS IEEE Transactions on Information Forensics and Security 9 ISJ Information Sciences Journal 10 TFS IEEE Transaction on Fuzzy Systems 11 TKDE IEEE Transaction on Knowledge and Data Engineering 12 KBS Knowledge-Based Systems
(1) The results indicate that the application of ML/DL techniques for software vulnerability detection has had a remarkable rising trend in the past few years. (2) A large proportion of papers are published in recent two years, i.e., 2021 and 2022. (3) MSR, IJCAI, and ECSE/FSE are the most popular conference venues. On the other hand, TDSC and TSE are the most popular journal venues.
Answer to RQ1
4.2 RQ2. What are the characteristics of software vulnerability detection datasets?
Data is important for building and evaluating ML/DL-based software vulnerability detection models [30, 32, 37, 90, 92]. The quality of datasets can be assessed by different factors such as the source of data, data size and scale, data types, and preprocessing steps performed on data. For example, inappropriate preprocessing (representation) on data may result in poor performance of DL models [121]. In this section, we examine data used in vulnerability detection studies and conducted a comprehensive analysis of the steps of data source, data type, and data representation.
4.2.1 RQ2.1. What are the sources of datasets? One of the main challenges in ML/DL-based software vulnerability detection is the insufficient amount of data available for training operations [24, 92]. Consequently, there exists a gap in research on how to obtain sufficient datasets to facilitate the training of ML/DL models for software security vulnerability detection. To this end, we analyze the sources of datasets in the studied 67 primary studies. Our analysis reveals that datasets for this purpose can be broadly classified into three categories, i.e., Benchmark, Collected, and Hybrid sources. Benchmark contains standardized datasets used to evaluate the performance of vulnerability detection methods and techniques [32, 33, 55, 60, 62, 64, 72, 95, 98, 109, 116, 123, 134, 138, 144, 145, 150, 161]. Benchmark datasets for software vulnerability detection are often built from three main sources. The first source of data is collecting code snippets from open sources. This can include open-source software projects [67], public vulnerability databases [11], and bug repositories [14]. The goal is to gather a diverse set of programs or code snippets that represent different application domains,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i1m4a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
Table 4. Distribution of publications based on conference and journal venues.
Conference Venue # Studies References Journal Venue # Studies References
MSR 4 [24, 45, 58, 60] TDSC 6 [86, 87, 90, 95, 159, 160] IJCAI 4 [31, 38, 97, 157] TSE 4 [25, 32, 33, 124] ECSE/FSE 4 [84, 108, 110, 155] CSJ 2 [64, 145] CCS 3 [29, 91, 114, 144] IST 2 [128, 134] ISSRE 3 [140, 150, 153] TIFS 2 [61, 136] ICLR 2 [35, 76] TOSEM 2 [28, 161] ICSE 2 [17, 138] ESM 1 [120] OOPSLA 2 [85, 118] ISJ 1 [49] NIPS 2 [55, 154] JSS 1 [113] ASE 2 [79, 152] TFS 1 [94] QRS 1 [82] TKDE 1 [98] KDDM 1 [109] KBS 1 [156] NDSS 1 [88] SUM 24 ARES 1 [72] INFOCOM 1 [158] MASCOT 1 [42] ICTAI 1 [116] ICSME 1 [123] ICDM 1 [62] GLOBCOM 1 [147] USENIX 1 [143] DSAA 1 [108] ISSTA 1 [30] SANER 1 [36] TrustCom 1 [146] SUM 43
programming languages, and vulnerability types. From the collected data, specific programs or code snippets are selected to be included in the benchmark dataset. The selection process considers factors such as program complexity, vulnerability diversity, and code quality. The aim is to create a dataset that covers a wide range of vulnerabilities and represents real-world scenarios. In some cases, benchmark datasets may include automatically generated synthetic programs [12]. These programs are typically created using code generation techniques and follow certain patterns or templates. Synthetic generation allows for the creation of large-scale datasets and can help cover a broader range of vulnerabilities systematically. Alongside synthetic programs, benchmark datasets often include real-world software applications or code snippets written by hand. These manually created cases ensure that the dataset contains realistic vulnerabilities that reflect actual coding practices. Manual creation involves identifying vulnerable points in the code, introducing appropriate weaknesses, and maintaining a balance between code quality and realism. Collected datasets are gathered from publicly available projects hosted on repository websites such as Github or Stack Overflow [24, 28, 94, 118, 154, 155]. Also, some studies use the combination of different sources for vulnerability detection to increase the external validity of their findings [24, 58, 79, 84, 122, 136, 155], which refers to Hybrid source in this work.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:15
Benchmark
65.7%
Collected
25.4% Hybrid
9%
Fig. 3. The source of the datasets used in primary study papers.
The distribution of dataset sources in the primary studies is illustrated in figure 3. As we can see, 65.7% of primary studies have utilized Benchmark datasets for software vulnerability detection. The rationale behind this trend is that benchmark datasets are readily accessible to all researchers and can facilitate the reproducibility of prior studies. Researchers often used Collected datasets in evaluating the proposed ML or DL-based security vulnerability detection models. According to our observation, 25.4% of studies build vulnerability detection models using collected datasets. There are a couple of reasons, first of all, open-source repositories like GitHub contain a vast amount of real-world code written by developers from diverse backgrounds. This data reflects the real-world coding practices, patterns, and vulnerabilities present in software projects. By analyzing such data, researchers can gain insights into the prevalent types of vulnerabilities and their occurrence frequencies in real-world software. Second, open-source repositories offer the opportunity to identify new vulnerabilities that may not be present in benchmark datasets. By analyzing diverse codebases, researchers can uncover previously unknown vulnerabilities or variations of known vulnerabilities, which helps in advancing the state-of-the-art in vulnerability detection and expanding the knowledge base of software security. The third major source of data is Hybrid accounting for 9% of primary studies which is the combination of different sources. Researchers often use hybrid sources for software vulnerability detection to address some of the limitations of individual data sources and to obtain more diverse and comprehensive datasets. For example, researchers may combine data from benchmark datasets with data from other sources such as Github, open-source projects, or data from commercial companies to create a hybrid dataset that is more representative of real-world scenarios. By doing so, they can improve the generalizability of their models and increase their chances of detecting a wider range of vulnerabilities. Table 5 shows the detailed distribution of benchmark data used in the primary studies. As it is observable, NVD and SARD is the most widely used source of data in the Benchmark category. This is because SARD and NVD are publicly available benchmark sources to which researchers have unrestricted access. They give a lot of vulnerability data, allowing researchers to get a broad variety of vulnerabilities for their experiments and analyses. The availability of these materials promotes repeatability and collaboration among researchers in software vulnerability detection. Overall, there are 35 unique primary studies that use benchmark datasets from different sources. Table 6 shows the detailed distribution of the Collected source of data. As shown, Github is the most popular source of data for software vulnerability detection, accounting for 14 primary studies. Researchers can collect datasets from Github by crawling the platform and extracting relevant code repositories or by using Github’s API to access data programmatically. One advantage of using GitHub as a source of data is that it provides access to real-world code written by developers, which
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i1m6a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
Table 5. Detailed distribution of benchmark sources.
No Source # Studies References
1 SARD 17 [17, 28, 38, 42, 64, 84, 86, 87, 90, 95, 134, 136, 146, 153, 158, 160, 160] 2 NVD 13 [17, 42, 58, 64, 72, 79, 86, 87, 95, 136, 153, 160, 160] 3 ESC and VSC 3 [97, 98, 157] 4 SmartBugs Wild 3 [107, 108, 140] 5 Juliet test suit 3 [36, 84, 145] 6 SmartBugs 2 [107, 108] 7 PROMISE 2 [138, 150] 8 D2A 2 [30, 36] 9 NDSS 2 [76, 109] 10 NIST 1 [55] 11 OWASP 1 [55] 12 SAMATE 1 [72] 13 Mozilla Firefox projects 1 [147] 14 ICLR2019 1 [147] 15 FQ 1 [30] 16 Bugs Wild Dataset 1 [152] 17 Others 1 [109] - SUM 53 (35) 
Table 6. Detailed distribution of Collected sources.
No Source # Studies References
1 Github 14 [24, 25, 28, 35, 45, 85, 94, 110, 113, 114, 118, 120, 154, 155] 2 Jira 2 [24, 155] 3 CVE 2 [24, 159] 4 Bugzilla 2 [24, 155] 6 Others 2 [60, 61] - SUM 22 (17) 
can be used to train and test vulnerability detection models. This data can be particularly useful for detecting new and emerging vulnerabilities that may not be covered by benchmark datasets. Jira, CVE, and Bugzilla come after with 2 primary studies for each. Overall, there are 17 unique primary studies that use collected sources of data for software vulnerability detection.
4.2.2 RQ2.2. What are the types of software vulnerability detection datasets used in prior studies? When it comes to detecting software vulnerabilities, datasets can have varying data types, e.g., existing software vulnerability detection models can find vulnerabilities in source code or commits. It is crucial to carefully examine the data types, as they require different preprocessing techniques and must be represented differently when using ML/DL models. Additionally, distinct data types necessitate different architectural approaches for ML/DL models. This section provides an overview of the various data types and their distributions. We classified the data types of employed datasets into three broad categories, i.e., code-based, repository-based, and hybrid.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:17
Code
70.1%
Repository
25.4%
Hybrid
4.5%
Fig. 4. The type of datasets in primary studies.
Table 7. Data types of datasets involved in primary studies.
Category Data Type #Studies Total References
Code-based Source code 42 47 [17, 30–33, 36, 38, 42, 49, 55, 62, 64, 72, 82, 84, 86–88, 90, 91, 95, 97, 98, 107, 108, 124, 128, 134, 138, 140, 143, 144, 146, 150, 152, 153, 156161] Binary code 5 [61, 76, 109, 116, 145] Repository-based Code change 8 13 [25, 35, 45, 58, 85, 94, 118, 154] Commit 5 [110, 113, 114, 120, 147] Hybrid Source Code+Code change 3 7 [28, 79, 136] Commits+Code Change 2 [60, 123] Commits+Bug reports 1 [155] Bug report+Commits+Emails 1 [24]
SUM - - 67(67) 
Figure 4 shows the distribution of the data types in primary studies. We can observe that the majority of primary studies (70.1%) primarily focus on analyzing the source code for software vulnerability detection. This indicates the significance of code-level analysis in identifying vulnerabilities. The utilization of repository-level data, such as commit history and change logs, is also prominent, representing a substantial portion (25.4%) of primary studies. This suggests that repository-level information is considered valuable in vulnerability detection. Additionally, a smaller portion (4.5%) of the studies adopt a hybrid approach, combining both code-level analysis and repository-level information. These techniques leverage the strengths of both data sources to improve the accuracy and effectiveness of vulnerability detection. Table 7 elaborates the detailed data types categories used in primary studies. The table shows that 42 primary studies used code-based category and the major data type of this category is Source code [38, 42, 82, 84, 86, 87, 90, 143, 153, 157, 158, 160]. Binary code is the second major data type in code-based category [61, 109, 116, 145] accounting for 5 primary studies. Regarding the Repository based category, 13 primary studies focused on extracting useful information and patterns by crawling different artifacts from software repositories from open source. The major artifact is Code change accounting for 8 primary studies [25, 35, 45, 58, 85, 94, 118, 154], and Commit comes as the second with 5 primary studies. The last category of data types is Hybrid where the studies used a combination of different data types for software security vulnerability detection, accounting
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i1m8a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
t!
Token
29.9% Graph 38.8%
Tree
16.4% Hybrid
10.4% Commit Metrics
4.5%
Fig. 5. Different representation used by primary studies.
for 7 primary studies. As can be seen, Source code+Code change is the most dominant data type combination [28, 79, 136].
4.2.3 RQ2.3. How input data are represented? As noted in earlier sections, research studies focused on software vulnerability detection rely on diverse sources of data and data types. This heterogeneity necessitates the use of varied representation techniques, which in turn requires different architectural approaches and design assumptions for ML/DL models. We classified the input representation of employed datasets into five broad categories, i.e., Graph-based, Tree-based, Token-based, Metric-based and Hybrid. Figure 5 shows the distribution of different input representations used in primary studies. From the pie chart, we can observe that the most popular input representation is the use of Graph/Tree-based representation, accounting for the largest slice (i.e., 38.8% for Graph-based and 16.4% for Tree-based). Token-based representation follows closely, representing a substantial portion (29.9%) of primary studies. Hybrid representation combines multiple representations or approaches, which makes up a smaller portion (10.4%). Finally, the use of Commit Metrics in vulnerability detection has the smallest portion (4.5%). In the following paragraphs, we elaborate on each category in detail. Graph/Tree-based representation [17, 28, 30, 36, 38, 49, 61, 64, 72, 85, 87, 97, 98, 107, 108, 116, 134, 140, 146, 150, 153, 154, 157, 159, 160, 160]: allows for the detection of complex patterns and relationships between different code elements. By representing source code as a graph or tree, it becomes possible to capture not only the syntax and structure of the code but also its semantics, control flow, and data flow. There are many graph/tree-based representation techniques like Abstract Syntax Trees (AST) [35, 86, 90, 91, 102, 142] and Code Property Graph (CPG) [38, 49, 154] used to transform source code into AST and CPG representations. Token-based representation [3, 33, 37, 60, 79, 94, 109, 118, 122, 144, 155]: treat the source code as string token sequences and then transforms source code into tokens vectors. The input data is first split into a sequence of tokens, which are then converted into numerical vectors that can be processed by machine learning algorithms. Tokenization involves breaking down a string of text or source code into smaller units, or tokens, which can then be used as the basis for further analysis. In the case of source code, tokens might include keywords, operators, variables, and other elements of the programming language syntax. Commit Metrics [110, 113, 147]: leverages the metrics extracted from commits to represent code commits. Features derived from commits, such as the size of code changes, the number of modified lines, the complexity of the changes, or the programming language used, can be used as inputs to train ML/DL models. These models can then learn patterns and relationships between commit characteristics and the presence of vulnerabilities, enabling automated detection based on new commits.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:19
Table 8. Distribution of input representations in primary studies.
Category Artifact #Studies Total References
Graph/Tree-baSsoeudrce code as a graph 22 32(32) [17, 30, 36, 38, 49, 64, 72, 85, 87, 88, 97, 98, 107, 108, 134, 140, 146, 150, 153, 157, 159, 160] Source code as a tree 9 [32, 82, 86, 90, 91, 95, 128, 138, 156] Binary code as graph 2 [61, 116] Token-based Source code as a token 10 14(14) [33, 42, 55, 60, 62, 124, 143, 144, 158, 161] Binary code as a token 3 [76, 109, 145] Text as a token 1 [155] Hybrid Token Sequence+Commit Metrics 4 8(8) [24, 110, 114, 120] Token Sequence+Term frequency 1 [123] Token Sequence+ Graph 1 [45] Graph+Tree+Token Sequence 1 [84] Token+Tree 1 [152] Commits Commit Metrics 3 3 [110, 113, 147] SUM - - 57(55) [110, 113, 147]
Hybrid representation [24, 28, 60, 79, 122, 123, 136, 155]: uses a combination of different representations for software security vulnerability detection. Combining different representations of input data can lead to a more comprehensive and richer input representation of source code, which can improve the performance of vulnerability detection models in tasks such as prediction or detection. Combining different representations such as token-based representations and graphbased representations can help capture both the syntax and semantics of the code, as well as the relationships between different components of the code. Table 8 shows the representation techniques distributed by different artifacts used by ML/DL models. It is observable that Graph/Tree-based representation is the most dominant technique used by primary studies, accounting for 32 unique primary studies in total. These studies represent the input to ML/DL models via Source code as a graph, Source code as a tree, and Binary code as a graph. Source code as a graph is the major representation technique used by primary studies [17, 28, 30, 36, 38, 49, 61, 64, 72, 85, 87, 97, 98, 107, 108, 116, 134, 140, 146, 150, 153, 154, 157, 159, 160, 160] accounting for 22 studies. Source code as a tree [32, 35, 82, 86, 90, 91, 95, 128, 136, 138, 156] is the second major representation technique accounting for 9 primary studies. Some researchers used Binary code as a graph [61, 116] to build binary-level vulnerability detection models, accounting for 2 primary studies. There are 14 primary studies that used Token-based representation, in which 10 primary studies represented source code as a token sequence, three primary studies modeled binary code as a token, and one study represented text as token sequences [155]. Hybrid representation has 5 different types accounting for 8 primary studies. Token Sequence+Commit Metrics is the major artifact used to enhance the input representation in software vulnerability detection, accounting for 4 primary studies. It combines information from the token sequence of the code and additional metrics derived from software commits. This approach leverages both the structural and historical aspects of the code to provide a more comprehensive representation for building vulnerability detection models. Commits is the fourth least input representation used by 3 primary studies. In this representation, commit characteristics are used to build software vulnerability detection models.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i2m0a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
20112013201420152016201720182019202020212022
0
5
10
15
20
1
2
12
1
2
2
111
2
43
2
4
2
1
11
2
2
133
2
10 7
11 1
Tree Token Hybrid Graph Commits
Fig. 6. Distribution of data type representations in software vulnerability detection studies over time.
Figure 6 shows the distribution of data type representation in software vulnerability detection studies over time. As shown in the figure, Graph-based representation shows a substantial presence compared to other input representation techniques. There are a couple of reasons for this trend. First, graphs provide a natural and intuitive way to represent the structural relationships within the source code. By modeling the code as a graph, the relationships between functions, classes, methods, and variables can be captured effectively. This allows vulnerability detection algorithms to analyze the code at a higher level of abstraction and capture complex dependencies and interactions between code elements. Second, graph-based representations enable a better understanding of the context in which vulnerabilities may exist. By considering the surrounding code structure and dependencies, graph-based approaches can capture the flow of information and identify potential paths that can lead to vulnerabilities. This contextual understanding helps in identifying code patterns, control flow paths, and data dependencies that may introduce security vulnerabilities. Token-based representation has also gained popularity, with a peak occurrence in 2021. This is because it provides a fine-grained representation of the code. It simplifies the code analysis process by reducing the complexity of the code to a sequence of tokens, making it easier to apply traditional natural language processing techniques or ML models. It is also easily applicable to a wide range of programming languages. While the tokens themselves may differ across languages, the concept of breaking the code into discrete units remains the same. This versatility allows vulnerability detection techniques based on token representation to be applied to different programming languages and codebases, which further increases the external validity of vulnerability detection models. However, there is a slight decline in 2022, indicating potential shifts or diversification in the selection of input representations. Hybrid representation is gained attention since 2021, which suggests that combining different representations is favored by researchers in software vulnerability detection, potentially due to the complementary benefits provided by multiple representations.
4.2.4 RQ2.4. How input data are embedded for feature space? In the previous section, we discussed various representation techniques, and in this section, we further look at embedding methods that can transform these representations into inputs that can be understood by ML/DL models. The representation techniques are in a human-readable format, they cannot be directly interpreted by machines. Therefore, researchers use different embedding techniques to convert these representations into a numeric format. We discuss the embedding techniques in the following paragraphs
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:21
Word2vec
25.4%
Graph embedding
25.4%
Token vector embedding
11.9%
Others
16.4%
Hybrid
7.5% One hot embedding
6.0% Code token embedding
4.5% N-gram features
3.0%
Fig. 7. Different feature embedding techniques used in primary studies.
based on the distribution shown in Figure 7. The figure illustrates the distribution of feature embedding techniques used in primary studies. The chart shows the following categories and their corresponding percentages: Word2vec (25.4%), Graph embedding (25.4%), Token vector embedding (11.9%), Others (16.4%), Hybrid (7.5%), One hot embedding (6.0%), Code token embedding (4.5%), and N-gram features (3.0%). Word2vec [24, 37, 62, 64, 84, 85, 90, 92, 94, 114, 118, 120, 123, 124, 144, 145, 154, 155, 160, 161]: is one of the most widely-used embedding techniques for source code embedding in the examined papers, accounting for 25.4% of primary studies. This can be because it has been shown to be effective in capturing the semantics and relationships between different code components. Word2vec can be trained on code corpus to learn embeddings for different code components, such as variables, functions, and operators. By considering the context in which these components appear, Word2vec can capture the semantic relationships between them. Furthermore, Word2vec is a computationally efficient and scalable technique, which can be trained on large code corpora. This is important for source code embedding, as the code corpus can be much larger than the text corpus typically used in natural language processing. Graph embedding [17, 35, 38, 49, 98, 116, 136, 140, 150, 153, 157]: is another widely-used embedding technique among the primary studies, accounting for 25.4% of primary studies, which is mostly used by graph neural networks. This can be because it can capture the structural relationships between different code components, such as functions, classes, and methods. In contrast to token-based representations or sequence-based representations, graph embedding can explicitly represent the connections and dependencies between different code components. In a graph-based representation, code components are represented as nodes, and the relationships between them are represented as edges. This allows for a more fine-grained representation of the code structure. Token vector embedding [25, 32, 33, 45, 58, 60, 91, 95, 102, 138, 142, 143]: is also a popular technique used by primary studies accounting for 11.9% of examined papers. In this technique, input is converted into a sequence of tokens and each token is transformed into a numeric value. Then, these values are fed into ML/DL models for further computations. One hot embedding [55, 60, 76, 109]: is a typical way for encoding categorical data, in which each category is represented by a binary vector of zeros and ones. This method can also be used to encode source code for vulnerability detection, which accounts for 6% of studies. Code token embedding [32, 33, 95]: is used to represent source code tokens as dense vectors in a continuous vector space. Code token embedding captures the semantic and syntactic links between tokens by transferring them to a lower-dimensional vector space, as opposed to one hot encoding, which represents each token as a sparse binary vector.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i2m2a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
N-gram features [79, 128]: is a method of expressing code snippets as fixed-length dense vectors, each vector representing an n-gram of tokens. N-grams are sequences of n tokens, such as words or letters, that capture local context and interdependence between neighboring tokens. We observed that 3% of primary studies use N-gram features for embedding. Hybrid [28, 64, 84, 85, 154]: We find that 7.5% of primary studies use multiple embedding techniques to convert inputs to ML/DL models. Different embedding techniques capture different aspects of the data. By combining multiple techniques, researchers can leverage the complementary information provided by each technique. For example, some embedding techniques may focus on syntax, while others may capture semantic or contextual information. Others [30, 42, 61, 72, 110, 113, 123, 147, 152, 156, 158]: The remaining 16.4% that emerge seldom and do not belong to any group are classified as Others. For example, Zhang et al. [152] customizes the graphCodeBERT [54] to propose a graph-guided masked attention mechanism for vulnerability detection in which it captures variable dependency relationships and integrates the graph structure into the Transformer model.
(1) 65.7% of primary studies use benchmark data for software vulnerability detection. This can be because benchmark datasets are readily accessible to all researchers and can facilitate the reproducibility of studies. (2) The most common data type among the examined vulnerability detection studies is Code-based data type, accounting for 47 studies. In this category, Source code is the most prominent sub-type accounting for 42 studies. (3) Graph-based and Token-based input representations are the most popular input representation techniques used by primary studies accounting for 38.8% and 29.9% of primary studies respectively. (4) Graph embedding and Word2vec are the two most widely used embedding techniques used in primary studies accounting for 25.4% of studies respectively.
Answer to RQ2
4.3 RQ3. What are the ML/DL models used for software vulnerability detection?
In this section, we provide detailed information about the various ML/DL models utilized for software vulnerability detection. Initially, we present an analysis of the usage distribution of ML/DL models based on primary studies. Subsequently, we delve into the distribution of the usage of DL models used in primary studies over time. However, we have not extensively analyzed the distribution of ML models since their prevalence is relatively small compared to DL models. Nonetheless, we provide a comprehensive list of classic ML models that have been commonly employed in primary studies. From Figure 8, it is observable that 79.1% of studies are using DL models for software vulnerability detection [4, 28, 32, 33, 60, 64, 79, 92, 94, 95, 98, 116, 118, 122, 134, 136, 138, 154] while merely 16.4% of studies use classic ML models [72, 109, 113, 114, 120, 124, 142, 143]. Also, a limited number of studies use Language models denoted as LM [112, 128] and Distance Measures denoted as DM [61, 144]. The graph in Figure 9 illustrates the usage trend of DL models in detecting software vulnerabilities from 2016 to 2022. According to the trend, DL models were first introduced in 2016 for vulnerability detection, since then the use of RNNs for vulnerability detection showed an upward trend. The graph also demonstrates a rising trend in using GNNs for vulnerability detection from 2020 to 2022.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:23
DL
79.1%
ML
16.4% DM
3.0% LM
1.5%
Fig. 8. Distribution of models used in primary studies. DM stands for Distance Measure and LM stands for Language Model.
2016 2017 2018 2019 2020 2021 2022
0
5
10
15
20
25
1
3
13
26
8
3
1
1 1
2
1
11 1
1
2
11 5
11
1
2 13
Transformers RNN Hybrid General GNN CNN Attention
Fig. 9. Trend of DL models over time.
This can be because GNNs are more powerful than RNNs in detecting vulnerabilities, as they can capture more meaningful and semantic representations of input source code. Since vulnerability types often have complex structures, GNNs are an excellent fit for detecting hidden structural information. Table 9 shows the distribution of DL models used in primary studies. As shown in the table, LSTM is the most frequently used recurrent model, appearing in 8 studies. BiLSTM and BGRU are also popular models with 8 and 6 studies respectively. It is also observable that GGNN is the most prevalent graph-based model, appearing in 4 studies. GCN, GAT, and DR-GCN are also commonly used accounting for 4, 3, and 2 studies respectively. The presence of these models highlights the importance of capturing graph structures and relationships between code elements in vulnerability detection. Attention models were also used in 7 primary studies. Attention mechanisms allow models to pay more attention to specific parts of the code or input that are more likely to contain vulnerability-related patterns. This ability to localize relevant information helps identify and understand the factors contributing to vulnerabilities more effectively. CNNs are used in 6 studies. While not as prevalent as recurrent or graph models, CNNs are still considered effective for capturing local patterns and features in vulnerability detection tasks. General models like DBN, Auto Encoders, Memory Neural Networks, GAN, and Para2Vec are also used to a lesser extent, indicating the exploration of diverse deep learning techniques in vulnerability detection. Transformers are the least frequent family of DL models used in primary studies. This is because they have been recently introduced for software vulnerability detection. Transformers are effective in this domain since
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i2m4a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
they generate contextualized representations for each token in the input sequence. By considering the surrounding tokens and their interactions, transformers capture rich contextual information, which is crucial for understanding vulnerabilities that depend on the overall context of the code or vulnerability-related text. Table 10 shows the distribution of ML models used in primary studies. As shown in the figure, Random Forest is the most frequently used ML model, appearing in 7 studies. SVM, Naive Bayes, and Logistic Regression are popular choices, with 6, 5, and 4 occurrences, respectively. N-Gram models are used in 1 study, indicating their application in capturing sequential patterns and languagebased features in vulnerability detection. N-Gram models are commonly used for text analysis and have been adapted for code analysis tasks. Distance measures are utilized in 2 studies for vulnerability detection. These metrics help quantify the similarity or dissimilarity between code elements or features, enabling the identification of potentially vulnerable code segments based on their proximity to known vulnerabilities.
Table 9. Distribution of DL models in primary studies.
Category Model Name # Studies Total References
Recurrent Models LSTM 8 31(21) [32, 33, 94, 95, 134, 138, 156, 158] BiLSTM 8 [64, 86, 88, 90, 91, 134, 160, 161] BGRU 6 [64, 86, 87, 134, 145, 161] GRU 4 [58, 64, 85, 134] RNN 3 [42, 76, 134] BRNN 2 [76, 134] Graph Models GGNN 4 18(15) [35, 36, 136, 159] GCN 4 [28, 49, 84, 150] GAT 3 [28, 45, 49] DR-GCN 2 [98, 157] RGCN 1 [153] FS-GNN 1 [17] K-GNN 1 [28] DGCNN 1 [116] GGRN 1 [154] Attention Models - 7 7 [30, 38, 79, 85, 97, 152, 160] Convolutional Models CNN 6 6 [42, 60, 62, 82, 145, 161] General Models DBN 1 5(4) [138] Auto Encoders 1 [76] Memory Neural Network 1 [60] GAN 1 [55] Para2Vec 1 [76] Transformers Seq2Seq Transformer 1 4(4) [25] Graph CodeBERT 1 [140] CodeBERT 1 [110] HGT 1 [146] SUM - - 71(49) 
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:25
Table 10. Distribution of ML and other models in primary studies.
Category Model Name Studies Total References
Classic ML Models Random Forest 7 35(11) [24, 72, 113, 123, 124, 147, 155] SVM 6 [24, 114, 120, 123, 124, 155] Naive Bayes 5 [24, 72, 113, 124, 155] Logistic Regression 4 [72, 113, 123, 155] K-NN 3 [24, 124, 155] Gradient Boosting 2 [24, 155] Decision Tree 2 [72, 124] AdaBoost 2 [24, 155] PCA 1 [143] Kernel Machine 1 [109] ADTree 1 [113] MLP 1 [113] Language Models N-Gram 1 1 [128] Distance Metrics Distance Measure 2 2 [61, 144] SUM - - 38(14) 
(1) 79.1% of primary studies use DL models for vulnerability detection while merely 16.4% of the primary studies use classic ML models. (2) RNNs and GNNs are by far the most popular DL-based models in software vulnerability detection accounting for 28% and 22% of primary studies. (3) LSTM is the most popular architecture in RNN-based models. (4) Graph-based models are the second most popular models used in software security vulnerability detection accounting for 15 studies. In this family, GGNN is the most popular architecture. (5) Besides DL models, ML models are popular for software vulnerability detection. Random Forest is the most popular model accounting for 7 studies.
Answer to RQ3
4.4 RQ4. What is the most frequent type of vulnerability covered in primary studies?
Software vulnerability detection datasets support different vulnerability types. For example, NVD and SARD benchmark together support 96 types of vulnerabilities. This research question intends to summarize what is the most popular vulnerability types covered by primary studies and what is their frequency. Table 11 shows the statistics regarding the vulnerability types. The column CWE-Type indicates the type of CWE4. There also exists a numerical score for some types of CWEs, which indicate the weakness’ severity. A larger value indicates a higher level of dangerousness and severity5. Please note that some frequent types do not have a CWE score, so we denote them as “-”. There are many categories on the CWE website for vulnerability categorization including
4https://cwe.mitre.org/ 5https://cwe.mitre.org/top25/archive/2021/2021_cwe_top25.html
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i2m6a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
categorization by software development, categorization by Hardware design, and categorization by research concepts. The categorization shown in Table 11 is based on categorization by research concepts as this categorization is a perfect match for vulnerability types reported in primary studies. Table 11 indicates that the vulnerability category that receives the highest attendance is related to the improper control of a resource through its lifetime (CWE-664), with a total of 42 studies (18 unique studies). This category primarily involves managing a system’s resources, which are created, utilized, and disposed of according to a predefined set of instructions. When a software system fails to follow these guidelines for resource usage, it can lead to unexpected behaviors that create potentially hazardous situations. Attackers can take advantage of these situations to exploit the software system for their own purposes. It is observable that CWE-119 [17, 25, 28, 38, 42, 45, 76, 88, 90, 95, 109, 122, 136, 145] is the most frequent vulnerability type addressed by the primary studies. This vulnerability occurs when a software system attempts to access or write to a memory location outside the permitted boundary of the system’s buffer. Attackers can exploit this vulnerability by controlling memory locations and executing their own code or commands, effectively manipulating the system’s memory. Although the score for this vulnerability type is not high, the frequency of primary studies addressing it can be a valuable indicator of its significance in terms of detecting and addressing the vulnerability. Within this category, the most severe vulnerability type with a score of 65.93 is CWE-787, which is discussed in 5 primary studies. This vulnerability is considered severe and critical because it can result in the corruption of data, system crashes, or the execution of malicious code. It occurs when a software system attempts to write data beyond the intended buffer, either before the beginning or past the end of the buffer. CWE-22 is another frequent vulnerability type addressed by primary studies accounting for 4 primary studies [17, 28, 45, 49]. This vulnerability is referred to as “Path Traversal”, where attackers exploit special elements, such as “..” or “/”, to construct their own path and gain unauthorized access to restricted locations. This vulnerability is particularly critical because attackers can use it to modify restricted files or directories in vulnerable software systems, potentially leading to system crashes by uploading malicious code or commands. In critical financial software systems, attackers can even gain access to customers’ bank account information. Given the severity of this vulnerability type with a score of 14.69 and the frequency of papers covering it, detecting and addressing this vulnerability is of utmost importance. Therefore, more advanced ML/DL models are needed to effectively detect this type of vulnerability. The least frequent vulnerability type of this family is CWE-120 [17, 122] which is a classic buffer overflow, accounting for 2 primary studies. This vulnerability occurs when the software attempts to copy a value to the output buffer without first validating its size. If the range of the value is too large for the length of the output buffer, a buffer overflow can occur. While the detection of buffer overflow can be challenging in some cases, static bug detection tools have already addressed this vulnerability, and detection methods are currently available. Improper Neutralization - (CWE-707) Is the second major family of vulnerability types covered by 16 primary studies, including 9 unique primary studies. In this type, the attackers exploit input and output data when they are malformed or not validated properly. There are several scenarios that can lead to data neutralization weaknesses. The first scenario involves checking whether input and output data are safe, while the second scenario involves filtering the input and output data to ensure that any data transformation is done safely. The third scenario involves preventing external attackers from directly manipulating the input and output data, while the fourth scenario relates to the lack of processing input and output data in any circumstances. All of these scenarios can be the root causes of data neutralization weaknesses. As can be seen, CWE-20 is the most frequent type of vulnerability accounting for 6 primary studies. CWE-20 refers to a situation where input validation is not done properly in software systems, making them vulnerable to attacks by malicious individuals who can exploit input data. This occurs when the input data is not verified
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:27
Table 11. Different vulnerability types covered in primary studies.
Category CWE-Type Severity Score #Studies Total References
CWE-664 CWE-119 5.84 14 42(18) [17, 25, 28, 38, 42, 45, 76, 88, 90, 95, 109, 122, 136, 145] CWE-787 65.93 5 [17, 28, 45, 136, 146] CWE-22 14.69 4 [28, 45, 49, 136] CWE-125 24.9 4 [17, 25, 28, 136] CWE-400 - 4 [28, 136, 146, 160] CWE-200 4.74 3 [25, 45, 136] CWE-121 - 3 [17, 122, 145] CWE-122 - 3 [17, 31, 122] CWE-120 - 2 [17, 122] CWE-707 CWE-20 20.47 6 16(9) [25, 28, 45, 122, 136, 146] CWE-78 19.55 5 [28, 49, 86, 136, 146] CWE-89 19.54 3 [49, 72, 136] CWE-79 46.84 2 [72, 136] CWE-682 CWE-190 7.12 5 5 [28, 45, 136, 146, 160] CWE-703 CWE-476 6.54 4 4 [17, 25, 122, 136] CWE-284 CWE-284 - 2 2 [3, 25] CWE-691 CWE-362 - 2 2 [25, 160] CWE-1215 CWE-129 - 1 1 [146] - CWE-789 - 1 1 [146] SUM - - - 73(21) 
to be safe or in line with the predefined specifications. The severity and occurrence of this issue are significant, highlighting the need for its detection as it can pose critical risks. CWE-78 is the second major vulnerability type covered by 5 primary studies [28, 49, 86, 136, 146]. This category of security vulnerability pertains to OS command injection, in which an external attacker can construct an OS command by using input data from components that have not been adequately verified. The attacker can then execute harmful commands, potentially causing the system to behave unexpectedly or crash, and putting it in a hazardous state.
(1) The most frequent type of vulnerabilities covered in primary studies is Improper Control of a Resource Through its Lifetime-(CWE-664) accounting for 18 unique primary studies. CWE-119: Improper Restriction of Operations within the Bounds of a Memory Buffer is the most frequent type of vulnerability in this category accounting for 14 primary studies, and CWE-787: Out-of-bounds Write comes subsequently covered by 5 primary studies. The least frequent type of vulnerability is CWE-120 covered by 2 studies. (2) Improper Neutralization(CWE-707) is the second major family of vulnerability types covered by 9 unique primary studies in total. In this family, CWE-20: Improper Input Validation is the most frequent type covered by 6 primary studies, and CWE-78: Improper Neutralization of Special Elements used in an OS Command (‘OS Command Injection’) comes next with 5 primary studies. (3) Some vulnerability types have a high CVSS score while not sufficiently addressed by existing vulnerability detection studies including but not limited to CWE-79 and CWE-89.
Answer to RQ4
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i2m8a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
4.5 RQ5. What are possible challenges and open directions in software vulnerability detection?
We have summarized the challenges from previous studies into five different categories, which are discussed as follows. Challenge 1: Semantic Representation. The biggest challenge in vulnerability detection through learning is the inadequate modeling of the comprehensive semantics of complex vulnerabilities by current models [28, 30, 32, 33, 38, 60, 64, 82, 86, 87, 97, 107, 108, 116, 128, 138, 140, 146, 153, 154, 156]. These vulnerabilities often exhibit intricate characteristics and patterns that are not fully captured by existing ML/DL models that treat source code snippets as a linear sequence like natural language, or only partially represent source code snippets. Unlike natural language, the source code of realworld projects contains structural and logical information that must be considered by ML/DL models using AST, data flow, and control flow. Therefore, current ML/DL approaches fall short of identifying complex vulnerability patterns. Challenge 2: Prediction Granularity. The ability of DL models to identify the location of vulnerabilities is influenced by the level of granularity in their inputs. Current DL models use a coarser level of granularity, such as method and file, for vulnerability detection. To achieve finer-grained inputs, program slicing is necessary, but it poses a challenge. The crucial question is how to perform program slicing effectively to eliminate unwanted noise in input data and provide more specific inputs. Current tools [17, 35, 36, 45, 84, 91, 113, 159, 160] concentrate on library/API function calls, arithmetic operations, and pointer usages, but this approach is not sufficient since not all vulnerabilities originate from these slicing criteria. Challenge 3: False Positive Removal. The most commonly used tools for detecting software vulnerabilities and bugs are static analyzers [55, 72, 85, 150, 155, 158, 160]. These tools utilize hardcoded rules that are defined by experts to model the runtime behavior of a program without the need for compilation. This approach has several benefits, such as effectively identifying the location of vulnerabilities in source codes, which is challenging in large-scale projects with thousands of files and artifacts. Additionally, static analyzers are used at the early stage of the software development process, which helps reduce software maintenance costs. However, relying on expert-defined rules comes with a high false-positive rate, as these rules may not be generalizable to new vulnerabilities that have intricate and sophisticated program semantics. Another significant issue is that defining and updating rules is a labor-intensive and time-consuming process that requires experts to have in-depth knowledge of emerging vulnerabilities. That is why data-driven vulnerability detection has emerged to overcome the aforementioned challenges [55, 72, 85, 150, 155, 158, 160]. Challenge 4: Lack of Training Data. A significant weakness of DL models, particularly in software vulnerability detection, is their insatiable need for data [24, 90, 95, 110, 147]. In domains such as image classification, there is an ample supply of labeled data, making it possible to train DL models effectively. Furthermore, there are many pre-trained models available that can be finetuned for detection tasks. However, in software vulnerability detection, data scarcity is a major problem since labeling ground truth information is a challenging task. To obtain training data, multiple online platforms such as Stack Overflow, GitHub, and issue tracking or bug tracking systems are used. While these platforms contain billions of records, the labeling process is difficult and is often done manually. One possible solution is the automatic labeling of data, but this approach is challenging as it often generates many false positives. Additionally, some researchers use unsupervised classification for vulnerability detection, but this method also suffers from limited precision.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:29
Challenge 5: Lack of Model Interpretability. Interpretability in DL models refers to the ability to understand and explain the decisions made by the model [40, 105]. In the context of software vulnerability detection [84, 97, 161], there are several challenges that make interpretability challenging for DL models. First, source code can be highly complex, especially in large software projects. It often consists of multiple files, functions, and dependencies, making it difficult to extract meaningful and concise explanations from the code [66]. Software systems are dynamic and undergo changes over time. Code is often maintained, updated, and refactored, which can introduce complexities in interpreting the decisions made by AI models. The model’s explanations may not be applicable to the current version of the code if it has evolved since the model’s training.
(1) Current models inadequately capture the comprehensive semantics of complex vulnerabilities, as they fail to consider the structural and logical information present in source code snippets. Existing ML/DL models treat source code as a linear sequence, which limits their ability to identify intricate vulnerability patterns. (2) DL models often use a coarse level of granularity for vulnerability detection, such as method and file level. Achieving finer-grained inputs requires effective programslicing techniques to eliminate noise and provide more specific inputs. Current approaches focus on certain slicing criteria, but vulnerabilities can originate from other sources as well. (3) DL models require a significant amount of labeled data for effective training. However, in software vulnerability detection, there is a scarcity of labeled data due to the challenging task of manual labeling. Automatic labeling approaches often generate false positives, and unsupervised classification suffers from limited precision.
Answer to RQ5
5 THREATS TO VALIDITY
External Validity. One of the major threats to the external validity of our work is the data collection internal. We collect data in an 11 years old period from 2011 to 2022 to coverall all possible studies published during this period. Another source of threat to external validity is the coverage of the input data types in software vulnerability detection. To tackle this threat, we focused on source code snippets as well as repository data, i.e., data that can be extracted from open source including GitHub and CVE. Internal Validity. One of the major threats to the internal validity of our work is the automatic collection of data for analysis. Our technique for data collection is automated at its initial steps in which we may miss some important vulnerability detection papers. In fact, we developed a set of scripts that allowed us to extract papers given an 11-year period. Even though the subsequent steps are manually supervised, still the automatic filtering suffers from this issue. The second important issue with our data collection is possible bias. The root cause of bias is unavoidable disagreements during paper classification as two researchers have worked together to categorize papers based on title, abstract, and content of papers. In case the researchers could not come up with an agreement, the third researcher join to resolve the differences which somehow relaxes the issue. Construct Validity. The major threat to the construct validity of our survey is the level of granularity of the analysis we conducted for each primary study. For each study, we deeply analyzed the artifacts explained in the paper and show their distribution as tables and graphs. For example, in terms of sources for benchmark data, we deeply analyzed 17 sources accounting for 53
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i3m0a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
primary studies overall. The second threat to the construct validity is the degree of coverage for each primary study. We analyzed each primary study from 5 aspects including input data, input representation, embedding techniques, models, vulnerability types, and whether the study supports the interpretability of vulnerability detection models or not.
6 CONCLUSION
In this study, we conducted a systematic survey on 67 primary studies using ML/DL models for software security vulnerability detection. We collected the papers from different journals and conference venues including 25 conferences and 12 journals. Our review is established based on five major research questions and a set of sub-research questions. We devised the research questions in a comprehensive manner where to cover various dimensions of software vulnerability detection. Our analysis of primary studies indicated that there is a booming trend in the growth of using ML/DL models for software vulnerability detection. Our deep analysis of data sources of primary studies revealed that 65.7% of studies use benchmark data for software vulnerability detection. We also find 6 broad categories of DL models along with 14 classic ML models used in software vulnerability detection. The categories of DL models are classified as recurrent models, graph models, attention models, convolutional models, general models, and transformer models. RNNs are by far the most popular DNNs in software vulnerability detection. Our analysis also finds that RNNs with LSTM cells are the most popular network architectures in recurrent models, accounting for 8 primary studies. In the category of graph models, GGNN is the most popular DL model used by 4 primary studies. Our results on vulnerability types reveal that the most frequent type of vulnerability covered in existing studies is Improper Control of a Resource Through its Lifetime - (CWE-664) accounting for 18 primary studies. In conclusion, we have identified a collection of ongoing challenges that necessitate further exploration in future studies involving the utilization of ML/DL models for software vulnerability detection.
REFERENCES
[1] Aleksi Aaltonen and Yiwen Gao. 2021. Does the Outsider Help? The Impact of Bug Bounty Programs on Data Breaches. The Impact of Bug Bounty Programs on Data Breaches (August 20, 2021). Fox School of Business Research Paper (2021). [2] Faranak Abri, Sima Siami-Namini, Mahdi Adl Khanghah, Fahimeh Mirza Soltani, and Akbar Siami Namin. 2019. Can machine/deep learning classifiers detect zero-day malware with high accuracy?. In 2019 IEEE international conference on big data (Big Data). IEEE, 3252–3259.
[3] Georgios Aivatoglou, Mike Anastasiadis, Georgios Spanos, Antonis Voulgaridis, Konstantinos Votis, and Dimitrios Tzovaras. 2021. A tree-based machine learning methodology to automatically classify software vulnerabilities. In 2021 IEEE International Conference on Cyber Security and Resilience (CSR). IEEE, 312–317.
[4] Wenyan An, Liwei Chen, Jinxin Wang, Gewangzi Du, Gang Shi, and Dan Meng. 2020. AVDHRAM: Automated Vulnerability Detection based on Hierarchical Representation and Attention Mechanism. In 2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom). IEEE, 337–344.
[5] Pauline Anthonysamy, Awais Rashid, and Ruzanna Chitchyan. 2017. Privacy requirements: present & future. In 2017 IEEE/ACM 39th international conference on software engineering: software engineering in society track (ICSE-SEIS). IEEE, 13–22. [6] Ömer Aslan, Semih Serkant Aktuğ, Merve Ozkan-Okay, Abdullah Asim Yilmaz, and Erdal Akin. 2023. A comprehensive review of cyber security vulnerabilities, threats, attacks, and solutions. Electronics 12, 6 (2023), 1333. [7] Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and challenges of modern code review. In 2013 35th International Conference on Software Engineering (ICSE). IEEE, 712–721.
[8] Guru Bhandari, Amara Naseer, and Leon Moonen. 2021. CVEfixes: automated collection of vulnerabilities and their fixes from open-source software. In Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering. 30–39.
[9] Yingzhou Bi, Jiangtao Huang, Penghui Liu, and Lianmei Wang. 2023. Benchmarking Software Vulnerability Detection Techniques: A Survey. arXiv preprint arXiv:2303.16362 (2023).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:31
[10] Leyla Bilge and Tudor Dumitraş. 2012. Before we knew it: an empirical study of zero-day attacks in the real world. In Proceedings of the 2012 ACM conference on Computer and communications security. 833–844. [11] Paul E Black. 2017. Sard: a software assurance reference dataset. (2017). [12] Harold Booth, Doug Rike, and Gregory A Witte. 2013. The national vulnerability database (nvd): Overview. (2013). [13] Mehran Bozorgi, Lawrence K Saul, Stefan Savage, and Geoffrey M Voelker. 2010. Beyond heuristics: learning to classify vulnerabilities and predict exploits. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. 105–114. [14] UI Bugzilla’s. 2021. Bugzilla. (2021). [15] Cristian Cadar, Daniel Dunbar, Dawson R Engler, et al. 2008. Klee: unassisted and automatic generation of highcoverage tests for complex systems programs.. In OSDI, Vol. 8. 209–224. [16] Gerardo Canfora, Andrea Di Sorbo, Sara Forootani, Matias Martinez, and Corrado A Visaggio. 2022. Patchworking: Exploring the code changes induced by vulnerability fixing activities. Information and Software Technology 142 (2022), 106745. [17] Sicong Cao, Xiaobing Sun, Lili Bo, Rongxin Wu, Bin Li, and Chuanqi Tao. 2022. MVD: Memory-Related Vulnerability Detection Based on Flow-Sensitive Graph Neural Networks. arXiv preprint arXiv:2203.02660 (2022). [18] Bengt Carlsson and Dejan Baca. 2005. Software security analysis-execution phase audit. In 31st EUROMICRO Conference on Software Engineering and Advanced Applications. IEEE, 240–247.
[19] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2021. Deep learning based vulnerability detection: Are we there yet. IEEE Transactions on Software Engineering (2021).
[20] Yung-Yu Chang, Pavol Zavarsky, Ron Ruhl, and Dale Lindskog. 2011. Trend analysis of the cve for software vulnerability management. In 2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing. IEEE, 1290–1293.
[21] Haipeng Chen, Jing Liu, Rui Liu, Noseong Park, and VS Subrahmanian. 2019. VEST: A System for Vulnerability Exploit Scoring & Timing.. In IJCAI. 6503–6505. [22] Haipeng Chen, Rui Liu, Noseong Park, and VS Subrahmanian. 2019. Using twitter to predict when vulnerabilities will be exploited. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data Mining. 3143–3152. [23] Jinfu Chen, Patrick Kwaku Kudjo, Solomon Mensah, Selasie Aformaley Brown, and George Akorfu. 2020. An automatic software vulnerability classification framework using term frequency-inverse gravity moment and feature selection. Journal of Systems and Software 167 (2020), 110616.
[24] Yang Chen, Andrew E Santosa, Ang Ming Yi, Abhishek Sharma, Asankhaya Sharma, and David Lo. 2020. A machine learning approach for vulnerability curation. In Proceedings of the 17th International Conference on Mining Software Repositories. 32–42.
[25] Zimin Chen, Steve Kommrusch, and Martin Monperrus. 2021. Neural Transfer Learning for Repairing Security Vulnerabilities in C Code. arXiv preprint arXiv:2104.08308 (2021). [26] Zimin Chen, Steve Kommrusch, and Martin Monperrus. 2022. Neural transfer learning for repairing security vulnerabilities in c code. IEEE Transactions on Software Engineering 49, 1 (2022), 147–165. [27] Zhongqiang Chen, Yuan Zhang, and Zhongrong Chen. 2010. A categorization framework for common computer vulnerabilities and exposures. Comput. J. 53, 5 (2010), 551–580. [28] Xiao Cheng, Haoyu Wang, Jiayi Hua, Guoai Xu, and Yulei Sui. 2021. DeepWukong: Statically detecting software vulnerabilities using deep graph neural network. ACM Transactions on Software Engineering and Methodology (TOSEM) 30, 3 (2021), 1–33. [29] Xiao Cheng, Haoyu Wang, Jiayi Hua, Miao Zhang, Guoai Xu, Li Yi, and Yulei Sui. 2019. Static detection of controlflow-related vulnerabilities using graph embedding. In 2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS). IEEE, 41–50.
[30] Xiao Cheng, Guanqin Zhang, Haoyu Wang, and Yulei Sui. 2022. Path-sensitive code embedding via contrastive learning for software vulnerability detection. In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. 519–531.
[31] Min-Je Choi, Sehun Jeong, Hakjoo Oh, and Jaegul Choo. 2017. End-to-End Prediction of Buffer Overruns from Raw Source Code via Neural Memory Networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (Melbourne, Australia) (IJCAI’17). AAAI Press, 1546–1553. [32] Hoa Khanh Dam, Truyen Tran, Trang Pham, Shien Wee Ng, John Grundy, and Aditya Ghose. 2017. Automatic feature learning for vulnerability prediction. arXiv preprint arXiv:1708.02368 (2017). [33] Hoa Khanh Dam, Truyen Tran, Trang Pham, Shien Wee Ng, John Grundy, and Aditya Ghose. 2018. Automatic feature learning for predicting vulnerable software components. IEEE Transactions on Software Engineering 47, 1 (2018), 67–85.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i3m2a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
[34] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [35] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. 2020. Hoppity: Learning graph transformations to detect and fix bugs in programs. In International Conference on Learning Representations (ICLR). [36] Yangruibo Ding, Sahil Suneja, Yunhui Zheng, Jim Laredo, Alessandro Morari, Gail Kaiser, and Baishakhi Ray. 2022. VELVET: a noVel Ensemble Learning approach to automatically locate VulnErable sTatements. In 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 959–970.
[37] Xiaoting Du, Zenghui Zhou, Beibei Yin, and Guanping Xiao. 2020. Cross-project bug type prediction based on transfer learning. Software Quality Journal 28, 1 (2020), 39–57. [38] Xu Duan, Jingzheng Wu, Shouling Ji, Zhiqing Rui, Tianyue Luo, Mutian Yang, and Yanjun Wu. 2019. VulSniper: Focus Your Attention to Shoot Fine-Grained Vulnerabilities.. In IJCAI. 4665–4671. [39] Andrew Dunham. 2009. rough-auditing-tool-for-security. https://github.com/andrew-d/rough-auditing-tool-forsecurity [40] Rudresh Dwivedi, Devam Dave, Het Naik, Smiti Singhal, Rana Omer, Pankesh Patel, Bin Qian, Zhenyu Wen, Tejal Shah, Graham Morgan, et al. 2023. Explainable AI (XAI): Core ideas, techniques, and solutions. Comput. Surveys 55, 9 (2023), 1–33. [41] Facebook. 2013. Infer. https://fbinfer.com/ [42] Katarzyna Filus, Miltiadis Siavvas, Joanna Domańska, and Erol Gelenbe. 2020. The random neural network as a bonding model for software vulnerability prediction. In Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems. Springer, 102–116.
[43] Park Foreman. 2019. Vulnerability management. CRC Press. [44] Stefan Frei, Martin May, Ulrich Fiedler, and Bernhard Plattner. 2006. Large-scale vulnerability analysis. In Proceedings of the 2006 SIGCOMM workshop on Large-scale attack defense. 131–138.
[45] Michael Fu and Chakkrit Tantithamthavorn. 2022. LineVul: A Transformer-based Line-Level Vulnerability Prediction. (2022). [46] Cong Gao, Geng Wang, Weisong Shi, Zhongmin Wang, and Yanping Chen. 2021. Autonomous driving security: State of the art and challenges. IEEE Internet of Things Journal 9, 10 (2021), 7572–7595. [47] Marian Gawron, Feng Cheng, and Christoph Meinel. 2018. Automatic vulnerability classification using machine learning. In Risks and Security of Internet and Systems: 12th International Conference, CRiSIS 2017, Dinard, France, September 19-21, 2017, Revised Selected Papers 12. Springer, 3–17.
[48] Seyed Mohammad Ghaffarian and Hamid Reza Shahriari. 2017. Software vulnerability analysis and discovery using machine-learning and data-mining techniques: A survey. ACM Computing Surveys (CSUR) 50, 4 (2017), 1–36. [49] Seyed Mohammad Ghaffarian and Hamid Reza Shahriari. 2021. Neural software vulnerability analysis using rich intermediate graph representations of programs. Information Sciences 553 (2021), 189–207. [50] Matthías Páll Gissurarson, Leonhard Applis, Annibale Panichella, Arie van Deursen, and David Sands. 2022. PropR: property-based automatic program repair. In Proceedings of the 44th International Conference on Software Engineering. 1768–1780. [51] Xi Gong, Zhenchang Xing, Xiaohong Li, Zhiyong Feng, and Zhuobing Han. 2019. Joint prediction of multiple vulnerability characteristics through multi-task learning. In 2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS). IEEE, 31–40.
[52] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Commun. ACM 63, 11 (2020), 139–144. [53] Katerina Goseva-Popstojanova and Jacob Tyo. 2017. Experience report: Security vulnerability profiles of mission critical software: Empirical analysis of security related bug reports. In 2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 152–163.
[54] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366 (2020).
[55] Jacob A. Harer, Onur Ozdemir, Tomo Lazovich, Christopher P. Reale, Rebecca L. Russell, Louis Y. Kim, and Peter Chin. 2018. Learning to Repair Software Vulnerabilities with Generative Adversarial Networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (Montréal, Canada) (NIPS’18). Curran Associates Inc., Red Hook, NY, USA, 7944–7954. [56] Nima Shiri Harzevili, Jiho Shin, Junjie Wang, and Song Wang. 2022. Characterizing and Understanding Software Security Vulnerabilities in Machine Learning Libraries. arXiv preprint arXiv:2203.06502 (2022). [57] Daojing He, Zhi Deng, Yuxing Zhang, Sammy Chan, Yao Cheng, and Nadra Guizani. 2020. Smart contract vulnerability analysis and security audit. IEEE Network 34, 5 (2020), 276–282.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:33
[58] David Hin, Andrey Kan, Huaming Chen, and M Ali Babar. 2022. LineVD: Statement-level Vulnerability Detection using Graph Neural Networks. arXiv preprint arXiv:2203.05181 (2022). [59] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation 18, 7 (2006), 1527–1554. [60] Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu Ubayashi. 2019. DeepJIT: an end-to-end deep learning framework for just-in-time defect prediction. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). IEEE, 34–45.
[61] Jianjun Huang, Songming Han, Wei You, Wenchang Shi, Bin Liang, Jingzheng Wu, and Yanjun Wu. 2021. Hunting vulnerable smart contracts via graph embedding based bytecode matching. IEEE Transactions on Information Forensics and Security 16 (2021), 2144–2156. [62] Xuan Huo, Yang Yang, Ming Li, and De-Chuan Zhan. 2018. Learning Semantic Features for Software Defect Prediction by Code Comments Embedding. In 2018 IEEE International Conference on Data Mining (ICDM). 1049–1054. https://doi.org/10.1109/ICDM.2018.00133 [63] Jay Jacobs, Sasha Romanosky, Octavian Suciuo, Benjamin Edwards, and Armin Sarabi. 2023. Enhancing Vulnerability Prioritization: Data-Driven Exploit Predictions with Community-Driven Insights. arXiv preprint arXiv:2302.14172 (2023). [64] Sanghoon Jeon and Huy Kang Kim. 2021. AutoVAS: An automated vulnerability analysis system with a deep learning approach. Computers & Security 106 (2021), 102308. [65] Yuning Jiang and Yacine Atif. 2020. An approach to discover and assess vulnerability severity automatically in cyber-physical systems. In 13th international conference on security of information and networks. 1–8.
[66] Yanjie Jiang, Hui Liu, Yuxia Zhang, Weixing Ji, Hao Zhong, and Lu Zhang. 2022. Do bugs lead to unnaturalness of source code?. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1085–1096.
[67] Matthieu Jimenez, Mike Papadakis, and Yves Le Traon. 2016. An empirical analysis of vulnerabilities in openssl and the linux kernel. In 2016 23rd Asia-Pacific Software Engineering Conference (APSEC). IEEE, 105–112. [68] Staffs Keele et al. 2007. Guidelines for performing systematic literature reviews in software engineering. Technical Report. Technical report, Ver. 2.3 EBSE Technical Report. EBSE. [69] Saad Khan and Simon Parkinson. 2018. Review into state of the art of vulnerability assessment using artificial intelligence. In Guide to Vulnerability Analysis for Computer Networks and Systems. Springer, 3–32.
[70] Taegyu Kim, Chung Hwan Kim, Junghwan Rhee, Fan Fei, Zhan Tu, Gregory Walkup, Xiangyu Zhang, Xinyan Deng, and Dongyan Xu. 2019. RVFuzzer: Finding Input Validation Bugs in Robotic Vehicles through Control-Guided Testing.. In USENIX Security Symposium. 425–442.
[71] Kyriakos Kritikos, Kostas Magoutis, Manos Papoutsakis, and Sotiris Ioannidis. 2019. A survey on vulnerability assessment tools and databases for cloud-based web applications. Array 3 (2019), 100011. [72] Jorrit Kronjee, Arjen Hommersom, and Harald Vranken. 2018. Discovering software vulnerabilities using data-flow analysis and machine learning. In Proceedings of the 13th international conference on availability, reliability and security. 1–10. [73] Patrick Kwaku Kudjo, Jinfu Chen, Solomon Mensah, Richard Amankwah, and Christopher Kudjo. 2020. The effect of Bellwether analysis on software vulnerability severity prediction models. Software Quality Journal 28 (2020), 1413–1446. [74] Patrick Kwaku Kudjo, Jinfu Chen, Minmin Zhou, Solomon Mensah, and Rubing Huang. 2019. Improving the accuracy of vulnerability report classification using term frequency-inverse gravity moment. In 2019 IEEE 19th International Conference on Software Quality, Reliability and Security (QRS). IEEE, 248–259. [75] Chris Lattner. 2008. LLVM and Clang: Next generation compiler technology.
[76] Tue Le, Tuan Nguyen, Trung Le, Dinh Phung, Paul Montague, Olivier De Vel, and Lizhen Qu. 2018. Maximal divergence sequential autoencoder for binary software vulnerability detection. In International Conference on Learning Representations.
[77] Triet HM Le, Huaming Chen, and M Ali Babar. 2021. A survey on data-driven software vulnerability assessment and prioritization. arXiv preprint arXiv:2107.08364 (2021).
[78] Triet HM Le, Huaming Chen, and M Ali Babar. 2022. A survey on data-driven software vulnerability assessment and prioritization. Comput. Surveys 55, 5 (2022), 1–39. [79] Triet Huynh Minh Le, David Hin, Roland Croft, and M Ali Babar. 2021. Deepcva: Automated commit-level vulnerability assessment with deep multi-task learning. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 717–729.
[80] Triet Huynh Minh Le, Bushra Sabir, and Muhammad Ali Babar. 2019. Automated software vulnerability assessment with concept drift. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). IEEE, 371–382.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i3m4a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
[81] Daniel Lehmann and Michael Pradel. 2019. Wasabi: A framework for dynamically analyzing webassembly. In Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems. 1045–1058.
[82] Jian Li, Pinjia He, Jieming Zhu, and Michael R Lyu. 2017. Software defect prediction via convolutional neural network. In 2017 IEEE International Conference on Software Quality, Reliability and Security (QRS). IEEE, 318–328.
[83] Leping Li, Hui Liu, Kejun Li, Yanjie Jiang, and Rui Sun. 2022. Generating Concise Patches for Newly Released Programming Assignments. IEEE Transactions on Software Engineering (2022).
[84] Yi Li, Shaohua Wang, and Tien N Nguyen. 2021. Vulnerability detection with fine-grained interpretations. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 292–303.
[85] Yi Li, Shaohua Wang, Tien N. Nguyen, and Son Van Nguyen. 2019. Improving Bug Detection via Context-Based Code Representation Learning and Attention-Based Neural Networks. Proc. ACM Program. Lang. 3, OOPSLA, Article 162 (oct 2019), 30 pages. https://doi.org/10.1145/3360588 [86] Zhen Li, Deqing Zou, Shouhuai Xu, Zhaoxuan Chen, Yawei Zhu, and Hai Jin. 2021. Vuldeelocator: a deep learningbased fine-grained vulnerability detector. IEEE Transactions on Dependable and Secure Computing (2021).
[87] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and Zhaoxuan Chen. 2021. Sysevr: A framework for using deep learning to detect software vulnerabilities. IEEE Transactions on Dependable and Secure Computing (2021). [88] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong. 2018. Vuldeepecker: A deep learning-based system for vulnerability detection. arXiv preprint arXiv:1801.01681 (2018). [89] Guanjun Lin, Sheng Wen, Qing-Long Han, Jun Zhang, and Yang Xiang. 2020. Software vulnerability detection using deep neural networks: a survey. Proc. IEEE 108, 10 (2020), 1825–1848. [90] Guanjun Lin, Jun Zhang, Wei Luo, Lei Pan, Olivier De Vel, Paul Montague, and Yang Xiang. 2019. Software vulnerability discovery via learning multi-domain knowledge bases. IEEE Transactions on Dependable and Secure Computing 18, 5 (2019), 2469–2485. [91] Guanjun Lin, Jun Zhang, Wei Luo, Lei Pan, and Yang Xiang. 2017. POSTER: Vulnerability discovery with function representation learning from unlabeled projects. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2539–2541.
[92] Guanjun Lin, Jun Zhang, Wei Luo, Lei Pan, Yang Xiang, Olivier De Vel, and Paul Montague. 2018. Cross-project transfer representation learning for vulnerable function discovery. IEEE Transactions on Industrial Informatics 14, 7 (2018), 3289–3297. [93] Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John Grundy, and Xiaohu Yang. 2021. On the reproducibility and replicability of deep learning in software engineering. ACM Transactions on Software Engineering and Methodology (TOSEM) 31, 1 (2021), 1–46. [94] Shigang Liu, Guanjun Lin, Qing-Long Han, Sheng Wen, Jun Zhang, and Yang Xiang. 2019. DeepBalance: Deep-learning and fuzzy oversampling for vulnerability detection. IEEE Transactions on Fuzzy Systems 28, 7 (2019), 1329–1343. [95] Shigang Liu, Guanjun Lin, Lizhen Qu, Jun Zhang, Olivier De Vel, Paul Montague, and Yang Xiang. 2020. CD-VulD: Cross-domain vulnerability discovery based on deep domain adaptation. IEEE Transactions on Dependable and Secure Computing (2020).
[96] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [97] Zhenguang Liu, Peng Qian, Xiang Wang, Lei Zhu, Qinming He, and Shouling Ji. 2021. Smart contract vulnerability detection: from pure neural network to interpretable graph feature and expert pattern fusion. arXiv preprint arXiv:2106.09282 (2021).
[98] Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu, and Xun Wang. 2021. Combining graph neural networks with expert knowledge for smart contract vulnerability detection. IEEE Transactions on Knowledge and Data Engineering (2021).
[99] Ning Lu, Bin Wang, Yongxin Zhang, Wenbo Shi, and Christian Esposito. 2021. NeuCheck: A more practical Ethereum smart contract security analysis tool. Software: Practice and Experience 51, 10 (2021), 2065–2084. [100] Qian Luo, Yurui Cao, Jiajia Liu, and Abderrahim Benslimane. 2019. Localization and navigation in autonomous driving: Threats and countermeasures. IEEE Wireless Communications 26, 4 (2019), 38–45. [101] Vinod Mahor, Kiran Pachlasiya, Bhagwati Garg, Mukesh Chouhan, Shrikant Telang, and Romil Rawat. 2022. Mobile Operating System (Android) Vulnerability Analysis Using Machine Learning. In Proceedings of International Conference on Network Security and Blockchain Technology: ICNSBT 2021. Springer, 159–169.
[102] Yi Mao, Yun Li, Jiatai Sun, and Yixin Chen. 2020. Explainable Software vulnerability detection based on Attentionbased Bidirectional Recurrent Neural Networks. In 2020 IEEE International Conference on Big Data (Big Data). IEEE, 4651–4656.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:35
[103] Daniel Marjamäki. 2016. Cppcheck. https://cppcheck.sourceforge.io/ [104] Nadia Medeiros, Naghmeh Ivaki, Pedro Costa, and Marco Vieira. 2023. Trustworthiness models to categorize and prioritize code for security improvement. Journal of Systems and Software (2023), 111621. [105] Dang Minh, H Xiang Wang, Y Fen Li, and Tan N Nguyen. 2022. Explainable artificial intelligence: a comprehensive review. Artificial Intelligence Review (2022), 1–66.
[106] Nicholas Nethercote and Julian Seward. 2007. Valgrind: a framework for heavyweight dynamic binary instrumentation. ACM Sigplan notices 42, 6 (2007), 89–100.
[107] Hoang H Nguyen, Nhat-Minh Nguyen, Hong-Phuc Doan, Zahra Ahmadi, Thanh-Nam Doan, and Lingxiao Jiang. 2022. MANDO-GURU: vulnerability detection for smart contract source code by heterogeneous graph embeddings. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1736–1740.
[108] Hoang H Nguyen, Nhat-Minh Nguyen, Chunyao Xie, Zahra Ahmadi, Daniel Kudendo, Thanh-Nam Doan, and Lingxiao Jiang. 2022. MANDO: Multi-Level Heterogeneous Graph Embeddings for Fine-Grained Detection of Smart Contract Vulnerabilities. arXiv preprint arXiv:2208.13252 (2022).
[109] Tuan Nguyen, Trung Le, Khanh Nguyen, Olivier de Vel, Paul Montague, John Grundy, and Dinh Phung. 2020. Deep cost-sensitive kernel machine for binary software vulnerability detection. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 164–177.
[110] Chao Ni, Wei Wang, Kaiwen Yang, Xin Xia, Kui Liu, and David Lo. 2022. The best of both worlds: integrating semantic features with expert features for defect prediction and localization. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 672–683.
[111] Yu Nong, Rainy Sharma, Abdelwahab Hamou-Lhadj, Xiapu Luo, and Haipeng Cai. 2022. Open Science in Software Engineering: A Study on Deep Learning-Based Vulnerability Detection. IEEE Transactions on Software Engineering (2022). [112] Yulei Pang, Xiaozhen Xue, and Akbar Siami Namin. 2015. Predicting vulnerable software components through n-gram analysis and statistical feature selection. In 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA). IEEE, 543–548.
[113] Luca Pascarella, Fabio Palomba, and Alberto Bacchelli. 2019. Fine-grained just-in-time defect prediction. Journal of Systems and Software 150 (2019), 22–36.
[114] Henning Perl, Sergej Dechand, Matthew Smith, Daniel Arp, Fabian Yamaguchi, Konrad Rieck, Sascha Fahl, and Yasemin Acar. 2015. Vccfinder: Finding potential vulnerabilities in open-source projects to assist code audits. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. 426–437.
[115] Kai Petersen, Sairam Vakkalanka, and Ludwik Kuzniarz. 2015. Guidelines for conducting systematic mapping studies in software engineering: An update. Information and software technology 64 (2015), 1–18. [116] Anh Viet Phan, Minh Le Nguyen, and Lam Thu Bui. 2017. Convolutional neural networks over control flow graphs for software defect prediction. In 2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI). IEEE, 45–52. [117] Valentina Piantadosi, Simone Scalabrino, and Rocco Oliveto. 2019. Fixing of security vulnerabilities in open source projects: A case study of apache http server and apache tomcat. In 2019 12th IEEE Conference on software testing, validation and verification (ICST). IEEE, 68–78.
[118] Michael Pradel and Koushik Sen. 2018. Deepbugs: A learning approach to name-based bug detection. Proceedings of the ACM on Programming Languages 2, OOPSLA (2018), 1–25.
[119] Ali Raza and Waseem Ahmed. 2022. Threat and Vulnerability management life cycle in operating systems. A systematic review. Journal of Multidisciplinary Engineering Science and Technology (JMEST) 9, 1 (2022).
[120] Timothé Riom, Arthur Sawadogo, Kevin Allix, Tegawendé F Bissyandé, Naouel Moha, and Jacques Klein. 2021. Revisiting the VCCFinder approach for the identification of vulnerability-contributing commits. Empirical Software Engineering 26, 3 (2021), 1–30. [121] Annachiara Ruospo, Alberto Bosio, Alessandro Ianne, and Ernesto Sanchez. 2020. Evaluating convolutional neural networks reliability depending on their data representation. In 2020 23rd Euromicro Conference on Digital System Design (DSD). IEEE, 672–679. [122] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood, and Marc McConley. 2018. Automated vulnerability detection in source code using deep representation learning. In 2018 17th IEEE international conference on machine learning and applications (ICMLA). IEEE, 757–762.
[123] Antonino Sabetta and Michele Bezzi. 2018. A practical approach to the automatic classification of security-relevant commits. In 2018 IEEE International conference on software maintenance and evolution (ICSME). IEEE, 579–582.
[124] Riccardo Scandariato, James Walden, Aram Hovsepyan, and Wouter Joosen. 2014. Predicting vulnerable software components via text mining. IEEE Transactions on Software Engineering 40, 10 (2014), 993–1006.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


11N1:i3m6a Shiri harzevili, Alvine Boaye Belle, Junjie Wang, Song Wang, Zhen Ming (Jack) Jiang, and Nachiappan Nagappan
[125] Abubakar Omari Abdallah Semasaba, Wei Zheng, Xiaoxue Wu, and Samuel Akwasi Agyemang. 2020. Literature survey of deep learning-based vulnerability analysis on source code. IET Software 14, 6 (2020), 654–664. [126] Lwin Khin Shar and Hee Beng Kuan Tan. 2010. Auditing the defense against cross site scripting in web applications. In 2010 International Conference on Security and Cryptography (SECRYPT). IEEE, 1–7.
[127] Lwin Khin Shar and Hee Beng Kuan Tan. 2012. Auditing the XSS defence features implemented in web application programs. IET software 6, 4 (2012), 377–390. [128] Thomas Shippey, David Bowes, and Tracy Hall. 2019. Automatically identifying code features for software defect prediction: Using AST N-grams. Information and Software Technology 106 (2019), 142–160. [129] SpotBugs. 2021. SpotBugs. .https://spotbugs.github.io/ [130] Miroslaw Staron, Mirosław Ochodek, Wilhelm Meding, and Ola Söder. 2020. Using machine learning to identify code fragments for manual review. In 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). IEEE, 513–516. [131] Octavian Suciu, Connor Nelson, Zhuoer Lyu, Tiffany Bao, and Tudor Dumitras, . 2022. Expected exploitability: Predicting the development of functional vulnerability exploits. In 31st USENIX Security Symposium (USENIX Security 22). 377–394. [132] Nan Sun, Jun Zhang, Paul Rimba, Shang Gao, Leo Yu Zhang, and Yang Xiang. 2018. Data-driven cybersecurity incident prediction: A survey. IEEE communications surveys & tutorials 21, 2 (2018), 1744–1772. [133] Youshuai Tan, Sijie Xu, Zhaowei Wang, Tao Zhang, Zhou Xu, and Xiapu Luo. 2020. Bug severity prediction using question-and-answer pairs from Stack Overflow. Journal of Systems and Software 165 (2020), 110567. [134] Junfeng Tian, Wenjing Xing, and Zhen Li. 2020. BVDetector: A program slice-based binary code vulnerability intelligent detection system. Information and Software Technology 123 (2020), 106289. [135] Michał Walkowski, Jacek Oko, and Sławomir Sujecki. 2021. Vulnerability Management Models Using a Common Vulnerability Scoring System. Applied Sciences 11, 18 (2021), 8735. [136] Huanting Wang, Guixin Ye, Zhanyong Tang, Shin Hwei Tan, Songfang Huang, Dingyi Fang, Yansong Feng, Lizhong Bian, and Zheng Wang. 2020. Combining graph-based learning with automated data collection for code vulnerability detection. IEEE Transactions on Information Forensics and Security 16 (2020), 1943–1958.
[137] Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2018. Deep semantic feature learning for software defect prediction. IEEE Transactions on Software Engineering 46, 12 (2018), 1267–1293.
[138] Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic features for defect prediction. In 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE). IEEE, 297–308. [139] David A. Wheeler. 2013. Dlawfinder. http://dwheeler.com/flawfinder/ [140] Hongjun Wu, Zhuo Zhang, Shangwen Wang, Yan Lei, Bo Lin, Yihao Qin, Haoyu Zhang, and Xiaoguang Mao. 2021. Peculiar: Smart Contract Vulnerability Detection Based on Crucial Data Flow Graph and Pre-training Techniques. In 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE). IEEE. 378–389.
[141] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated program repair in the era of large pretrained language models. In Proceedings of the 45th International Conference on Software Engineering (ICSE 2023). Association for Computing Machinery.
[142] Fabian Yamaguchi, Markus Lottmann, and Konrad Rieck. 2012. Generalized vulnerability extrapolation using abstract syntax trees. In Proceedings of the 28th Annual Computer Security Applications Conference. 359–368.
[143] Fabian Yamaguchi, Konrad Rieck, et al. 2011. Vulnerability extrapolation: Assisted discovery of vulnerabilities using machine learning. In 5th USENIX Workshop on Offensive Technologies (WOOT 11).
[144] Fabian Yamaguchi, Christian Wressnegger, Hugo Gascon, and Konrad Rieck. 2013. Chucky: Exposing missing checks in source code for vulnerability discovery. In Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security. 499–510.
[145] Han Yan, Senlin Luo, Limin Pan, and Yifei Zhang. 2021. HAN-BSVD: a hierarchical attention network for binary software vulnerability detection. Computers & Security 108 (2021), 102286. [146] Hongyu Yang, Haiyun Yang, Liang Zhang, and Xiang Cheng. 2022. Source Code Vulnerability Detection Using Vulnerability Dependency Representation Graph. In 2022 IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom). IEEE, 457–464.
[147] Limin Yang, Xiangxue Li, and Yu Yu. 2017. Vuldigger: A just-in-time and cost-aware tool for digging vulnerabilitycontributing changes. In GLOBECOM 2017-2017 IEEE Global Communications Conference. IEEE, 1–7.
[148] Jiao Yin, MingJian Tang, Jinli Cao, Hua Wang, and Mingshan You. 2022. A real-time dynamic concept adaptive learning algorithm for exploitability prediction. Neurocomputing 472 (2022), 252–265. [149] Suan Hsi Yong and Susan Horwitz. 2005. Using static analysis to reduce dynamic analysis overhead. Formal Methods in System Design 27 (2005), 313–334. [150] Cheng Zeng, Chun Ying Zhou, Sheng Kai Lv, Peng He, and Jie Huang. 2021. GCN2defect: Graph Convolutional Networks for SMOTETomek-based Software Defect Prediction. In 2021 IEEE 32nd International Symposium on Software
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.


A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning 111:37
Reliability Engineering (ISSRE). IEEE, 69–79.
[151] Peng Zeng, Guanjun Lin, Lei Pan, Yonghang Tai, and Jun Zhang. 2020. Software vulnerability analysis and discovery using deep learning techniques: A survey. IEEE Access 8 (2020), 197158–197172. [152] Zhuo Zhang, Yan Lei, Meng Yan, Yue Yu, Jiachi Chen, Shangwen Wang, and Xiaoguang Mao. 2022. Reentrancy Vulnerability Detection and Localization: A Deep Learning Based Two-phase Approach. In 37th IEEE/ACM International Conference on Automated Software Engineering. 1–13.
[153] Weining Zheng, Yuan Jiang, and Xiaohong Su. 2021. VulSPG: Vulnerability detection based on slice property graph representation learning. arXiv preprint arXiv:2109.02527 (2021). [154] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. Advances in neural information processing systems 32 (2019).
[155] Yaqin Zhou and Asankhaya Sharma. 2017. Automated identification of security issues from commit messages and bug reports. In Proceedings of the 2017 11th joint meeting on foundations of software engineering. 914–919.
[156] Weiyuan Zhuang, Hao Wang, and Xiaofang Zhang. 2022. Just-in-time defect prediction based on AST change embedding. Knowledge-Based Systems 248 (2022), 108852. [157] Yuan Zhuang, Zhenguang Liu, Peng Qian, Qi Liu, Xiang Wang, and Qinming He. 2020. Smart Contract Vulnerability Detection using Graph Neural Network.. In IJCAI. 3283–3290. [158] Noah Ziems and Shaoen Wu. 2021. Security Vulnerability Detection Using Deep Learning Natural Language Processing. In IEEE INFOCOM 2021-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 1–6. [159] Deqing Zou, Yutao Hu, Wenke Li, Yueming Wu, Haojun Zhao, and Hai Jin. 2022. mVulPreter: A Multi-Granularity Vulnerability Detection System With Interpretations. IEEE Transactions on Dependable and Secure Computing (2022). [160] Deqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li, and Hai Jin. 2019. muVulDeePecker: A Deep Learning-Based System for Multiclass Vulnerability Detection. IEEE Transactions on Dependable and Secure Computing 18, 5 (2019), 2224–2236. [161] Deqing Zou, Yawei Zhu, Shouhuai Xu, Zhen Li, Hai Jin, and Hengkai Ye. 2021. Interpreting Deep Learning-Based Vulnerability Detector Predictions Based on Heuristic Searching. 30, 2, Article 23 (mar 2021), 31 pages. https: //doi.org/10.1145/3429444
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.